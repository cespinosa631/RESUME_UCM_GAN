{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "733c7476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from custom_dataset import CustomImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2f85d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(opt.latent_dim, 512, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, opt.channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
    "        img = self.model(z)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(opt.channels, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        validity = self.model(img)\n",
    "        return validity\n",
    "\n",
    "\n",
    "def adversarial_loss(output, target):\n",
    "    target = target.expand_as(output)\n",
    "    loss = F.binary_cross_entropy(output, target)\n",
    "    return loss\n",
    "\n",
    "def save_model(opt, generator, discriminator, optimizer_G, optimizer_D, current_epoch):\n",
    "    out = os.path.join(opt.model_path, \"checkpoint_{}.tar\".format(current_epoch))\n",
    "    state = {\n",
    "        'generator': generator.state_dict(),\n",
    "        'discriminator': discriminator.state_dict(),\n",
    "        'optimizer_G': optimizer_G.state_dict(),\n",
    "        'optimizer_D': optimizer_D.state_dict(),\n",
    "        'epoch': current_epoch\n",
    "    }\n",
    "    torch.save(state, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f5727677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(opt):\n",
    "    os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    cid = CustomImageDataset(opt.csv_file, opt.img_size)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        cid,\n",
    "        batch_size=opt.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=opt.n_cpu,\n",
    "    )\n",
    "\n",
    "    # Define Generator and Discriminator\n",
    "    generator = Generator().to(device)\n",
    "    discriminator = Discriminator().to(device)\n",
    "\n",
    "    # Checking if GPU is available to run code\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        generator.cuda()\n",
    "        discriminator.cuda()\n",
    "\n",
    "    # Defining loss functions / optimizers\n",
    "    optimizer_G = optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "    optimizer_D = optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    # Keeping track of both gennerator and discriminator losses\n",
    "    generator_losses = []\n",
    "    discriminator_losses = []\n",
    "\n",
    "    counter = 0   # Will keep track of gen/disc training sequence\n",
    "    g = 5   # Training epoch sequence for generator\n",
    "    d = 1   # Training epoch sequence for discriminator\n",
    "\n",
    "    for epoch in range(opt.n_epochs):\n",
    "        for i, (imgs, _) in enumerate(dataloader):\n",
    "            valid = Tensor(imgs.size(0), 1, 1, 1).fill_(1.0)\n",
    "            fake = Tensor(imgs.size(0), 1, 1, 1).fill_(0.0)\n",
    "\n",
    "            real_imgs = imgs.type(Tensor)\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Making noisy image from a gaussian distribution\n",
    "            z = Tensor(np.random.normal(0, 1, (imgs.shape[0], opt.latent_dim)))\n",
    "\n",
    "            # Generating fake images\n",
    "            gen_imgs = generator(z)\n",
    "\n",
    "            g_loss = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            # Training discriminator for 15 epochs, then train generator for 30 epochs \n",
    "            if counter < d or counter >= (g+d):\n",
    "                if counter >= (g+d):   #reset counter\n",
    "                    counter = 0\n",
    "                d_loss.backward()\n",
    "                optimizer_D.step()\n",
    "                print(\"[Epoch %d/%d] [Batch %d/%d] [D IS TRAINING] [D loss: %.4f] [G loss: %.4f]\"\n",
    "                % (epoch+1, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()))                \n",
    "\n",
    "            elif counter < (g+d):\n",
    "                g_loss.backward()\n",
    "                optimizer_G.step()\n",
    "                print(\"[Epoch %d/%d] [Batch %d/%d] [G IS TRAINING] [D loss: %.4f] [G loss: %.4f]\"\n",
    "                % (epoch+1, opt.n_epochs, i, len(dataloader), d_loss.item(), g_loss.item()))\n",
    "\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            if batches_done % opt.sample_interval == 0:\n",
    "                save_image(gen_imgs.data[:25], \"images/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "\n",
    "            if (epoch + 1) % opt.save_interval == 0:\n",
    "                save_model(opt, generator, discriminator, optimizer_G, optimizer_D, epoch + 1)\n",
    "\n",
    "        generator_losses.append(g_loss.item())\n",
    "        discriminator_losses.append(d_loss.item())\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "    plt.plot(generator_losses, label='Generator Loss')\n",
    "    plt.plot(discriminator_losses, label='Discriminator Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig('plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb64f890",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cristianespinosa/opt/anaconda3/envs/tensor/lib/python3.9/site-packages/torch/utils/data/dataloader.py:563: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/500] [Batch 0/312] [D IS TRAINING] [D loss: 0.6781] [G loss: 0.7218]\n",
      "[Epoch 1/500] [Batch 1/312] [D IS TRAINING] [D loss: 0.5342] [G loss: 0.7881]\n",
      "[Epoch 1/500] [Batch 2/312] [D IS TRAINING] [D loss: 0.4287] [G loss: 0.9648]\n",
      "[Epoch 1/500] [Batch 3/312] [D IS TRAINING] [D loss: 0.3643] [G loss: 1.0988]\n",
      "[Epoch 1/500] [Batch 4/312] [D IS TRAINING] [D loss: 0.3380] [G loss: 1.1937]\n",
      "[Epoch 1/500] [Batch 5/312] [D IS TRAINING] [D loss: 0.2916] [G loss: 1.2676]\n",
      "[Epoch 1/500] [Batch 6/312] [D IS TRAINING] [D loss: 0.2688] [G loss: 1.4994]\n",
      "[Epoch 1/500] [Batch 7/312] [D IS TRAINING] [D loss: 0.2400] [G loss: 1.6501]\n",
      "[Epoch 1/500] [Batch 8/312] [D IS TRAINING] [D loss: 0.1965] [G loss: 1.6997]\n",
      "[Epoch 1/500] [Batch 9/312] [D IS TRAINING] [D loss: 0.1855] [G loss: 1.7663]\n",
      "[Epoch 1/500] [Batch 10/312] [D IS TRAINING] [D loss: 0.1660] [G loss: 1.8491]\n",
      "[Epoch 1/500] [Batch 11/312] [D IS TRAINING] [D loss: 0.1482] [G loss: 1.9530]\n",
      "[Epoch 1/500] [Batch 12/312] [D IS TRAINING] [D loss: 0.1260] [G loss: 2.1393]\n",
      "[Epoch 1/500] [Batch 13/312] [D IS TRAINING] [D loss: 0.1303] [G loss: 2.2435]\n",
      "[Epoch 1/500] [Batch 14/312] [D IS TRAINING] [D loss: 0.1230] [G loss: 2.1154]\n",
      "[Epoch 1/500] [Batch 15/312] [D IS TRAINING] [D loss: 0.1087] [G loss: 2.2517]\n",
      "[Epoch 1/500] [Batch 16/312] [D IS TRAINING] [D loss: 0.1015] [G loss: 2.3475]\n",
      "[Epoch 1/500] [Batch 17/312] [D IS TRAINING] [D loss: 0.0841] [G loss: 2.4104]\n",
      "[Epoch 1/500] [Batch 18/312] [D IS TRAINING] [D loss: 0.0774] [G loss: 2.4859]\n",
      "[Epoch 1/500] [Batch 19/312] [D IS TRAINING] [D loss: 0.0702] [G loss: 2.6346]\n",
      "[Epoch 1/500] [Batch 20/312] [D IS TRAINING] [D loss: 0.0664] [G loss: 2.6295]\n",
      "[Epoch 1/500] [Batch 21/312] [D IS TRAINING] [D loss: 0.0663] [G loss: 2.6989]\n",
      "[Epoch 1/500] [Batch 22/312] [D IS TRAINING] [D loss: 0.0510] [G loss: 2.9781]\n",
      "[Epoch 1/500] [Batch 23/312] [D IS TRAINING] [D loss: 0.0483] [G loss: 3.1501]\n",
      "[Epoch 1/500] [Batch 24/312] [D IS TRAINING] [D loss: 0.0425] [G loss: 3.2634]\n",
      "[Epoch 1/500] [Batch 25/312] [D IS TRAINING] [D loss: 0.0439] [G loss: 3.1625]\n",
      "[Epoch 1/500] [Batch 26/312] [D IS TRAINING] [D loss: 0.0388] [G loss: 3.1973]\n",
      "[Epoch 1/500] [Batch 27/312] [D IS TRAINING] [D loss: 0.0338] [G loss: 3.3466]\n",
      "[Epoch 1/500] [Batch 28/312] [D IS TRAINING] [D loss: 0.0347] [G loss: 3.3324]\n",
      "[Epoch 1/500] [Batch 29/312] [D IS TRAINING] [D loss: 0.0366] [G loss: 3.2549]\n",
      "[Epoch 1/500] [Batch 30/312] [D IS TRAINING] [D loss: 0.0352] [G loss: 3.3353]\n",
      "[Epoch 1/500] [Batch 31/312] [D IS TRAINING] [D loss: 0.0286] [G loss: 3.5050]\n",
      "[Epoch 1/500] [Batch 32/312] [D IS TRAINING] [D loss: 0.0239] [G loss: 3.7735]\n",
      "[Epoch 1/500] [Batch 33/312] [D IS TRAINING] [D loss: 0.0264] [G loss: 3.7366]\n",
      "[Epoch 1/500] [Batch 34/312] [D IS TRAINING] [D loss: 0.0272] [G loss: 3.5066]\n",
      "[Epoch 1/500] [Batch 35/312] [D IS TRAINING] [D loss: 0.0247] [G loss: 3.6492]\n",
      "[Epoch 1/500] [Batch 36/312] [D IS TRAINING] [D loss: 0.0190] [G loss: 3.9896]\n",
      "[Epoch 1/500] [Batch 37/312] [D IS TRAINING] [D loss: 0.0174] [G loss: 4.2088]\n",
      "[Epoch 1/500] [Batch 38/312] [D IS TRAINING] [D loss: 0.0171] [G loss: 4.2507]\n",
      "[Epoch 1/500] [Batch 39/312] [D IS TRAINING] [D loss: 0.0177] [G loss: 4.1259]\n",
      "[Epoch 1/500] [Batch 40/312] [D IS TRAINING] [D loss: 0.0193] [G loss: 4.1156]\n",
      "[Epoch 1/500] [Batch 41/312] [D IS TRAINING] [D loss: 0.0159] [G loss: 4.2034]\n",
      "[Epoch 1/500] [Batch 42/312] [D IS TRAINING] [D loss: 0.0161] [G loss: 4.2427]\n",
      "[Epoch 1/500] [Batch 43/312] [D IS TRAINING] [D loss: 0.0158] [G loss: 4.2354]\n",
      "[Epoch 1/500] [Batch 44/312] [D IS TRAINING] [D loss: 0.0139] [G loss: 4.3330]\n",
      "[Epoch 1/500] [Batch 45/312] [D IS TRAINING] [D loss: 0.0142] [G loss: 4.3908]\n",
      "[Epoch 1/500] [Batch 46/312] [D IS TRAINING] [D loss: 0.0152] [G loss: 4.2658]\n",
      "[Epoch 1/500] [Batch 47/312] [D IS TRAINING] [D loss: 0.0137] [G loss: 4.2688]\n",
      "[Epoch 1/500] [Batch 48/312] [D IS TRAINING] [D loss: 0.0128] [G loss: 4.3781]\n",
      "[Epoch 1/500] [Batch 49/312] [D IS TRAINING] [D loss: 0.0130] [G loss: 4.4129]\n",
      "[Epoch 1/500] [Batch 50/312] [D IS TRAINING] [D loss: 0.0120] [G loss: 4.3639]\n",
      "[Epoch 1/500] [Batch 51/312] [D IS TRAINING] [D loss: 0.0125] [G loss: 4.4289]\n",
      "[Epoch 1/500] [Batch 52/312] [D IS TRAINING] [D loss: 0.0119] [G loss: 4.5939]\n",
      "[Epoch 1/500] [Batch 53/312] [D IS TRAINING] [D loss: 0.0103] [G loss: 4.6454]\n",
      "[Epoch 1/500] [Batch 54/312] [D IS TRAINING] [D loss: 0.0106] [G loss: 4.5481]\n",
      "[Epoch 1/500] [Batch 55/312] [D IS TRAINING] [D loss: 0.0111] [G loss: 4.5727]\n",
      "[Epoch 1/500] [Batch 56/312] [D IS TRAINING] [D loss: 0.0105] [G loss: 4.6074]\n",
      "[Epoch 1/500] [Batch 57/312] [D IS TRAINING] [D loss: 0.0097] [G loss: 4.6571]\n",
      "[Epoch 1/500] [Batch 58/312] [D IS TRAINING] [D loss: 0.0098] [G loss: 4.6479]\n",
      "[Epoch 1/500] [Batch 59/312] [D IS TRAINING] [D loss: 0.0102] [G loss: 4.6562]\n",
      "[Epoch 1/500] [Batch 60/312] [D IS TRAINING] [D loss: 0.0106] [G loss: 4.6230]\n",
      "[Epoch 1/500] [Batch 61/312] [D IS TRAINING] [D loss: 0.0095] [G loss: 4.6842]\n",
      "[Epoch 1/500] [Batch 62/312] [D IS TRAINING] [D loss: 0.0083] [G loss: 4.8187]\n",
      "[Epoch 1/500] [Batch 63/312] [D IS TRAINING] [D loss: 0.0085] [G loss: 4.8866]\n",
      "[Epoch 1/500] [Batch 64/312] [D IS TRAINING] [D loss: 0.0080] [G loss: 4.7691]\n",
      "[Epoch 1/500] [Batch 65/312] [D IS TRAINING] [D loss: 0.0083] [G loss: 4.8388]\n",
      "[Epoch 1/500] [Batch 66/312] [D IS TRAINING] [D loss: 0.0085] [G loss: 4.8137]\n",
      "[Epoch 1/500] [Batch 67/312] [D IS TRAINING] [D loss: 0.0090] [G loss: 4.7958]\n",
      "[Epoch 1/500] [Batch 68/312] [D IS TRAINING] [D loss: 0.0090] [G loss: 4.8407]\n",
      "[Epoch 1/500] [Batch 69/312] [D IS TRAINING] [D loss: 0.0071] [G loss: 4.9308]\n",
      "[Epoch 1/500] [Batch 70/312] [D IS TRAINING] [D loss: 0.0072] [G loss: 4.9958]\n",
      "[Epoch 1/500] [Batch 71/312] [D IS TRAINING] [D loss: 0.0082] [G loss: 4.9687]\n",
      "[Epoch 1/500] [Batch 72/312] [D IS TRAINING] [D loss: 0.0074] [G loss: 5.0081]\n",
      "[Epoch 1/500] [Batch 73/312] [D IS TRAINING] [D loss: 0.0064] [G loss: 5.0715]\n",
      "[Epoch 1/500] [Batch 74/312] [D IS TRAINING] [D loss: 0.0069] [G loss: 5.1607]\n",
      "[Epoch 1/500] [Batch 75/312] [D IS TRAINING] [D loss: 0.0065] [G loss: 5.0334]\n",
      "[Epoch 1/500] [Batch 76/312] [D IS TRAINING] [D loss: 0.0077] [G loss: 4.9339]\n",
      "[Epoch 1/500] [Batch 77/312] [D IS TRAINING] [D loss: 0.0075] [G loss: 4.9430]\n",
      "[Epoch 1/500] [Batch 78/312] [D IS TRAINING] [D loss: 0.0068] [G loss: 4.9986]\n",
      "[Epoch 1/500] [Batch 79/312] [D IS TRAINING] [D loss: 0.0065] [G loss: 5.0976]\n",
      "[Epoch 1/500] [Batch 80/312] [D IS TRAINING] [D loss: 0.0069] [G loss: 5.0732]\n",
      "[Epoch 1/500] [Batch 81/312] [D IS TRAINING] [D loss: 0.0065] [G loss: 5.1108]\n",
      "[Epoch 1/500] [Batch 82/312] [D IS TRAINING] [D loss: 0.0061] [G loss: 5.1792]\n",
      "[Epoch 1/500] [Batch 83/312] [D IS TRAINING] [D loss: 0.0059] [G loss: 5.2458]\n",
      "[Epoch 1/500] [Batch 84/312] [D IS TRAINING] [D loss: 0.0059] [G loss: 5.2750]\n",
      "[Epoch 1/500] [Batch 85/312] [D IS TRAINING] [D loss: 0.0051] [G loss: 5.2993]\n",
      "[Epoch 1/500] [Batch 86/312] [D IS TRAINING] [D loss: 0.0053] [G loss: 5.2643]\n",
      "[Epoch 1/500] [Batch 87/312] [D IS TRAINING] [D loss: 0.0050] [G loss: 5.3168]\n",
      "[Epoch 1/500] [Batch 88/312] [D IS TRAINING] [D loss: 0.0051] [G loss: 5.3519]\n",
      "[Epoch 1/500] [Batch 89/312] [D IS TRAINING] [D loss: 0.0050] [G loss: 5.3104]\n",
      "[Epoch 1/500] [Batch 90/312] [D IS TRAINING] [D loss: 0.0047] [G loss: 5.3172]\n",
      "[Epoch 1/500] [Batch 91/312] [D IS TRAINING] [D loss: 0.0049] [G loss: 5.3478]\n",
      "[Epoch 1/500] [Batch 92/312] [D IS TRAINING] [D loss: 0.0056] [G loss: 5.3454]\n",
      "[Epoch 1/500] [Batch 93/312] [D IS TRAINING] [D loss: 0.0048] [G loss: 5.3007]\n",
      "[Epoch 1/500] [Batch 94/312] [D IS TRAINING] [D loss: 0.0057] [G loss: 5.2931]\n",
      "[Epoch 1/500] [Batch 95/312] [D IS TRAINING] [D loss: 0.0048] [G loss: 5.3029]\n",
      "[Epoch 1/500] [Batch 96/312] [D IS TRAINING] [D loss: 0.0051] [G loss: 5.2997]\n",
      "[Epoch 1/500] [Batch 97/312] [D IS TRAINING] [D loss: 0.0052] [G loss: 5.3225]\n",
      "[Epoch 1/500] [Batch 98/312] [D IS TRAINING] [D loss: 0.0048] [G loss: 5.2498]\n",
      "[Epoch 1/500] [Batch 99/312] [D IS TRAINING] [D loss: 0.0057] [G loss: 5.1601]\n",
      "[Epoch 1/500] [Batch 100/312] [D IS TRAINING] [D loss: 0.0054] [G loss: 5.1490]\n",
      "[Epoch 1/500] [Batch 101/312] [D IS TRAINING] [D loss: 0.0059] [G loss: 5.1320]\n",
      "[Epoch 1/500] [Batch 102/312] [D IS TRAINING] [D loss: 0.0056] [G loss: 5.1326]\n",
      "[Epoch 1/500] [Batch 103/312] [D IS TRAINING] [D loss: 0.0051] [G loss: 5.2456]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/500] [Batch 104/312] [D IS TRAINING] [D loss: 0.0052] [G loss: 5.2156]\n",
      "[Epoch 1/500] [Batch 105/312] [D IS TRAINING] [D loss: 0.0052] [G loss: 5.2581]\n",
      "[Epoch 1/500] [Batch 106/312] [D IS TRAINING] [D loss: 0.0044] [G loss: 5.4112]\n",
      "[Epoch 1/500] [Batch 107/312] [D IS TRAINING] [D loss: 0.0045] [G loss: 5.4834]\n",
      "[Epoch 1/500] [Batch 108/312] [D IS TRAINING] [D loss: 0.0047] [G loss: 5.5548]\n",
      "[Epoch 1/500] [Batch 109/312] [D IS TRAINING] [D loss: 0.0040] [G loss: 5.5989]\n",
      "[Epoch 1/500] [Batch 110/312] [D IS TRAINING] [D loss: 0.0040] [G loss: 5.5262]\n",
      "[Epoch 1/500] [Batch 111/312] [D IS TRAINING] [D loss: 0.0042] [G loss: 5.4543]\n",
      "[Epoch 1/500] [Batch 112/312] [D IS TRAINING] [D loss: 0.0040] [G loss: 5.5044]\n",
      "[Epoch 1/500] [Batch 113/312] [D IS TRAINING] [D loss: 0.0039] [G loss: 5.5220]\n",
      "[Epoch 1/500] [Batch 114/312] [D IS TRAINING] [D loss: 0.0040] [G loss: 5.5827]\n",
      "[Epoch 1/500] [Batch 115/312] [D IS TRAINING] [D loss: 0.0043] [G loss: 5.5127]\n",
      "[Epoch 1/500] [Batch 116/312] [D IS TRAINING] [D loss: 0.0041] [G loss: 5.4962]\n",
      "[Epoch 1/500] [Batch 117/312] [D IS TRAINING] [D loss: 0.0041] [G loss: 5.4798]\n",
      "[Epoch 1/500] [Batch 118/312] [D IS TRAINING] [D loss: 0.0044] [G loss: 5.3962]\n",
      "[Epoch 1/500] [Batch 119/312] [D IS TRAINING] [D loss: 0.0043] [G loss: 5.3917]\n",
      "[Epoch 1/500] [Batch 120/312] [D IS TRAINING] [D loss: 0.0045] [G loss: 5.4599]\n",
      "[Epoch 1/500] [Batch 121/312] [D IS TRAINING] [D loss: 0.0037] [G loss: 5.5896]\n",
      "[Epoch 1/500] [Batch 122/312] [D IS TRAINING] [D loss: 0.0039] [G loss: 5.6293]\n",
      "[Epoch 1/500] [Batch 123/312] [D IS TRAINING] [D loss: 0.0038] [G loss: 5.5925]\n",
      "[Epoch 1/500] [Batch 124/312] [D IS TRAINING] [D loss: 0.0039] [G loss: 5.5277]\n",
      "[Epoch 1/500] [Batch 125/312] [D IS TRAINING] [D loss: 0.0038] [G loss: 5.5121]\n",
      "[Epoch 1/500] [Batch 126/312] [D IS TRAINING] [D loss: 0.0038] [G loss: 5.5876]\n",
      "[Epoch 1/500] [Batch 127/312] [D IS TRAINING] [D loss: 0.0037] [G loss: 5.7276]\n",
      "[Epoch 1/500] [Batch 128/312] [D IS TRAINING] [D loss: 0.0031] [G loss: 5.7551]\n",
      "[Epoch 1/500] [Batch 129/312] [D IS TRAINING] [D loss: 0.0033] [G loss: 5.6623]\n",
      "[Epoch 1/500] [Batch 130/312] [D IS TRAINING] [D loss: 0.0039] [G loss: 5.6290]\n",
      "[Epoch 1/500] [Batch 131/312] [D IS TRAINING] [D loss: 0.0035] [G loss: 5.6845]\n",
      "[Epoch 1/500] [Batch 132/312] [D IS TRAINING] [D loss: 0.0035] [G loss: 5.7269]\n",
      "[Epoch 1/500] [Batch 133/312] [D IS TRAINING] [D loss: 0.0030] [G loss: 5.8489]\n",
      "[Epoch 1/500] [Batch 134/312] [D IS TRAINING] [D loss: 0.0030] [G loss: 5.9769]\n",
      "[Epoch 1/500] [Batch 135/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 6.0792]\n",
      "[Epoch 1/500] [Batch 136/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 6.1056]\n",
      "[Epoch 1/500] [Batch 137/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.0459]\n",
      "[Epoch 1/500] [Batch 138/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 5.9582]\n",
      "[Epoch 1/500] [Batch 139/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 5.8661]\n",
      "[Epoch 1/500] [Batch 140/312] [D IS TRAINING] [D loss: 0.0033] [G loss: 5.7595]\n",
      "[Epoch 1/500] [Batch 141/312] [D IS TRAINING] [D loss: 0.0034] [G loss: 5.8236]\n",
      "[Epoch 1/500] [Batch 142/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 5.9522]\n",
      "[Epoch 1/500] [Batch 143/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.0800]\n",
      "[Epoch 1/500] [Batch 144/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 6.0160]\n",
      "[Epoch 1/500] [Batch 145/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 6.0066]\n",
      "[Epoch 1/500] [Batch 146/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 6.0310]\n",
      "[Epoch 1/500] [Batch 147/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.0610]\n",
      "[Epoch 1/500] [Batch 148/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 6.0151]\n",
      "[Epoch 1/500] [Batch 149/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 5.9589]\n",
      "[Epoch 1/500] [Batch 150/312] [D IS TRAINING] [D loss: 0.0032] [G loss: 5.9483]\n",
      "[Epoch 1/500] [Batch 151/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 5.9611]\n",
      "[Epoch 1/500] [Batch 152/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.0245]\n",
      "[Epoch 1/500] [Batch 153/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 6.0333]\n",
      "[Epoch 1/500] [Batch 154/312] [D IS TRAINING] [D loss: 0.0030] [G loss: 5.9658]\n",
      "[Epoch 1/500] [Batch 155/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 5.9884]\n",
      "[Epoch 1/500] [Batch 156/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 5.9454]\n",
      "[Epoch 1/500] [Batch 157/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 5.9301]\n",
      "[Epoch 1/500] [Batch 158/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 5.9792]\n",
      "[Epoch 1/500] [Batch 159/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 6.0321]\n",
      "[Epoch 1/500] [Batch 160/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 5.9824]\n",
      "[Epoch 1/500] [Batch 161/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 5.9472]\n",
      "[Epoch 1/500] [Batch 162/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 5.9732]\n",
      "[Epoch 1/500] [Batch 163/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 5.9887]\n",
      "[Epoch 1/500] [Batch 164/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 6.1226]\n",
      "[Epoch 1/500] [Batch 165/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.1872]\n",
      "[Epoch 1/500] [Batch 166/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 6.1811]\n",
      "[Epoch 1/500] [Batch 167/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.0632]\n",
      "[Epoch 1/500] [Batch 168/312] [D IS TRAINING] [D loss: 0.0030] [G loss: 5.8621]\n",
      "[Epoch 1/500] [Batch 169/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 5.8529]\n",
      "[Epoch 1/500] [Batch 170/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 5.8968]\n",
      "[Epoch 1/500] [Batch 171/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 5.9851]\n",
      "[Epoch 1/500] [Batch 172/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 6.1720]\n",
      "[Epoch 1/500] [Batch 173/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 6.1609]\n",
      "[Epoch 1/500] [Batch 174/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 6.0813]\n",
      "[Epoch 1/500] [Batch 175/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 5.8852]\n",
      "[Epoch 1/500] [Batch 176/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 5.9050]\n",
      "[Epoch 1/500] [Batch 177/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.0702]\n",
      "[Epoch 1/500] [Batch 178/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.2211]\n",
      "[Epoch 1/500] [Batch 179/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.1101]\n",
      "[Epoch 1/500] [Batch 180/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 6.0126]\n",
      "[Epoch 1/500] [Batch 181/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.0910]\n",
      "[Epoch 1/500] [Batch 182/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.0979]\n",
      "[Epoch 1/500] [Batch 183/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.1414]\n",
      "[Epoch 1/500] [Batch 184/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.1442]\n",
      "[Epoch 1/500] [Batch 185/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.2188]\n",
      "[Epoch 1/500] [Batch 186/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.2958]\n",
      "[Epoch 1/500] [Batch 187/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.2826]\n",
      "[Epoch 1/500] [Batch 188/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 6.2193]\n",
      "[Epoch 1/500] [Batch 189/312] [D IS TRAINING] [D loss: 0.0021] [G loss: 6.0142]\n",
      "[Epoch 1/500] [Batch 190/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 6.1571]\n",
      "[Epoch 1/500] [Batch 191/312] [D IS TRAINING] [D loss: 0.0017] [G loss: 6.5246]\n",
      "[Epoch 1/500] [Batch 192/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.2515]\n",
      "[Epoch 1/500] [Batch 193/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.3543]\n",
      "[Epoch 1/500] [Batch 194/312] [D IS TRAINING] [D loss: 0.0022] [G loss: 6.1663]\n",
      "[Epoch 1/500] [Batch 195/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 5.8006]\n",
      "[Epoch 1/500] [Batch 196/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.1541]\n",
      "[Epoch 1/500] [Batch 197/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.1501]\n",
      "[Epoch 1/500] [Batch 198/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 5.8580]\n",
      "[Epoch 1/500] [Batch 199/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 5.7738]\n",
      "[Epoch 1/500] [Batch 200/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.3596]\n",
      "[Epoch 1/500] [Batch 201/312] [D IS TRAINING] [D loss: 0.0044] [G loss: 5.6588]\n",
      "[Epoch 1/500] [Batch 202/312] [D IS TRAINING] [D loss: 0.0053] [G loss: 5.3246]\n",
      "[Epoch 1/500] [Batch 203/312] [D IS TRAINING] [D loss: 0.0052] [G loss: 6.0291]\n",
      "[Epoch 1/500] [Batch 204/312] [D IS TRAINING] [D loss: 0.0058] [G loss: 5.7746]\n",
      "[Epoch 1/500] [Batch 205/312] [D IS TRAINING] [D loss: 0.0142] [G loss: 4.2766]\n",
      "[Epoch 1/500] [Batch 206/312] [D IS TRAINING] [D loss: 0.5670] [G loss: 11.0841]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/500] [Batch 207/312] [D IS TRAINING] [D loss: 4.4536] [G loss: 0.0005]\n",
      "[Epoch 1/500] [Batch 208/312] [D IS TRAINING] [D loss: 1.8084] [G loss: 0.0420]\n",
      "[Epoch 1/500] [Batch 209/312] [D IS TRAINING] [D loss: 0.7527] [G loss: 3.0419]\n",
      "[Epoch 1/500] [Batch 210/312] [D IS TRAINING] [D loss: 0.3847] [G loss: 2.5696]\n",
      "[Epoch 1/500] [Batch 211/312] [D IS TRAINING] [D loss: 0.1897] [G loss: 2.5822]\n",
      "[Epoch 1/500] [Batch 212/312] [D IS TRAINING] [D loss: 0.1260] [G loss: 2.3859]\n",
      "[Epoch 1/500] [Batch 213/312] [D IS TRAINING] [D loss: 0.0907] [G loss: 2.4183]\n",
      "[Epoch 1/500] [Batch 214/312] [D IS TRAINING] [D loss: 0.0662] [G loss: 2.7375]\n",
      "[Epoch 1/500] [Batch 215/312] [D IS TRAINING] [D loss: 0.0470] [G loss: 3.1248]\n",
      "[Epoch 1/500] [Batch 216/312] [D IS TRAINING] [D loss: 0.0430] [G loss: 3.4025]\n",
      "[Epoch 1/500] [Batch 217/312] [D IS TRAINING] [D loss: 0.0330] [G loss: 3.5182]\n",
      "[Epoch 1/500] [Batch 218/312] [D IS TRAINING] [D loss: 0.0348] [G loss: 3.5400]\n",
      "[Epoch 1/500] [Batch 219/312] [D IS TRAINING] [D loss: 0.0392] [G loss: 3.4484]\n",
      "[Epoch 1/500] [Batch 220/312] [D IS TRAINING] [D loss: 0.0399] [G loss: 3.2940]\n",
      "[Epoch 1/500] [Batch 221/312] [D IS TRAINING] [D loss: 0.0370] [G loss: 3.2969]\n",
      "[Epoch 1/500] [Batch 222/312] [D IS TRAINING] [D loss: 0.0348] [G loss: 3.3120]\n",
      "[Epoch 1/500] [Batch 223/312] [D IS TRAINING] [D loss: 0.0464] [G loss: 3.2359]\n",
      "[Epoch 1/500] [Batch 224/312] [D IS TRAINING] [D loss: 0.0377] [G loss: 3.2269]\n",
      "[Epoch 1/500] [Batch 225/312] [D IS TRAINING] [D loss: 0.0287] [G loss: 3.6847]\n",
      "[Epoch 1/500] [Batch 226/312] [D IS TRAINING] [D loss: 0.0258] [G loss: 3.8584]\n",
      "[Epoch 1/500] [Batch 227/312] [D IS TRAINING] [D loss: 0.0266] [G loss: 3.8083]\n",
      "[Epoch 1/500] [Batch 228/312] [D IS TRAINING] [D loss: 0.0214] [G loss: 3.9385]\n",
      "[Epoch 1/500] [Batch 229/312] [D IS TRAINING] [D loss: 0.0215] [G loss: 3.9739]\n",
      "[Epoch 1/500] [Batch 230/312] [D IS TRAINING] [D loss: 0.0250] [G loss: 3.9861]\n",
      "[Epoch 1/500] [Batch 231/312] [D IS TRAINING] [D loss: 0.0224] [G loss: 3.9687]\n",
      "[Epoch 1/500] [Batch 232/312] [D IS TRAINING] [D loss: 0.0187] [G loss: 3.9841]\n",
      "[Epoch 1/500] [Batch 233/312] [D IS TRAINING] [D loss: 0.0146] [G loss: 4.3924]\n",
      "[Epoch 1/500] [Batch 234/312] [D IS TRAINING] [D loss: 0.0128] [G loss: 4.5619]\n",
      "[Epoch 1/500] [Batch 235/312] [D IS TRAINING] [D loss: 0.0124] [G loss: 4.5269]\n",
      "[Epoch 1/500] [Batch 236/312] [D IS TRAINING] [D loss: 0.0127] [G loss: 4.4285]\n",
      "[Epoch 1/500] [Batch 237/312] [D IS TRAINING] [D loss: 0.0127] [G loss: 4.3691]\n",
      "[Epoch 1/500] [Batch 238/312] [D IS TRAINING] [D loss: 0.0144] [G loss: 4.3075]\n",
      "[Epoch 1/500] [Batch 239/312] [D IS TRAINING] [D loss: 0.0150] [G loss: 4.3312]\n",
      "[Epoch 1/500] [Batch 240/312] [D IS TRAINING] [D loss: 0.0153] [G loss: 4.4843]\n",
      "[Epoch 1/500] [Batch 241/312] [D IS TRAINING] [D loss: 0.0155] [G loss: 4.4366]\n",
      "[Epoch 1/500] [Batch 242/312] [D IS TRAINING] [D loss: 0.0132] [G loss: 4.4325]\n",
      "[Epoch 1/500] [Batch 243/312] [D IS TRAINING] [D loss: 0.0140] [G loss: 4.4365]\n",
      "[Epoch 1/500] [Batch 244/312] [D IS TRAINING] [D loss: 0.0144] [G loss: 4.3843]\n",
      "[Epoch 1/500] [Batch 245/312] [D IS TRAINING] [D loss: 0.0142] [G loss: 4.3549]\n",
      "[Epoch 1/500] [Batch 246/312] [D IS TRAINING] [D loss: 0.0115] [G loss: 4.6505]\n",
      "[Epoch 1/500] [Batch 247/312] [D IS TRAINING] [D loss: 0.0121] [G loss: 4.5947]\n",
      "[Epoch 1/500] [Batch 248/312] [D IS TRAINING] [D loss: 0.0119] [G loss: 4.6893]\n",
      "[Epoch 1/500] [Batch 249/312] [D IS TRAINING] [D loss: 0.0106] [G loss: 4.8605]\n",
      "[Epoch 1/500] [Batch 250/312] [D IS TRAINING] [D loss: 0.0105] [G loss: 4.8190]\n",
      "[Epoch 1/500] [Batch 251/312] [D IS TRAINING] [D loss: 0.0126] [G loss: 4.8723]\n",
      "[Epoch 1/500] [Batch 252/312] [D IS TRAINING] [D loss: 0.0104] [G loss: 4.9545]\n",
      "[Epoch 1/500] [Batch 253/312] [D IS TRAINING] [D loss: 0.0092] [G loss: 5.0849]\n",
      "[Epoch 1/500] [Batch 254/312] [D IS TRAINING] [D loss: 0.0090] [G loss: 5.2507]\n",
      "[Epoch 1/500] [Batch 255/312] [D IS TRAINING] [D loss: 0.0085] [G loss: 5.3227]\n",
      "[Epoch 1/500] [Batch 256/312] [D IS TRAINING] [D loss: 0.0074] [G loss: 5.3242]\n",
      "[Epoch 1/500] [Batch 257/312] [D IS TRAINING] [D loss: 0.0098] [G loss: 5.2568]\n",
      "[Epoch 1/500] [Batch 258/312] [D IS TRAINING] [D loss: 0.0090] [G loss: 5.2382]\n",
      "[Epoch 1/500] [Batch 259/312] [D IS TRAINING] [D loss: 0.0075] [G loss: 5.2265]\n",
      "[Epoch 1/500] [Batch 260/312] [D IS TRAINING] [D loss: 0.0074] [G loss: 5.2537]\n",
      "[Epoch 1/500] [Batch 261/312] [D IS TRAINING] [D loss: 0.0067] [G loss: 5.3142]\n",
      "[Epoch 1/500] [Batch 262/312] [D IS TRAINING] [D loss: 0.0076] [G loss: 5.3397]\n",
      "[Epoch 1/500] [Batch 263/312] [D IS TRAINING] [D loss: 0.0067] [G loss: 5.3360]\n",
      "[Epoch 1/500] [Batch 264/312] [D IS TRAINING] [D loss: 0.0062] [G loss: 5.4037]\n",
      "[Epoch 1/500] [Batch 265/312] [D IS TRAINING] [D loss: 0.0064] [G loss: 5.4709]\n",
      "[Epoch 1/500] [Batch 266/312] [D IS TRAINING] [D loss: 0.0069] [G loss: 5.4092]\n",
      "[Epoch 1/500] [Batch 267/312] [D IS TRAINING] [D loss: 0.0064] [G loss: 5.2695]\n",
      "[Epoch 1/500] [Batch 268/312] [D IS TRAINING] [D loss: 0.0071] [G loss: 5.1373]\n",
      "[Epoch 1/500] [Batch 269/312] [D IS TRAINING] [D loss: 0.0070] [G loss: 5.1813]\n",
      "[Epoch 1/500] [Batch 270/312] [D IS TRAINING] [D loss: 0.0061] [G loss: 5.2938]\n",
      "[Epoch 1/500] [Batch 271/312] [D IS TRAINING] [D loss: 0.0062] [G loss: 5.3140]\n",
      "[Epoch 1/500] [Batch 272/312] [D IS TRAINING] [D loss: 0.0067] [G loss: 5.4007]\n",
      "[Epoch 1/500] [Batch 273/312] [D IS TRAINING] [D loss: 0.0070] [G loss: 5.1862]\n",
      "[Epoch 1/500] [Batch 274/312] [D IS TRAINING] [D loss: 0.0075] [G loss: 5.1993]\n",
      "[Epoch 1/500] [Batch 275/312] [D IS TRAINING] [D loss: 0.0067] [G loss: 5.2228]\n",
      "[Epoch 1/500] [Batch 276/312] [D IS TRAINING] [D loss: 0.0071] [G loss: 5.2802]\n",
      "[Epoch 1/500] [Batch 277/312] [D IS TRAINING] [D loss: 0.0063] [G loss: 5.2818]\n",
      "[Epoch 1/500] [Batch 278/312] [D IS TRAINING] [D loss: 0.0065] [G loss: 5.2623]\n",
      "[Epoch 1/500] [Batch 279/312] [D IS TRAINING] [D loss: 0.0061] [G loss: 5.3858]\n",
      "[Epoch 1/500] [Batch 280/312] [D IS TRAINING] [D loss: 0.0054] [G loss: 5.5935]\n",
      "[Epoch 1/500] [Batch 281/312] [D IS TRAINING] [D loss: 0.0063] [G loss: 5.5508]\n",
      "[Epoch 1/500] [Batch 282/312] [D IS TRAINING] [D loss: 0.0055] [G loss: 5.4879]\n",
      "[Epoch 1/500] [Batch 283/312] [D IS TRAINING] [D loss: 0.0051] [G loss: 5.5548]\n",
      "[Epoch 1/500] [Batch 284/312] [D IS TRAINING] [D loss: 0.0050] [G loss: 5.6638]\n",
      "[Epoch 1/500] [Batch 285/312] [D IS TRAINING] [D loss: 0.0042] [G loss: 5.9885]\n",
      "[Epoch 1/500] [Batch 286/312] [D IS TRAINING] [D loss: 0.0046] [G loss: 5.5172]\n",
      "[Epoch 1/500] [Batch 287/312] [D IS TRAINING] [D loss: 0.0067] [G loss: 5.4644]\n",
      "[Epoch 1/500] [Batch 288/312] [D IS TRAINING] [D loss: 0.0050] [G loss: 5.5822]\n",
      "[Epoch 1/500] [Batch 289/312] [D IS TRAINING] [D loss: 0.0049] [G loss: 5.6973]\n",
      "[Epoch 1/500] [Batch 290/312] [D IS TRAINING] [D loss: 0.0053] [G loss: 5.5628]\n",
      "[Epoch 1/500] [Batch 291/312] [D IS TRAINING] [D loss: 0.0056] [G loss: 5.4938]\n",
      "[Epoch 1/500] [Batch 292/312] [D IS TRAINING] [D loss: 0.0054] [G loss: 5.4677]\n",
      "[Epoch 1/500] [Batch 293/312] [D IS TRAINING] [D loss: 0.0057] [G loss: 5.6148]\n",
      "[Epoch 1/500] [Batch 294/312] [D IS TRAINING] [D loss: 0.0049] [G loss: 5.5466]\n",
      "[Epoch 1/500] [Batch 295/312] [D IS TRAINING] [D loss: 0.0066] [G loss: 5.4505]\n",
      "[Epoch 1/500] [Batch 296/312] [D IS TRAINING] [D loss: 0.0050] [G loss: 5.4972]\n",
      "[Epoch 1/500] [Batch 297/312] [D IS TRAINING] [D loss: 0.0050] [G loss: 5.6482]\n",
      "[Epoch 1/500] [Batch 298/312] [D IS TRAINING] [D loss: 0.0053] [G loss: 5.6512]\n",
      "[Epoch 1/500] [Batch 299/312] [D IS TRAINING] [D loss: 0.0050] [G loss: 5.7260]\n",
      "[Epoch 1/500] [Batch 300/312] [D IS TRAINING] [D loss: 0.0040] [G loss: 5.5753]\n",
      "[Epoch 1/500] [Batch 301/312] [D IS TRAINING] [D loss: 0.0050] [G loss: 5.7011]\n",
      "[Epoch 1/500] [Batch 302/312] [D IS TRAINING] [D loss: 0.0053] [G loss: 5.6864]\n",
      "[Epoch 1/500] [Batch 303/312] [D IS TRAINING] [D loss: 0.0046] [G loss: 5.6827]\n",
      "[Epoch 1/500] [Batch 304/312] [D IS TRAINING] [D loss: 0.0048] [G loss: 5.6093]\n",
      "[Epoch 1/500] [Batch 305/312] [D IS TRAINING] [D loss: 0.0042] [G loss: 5.7017]\n",
      "[Epoch 1/500] [Batch 306/312] [D IS TRAINING] [D loss: 0.0044] [G loss: 5.7307]\n",
      "[Epoch 1/500] [Batch 307/312] [D IS TRAINING] [D loss: 0.0057] [G loss: 5.5212]\n",
      "[Epoch 1/500] [Batch 308/312] [D IS TRAINING] [D loss: 0.0045] [G loss: 5.4591]\n",
      "[Epoch 1/500] [Batch 309/312] [D IS TRAINING] [D loss: 0.0054] [G loss: 5.5965]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/500] [Batch 310/312] [D IS TRAINING] [D loss: 0.0047] [G loss: 5.6579]\n",
      "[Epoch 1/500] [Batch 311/312] [D IS TRAINING] [D loss: 0.0037] [G loss: 5.7694]\n",
      "[Epoch 2/500] [Batch 0/312] [D IS TRAINING] [D loss: 0.0045] [G loss: 5.5326]\n",
      "[Epoch 2/500] [Batch 1/312] [D IS TRAINING] [D loss: 0.0042] [G loss: 5.4022]\n",
      "[Epoch 2/500] [Batch 2/312] [D IS TRAINING] [D loss: 0.0048] [G loss: 5.5514]\n",
      "[Epoch 2/500] [Batch 3/312] [D IS TRAINING] [D loss: 0.0042] [G loss: 5.8134]\n",
      "[Epoch 2/500] [Batch 4/312] [D IS TRAINING] [D loss: 0.0046] [G loss: 5.7613]\n",
      "[Epoch 2/500] [Batch 5/312] [D IS TRAINING] [D loss: 0.0049] [G loss: 5.6686]\n",
      "[Epoch 2/500] [Batch 6/312] [D IS TRAINING] [D loss: 0.0042] [G loss: 5.6047]\n",
      "[Epoch 2/500] [Batch 7/312] [D IS TRAINING] [D loss: 0.0040] [G loss: 5.7397]\n",
      "[Epoch 2/500] [Batch 8/312] [D IS TRAINING] [D loss: 0.0038] [G loss: 5.8406]\n",
      "[Epoch 2/500] [Batch 9/312] [D IS TRAINING] [D loss: 0.0039] [G loss: 5.9317]\n",
      "[Epoch 2/500] [Batch 10/312] [D IS TRAINING] [D loss: 0.0041] [G loss: 5.8672]\n",
      "[Epoch 2/500] [Batch 11/312] [D IS TRAINING] [D loss: 0.0036] [G loss: 5.8891]\n",
      "[Epoch 2/500] [Batch 12/312] [D IS TRAINING] [D loss: 0.0033] [G loss: 5.8663]\n",
      "[Epoch 2/500] [Batch 13/312] [D IS TRAINING] [D loss: 0.0037] [G loss: 5.9253]\n",
      "[Epoch 2/500] [Batch 14/312] [D IS TRAINING] [D loss: 0.0045] [G loss: 5.9757]\n",
      "[Epoch 2/500] [Batch 15/312] [D IS TRAINING] [D loss: 0.0036] [G loss: 5.9687]\n",
      "[Epoch 2/500] [Batch 16/312] [D IS TRAINING] [D loss: 0.0032] [G loss: 5.9296]\n",
      "[Epoch 2/500] [Batch 17/312] [D IS TRAINING] [D loss: 0.0036] [G loss: 5.9415]\n",
      "[Epoch 2/500] [Batch 18/312] [D IS TRAINING] [D loss: 0.0035] [G loss: 5.8811]\n",
      "[Epoch 2/500] [Batch 19/312] [D IS TRAINING] [D loss: 0.0032] [G loss: 5.8641]\n",
      "[Epoch 2/500] [Batch 20/312] [D IS TRAINING] [D loss: 0.0039] [G loss: 5.9177]\n",
      "[Epoch 2/500] [Batch 21/312] [D IS TRAINING] [D loss: 0.0031] [G loss: 6.0218]\n",
      "[Epoch 2/500] [Batch 22/312] [D IS TRAINING] [D loss: 0.0030] [G loss: 6.0339]\n",
      "[Epoch 2/500] [Batch 23/312] [D IS TRAINING] [D loss: 0.0034] [G loss: 6.0766]\n",
      "[Epoch 2/500] [Batch 24/312] [D IS TRAINING] [D loss: 0.0031] [G loss: 5.9309]\n",
      "[Epoch 2/500] [Batch 25/312] [D IS TRAINING] [D loss: 0.0032] [G loss: 5.9597]\n",
      "[Epoch 2/500] [Batch 26/312] [D IS TRAINING] [D loss: 0.0033] [G loss: 5.8908]\n",
      "[Epoch 2/500] [Batch 27/312] [D IS TRAINING] [D loss: 0.0034] [G loss: 5.9851]\n",
      "[Epoch 2/500] [Batch 28/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 6.0721]\n",
      "[Epoch 2/500] [Batch 29/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.3652]\n",
      "[Epoch 2/500] [Batch 30/312] [D IS TRAINING] [D loss: 0.0035] [G loss: 5.9451]\n",
      "[Epoch 2/500] [Batch 31/312] [D IS TRAINING] [D loss: 0.0031] [G loss: 5.8880]\n",
      "[Epoch 2/500] [Batch 32/312] [D IS TRAINING] [D loss: 0.0033] [G loss: 5.8437]\n",
      "[Epoch 2/500] [Batch 33/312] [D IS TRAINING] [D loss: 0.0037] [G loss: 5.8144]\n",
      "[Epoch 2/500] [Batch 34/312] [D IS TRAINING] [D loss: 0.0032] [G loss: 5.8808]\n",
      "[Epoch 2/500] [Batch 35/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 6.0569]\n",
      "[Epoch 2/500] [Batch 36/312] [D IS TRAINING] [D loss: 0.0031] [G loss: 5.9658]\n",
      "[Epoch 2/500] [Batch 37/312] [D IS TRAINING] [D loss: 0.0033] [G loss: 5.7859]\n",
      "[Epoch 2/500] [Batch 38/312] [D IS TRAINING] [D loss: 0.0033] [G loss: 5.7905]\n",
      "[Epoch 2/500] [Batch 39/312] [D IS TRAINING] [D loss: 0.0030] [G loss: 5.9408]\n",
      "[Epoch 2/500] [Batch 40/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 6.1150]\n",
      "[Epoch 2/500] [Batch 41/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 5.9921]\n",
      "[Epoch 2/500] [Batch 42/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 5.9018]\n",
      "[Epoch 2/500] [Batch 43/312] [D IS TRAINING] [D loss: 0.0032] [G loss: 5.9321]\n",
      "[Epoch 2/500] [Batch 44/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 6.0757]\n",
      "[Epoch 2/500] [Batch 45/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.3423]\n",
      "[Epoch 2/500] [Batch 46/312] [D IS TRAINING] [D loss: 0.0030] [G loss: 6.1661]\n",
      "[Epoch 2/500] [Batch 47/312] [D IS TRAINING] [D loss: 0.0034] [G loss: 6.0050]\n",
      "[Epoch 2/500] [Batch 48/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 5.9343]\n",
      "[Epoch 2/500] [Batch 49/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.1374]\n",
      "[Epoch 2/500] [Batch 50/312] [D IS TRAINING] [D loss: 0.0022] [G loss: 6.3525]\n",
      "[Epoch 2/500] [Batch 51/312] [D IS TRAINING] [D loss: 0.0021] [G loss: 6.3362]\n",
      "[Epoch 2/500] [Batch 52/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 6.1591]\n",
      "[Epoch 2/500] [Batch 53/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 6.0707]\n",
      "[Epoch 2/500] [Batch 54/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 6.0355]\n",
      "[Epoch 2/500] [Batch 55/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.1393]\n",
      "[Epoch 2/500] [Batch 56/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.2838]\n",
      "[Epoch 2/500] [Batch 57/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.4371]\n",
      "[Epoch 2/500] [Batch 58/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 6.1342]\n",
      "[Epoch 2/500] [Batch 59/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.0387]\n",
      "[Epoch 2/500] [Batch 60/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.0514]\n",
      "[Epoch 2/500] [Batch 61/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.1211]\n",
      "[Epoch 2/500] [Batch 62/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 6.0477]\n",
      "[Epoch 2/500] [Batch 63/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.0167]\n",
      "[Epoch 2/500] [Batch 64/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 5.9737]\n",
      "[Epoch 2/500] [Batch 65/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 5.9859]\n",
      "[Epoch 2/500] [Batch 66/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.0133]\n",
      "[Epoch 2/500] [Batch 67/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.0608]\n",
      "[Epoch 2/500] [Batch 68/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 6.0588]\n",
      "[Epoch 2/500] [Batch 69/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 6.0789]\n",
      "[Epoch 2/500] [Batch 70/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 6.0294]\n",
      "[Epoch 2/500] [Batch 71/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 6.0447]\n",
      "[Epoch 2/500] [Batch 72/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.1321]\n",
      "[Epoch 2/500] [Batch 73/312] [D IS TRAINING] [D loss: 0.0026] [G loss: 6.2382]\n",
      "[Epoch 2/500] [Batch 74/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 6.2752]\n",
      "[Epoch 2/500] [Batch 75/312] [D IS TRAINING] [D loss: 0.0021] [G loss: 6.3076]\n",
      "[Epoch 2/500] [Batch 76/312] [D IS TRAINING] [D loss: 0.0022] [G loss: 6.2887]\n",
      "[Epoch 2/500] [Batch 77/312] [D IS TRAINING] [D loss: 0.0022] [G loss: 6.2449]\n",
      "[Epoch 2/500] [Batch 78/312] [D IS TRAINING] [D loss: 0.0021] [G loss: 6.2048]\n",
      "[Epoch 2/500] [Batch 79/312] [D IS TRAINING] [D loss: 0.0022] [G loss: 6.2152]\n",
      "[Epoch 2/500] [Batch 80/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.2006]\n",
      "[Epoch 2/500] [Batch 81/312] [D IS TRAINING] [D loss: 0.0021] [G loss: 6.2057]\n",
      "[Epoch 2/500] [Batch 82/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.2076]\n",
      "[Epoch 2/500] [Batch 83/312] [D IS TRAINING] [D loss: 0.0022] [G loss: 6.2337]\n",
      "[Epoch 2/500] [Batch 84/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.2177]\n",
      "[Epoch 2/500] [Batch 85/312] [D IS TRAINING] [D loss: 0.0019] [G loss: 6.1992]\n",
      "[Epoch 2/500] [Batch 86/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.1759]\n",
      "[Epoch 2/500] [Batch 87/312] [D IS TRAINING] [D loss: 0.0022] [G loss: 6.1550]\n",
      "[Epoch 2/500] [Batch 88/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.1226]\n",
      "[Epoch 2/500] [Batch 89/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.1171]\n",
      "[Epoch 2/500] [Batch 90/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.0584]\n",
      "[Epoch 2/500] [Batch 91/312] [D IS TRAINING] [D loss: 0.0021] [G loss: 6.1036]\n",
      "[Epoch 2/500] [Batch 92/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.1076]\n",
      "[Epoch 2/500] [Batch 93/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.1356]\n",
      "[Epoch 2/500] [Batch 94/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.1600]\n",
      "[Epoch 2/500] [Batch 95/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.1755]\n",
      "[Epoch 2/500] [Batch 96/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.2186]\n",
      "[Epoch 2/500] [Batch 97/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 6.2131]\n",
      "[Epoch 2/500] [Batch 98/312] [D IS TRAINING] [D loss: 0.0021] [G loss: 6.2013]\n",
      "[Epoch 2/500] [Batch 99/312] [D IS TRAINING] [D loss: 0.0021] [G loss: 6.1828]\n",
      "[Epoch 2/500] [Batch 100/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 6.2246]\n",
      "[Epoch 2/500] [Batch 101/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.1956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/500] [Batch 102/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.2536]\n",
      "[Epoch 2/500] [Batch 103/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.2636]\n",
      "[Epoch 2/500] [Batch 104/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.3243]\n",
      "[Epoch 2/500] [Batch 105/312] [D IS TRAINING] [D loss: 0.0019] [G loss: 6.3350]\n",
      "[Epoch 2/500] [Batch 106/312] [D IS TRAINING] [D loss: 0.0017] [G loss: 6.3656]\n",
      "[Epoch 2/500] [Batch 107/312] [D IS TRAINING] [D loss: 0.0019] [G loss: 6.3818]\n",
      "[Epoch 2/500] [Batch 108/312] [D IS TRAINING] [D loss: 0.0017] [G loss: 6.4162]\n",
      "[Epoch 2/500] [Batch 109/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.4302]\n",
      "[Epoch 2/500] [Batch 110/312] [D IS TRAINING] [D loss: 0.0017] [G loss: 6.4571]\n",
      "[Epoch 2/500] [Batch 111/312] [D IS TRAINING] [D loss: 0.0021] [G loss: 6.4795]\n",
      "[Epoch 2/500] [Batch 112/312] [D IS TRAINING] [D loss: 0.0017] [G loss: 6.4479]\n",
      "[Epoch 2/500] [Batch 113/312] [D IS TRAINING] [D loss: 0.0015] [G loss: 6.4363]\n",
      "[Epoch 2/500] [Batch 114/312] [D IS TRAINING] [D loss: 0.0013] [G loss: 6.4652]\n",
      "[Epoch 2/500] [Batch 115/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.4871]\n",
      "[Epoch 2/500] [Batch 116/312] [D IS TRAINING] [D loss: 0.0019] [G loss: 6.5078]\n",
      "[Epoch 2/500] [Batch 117/312] [D IS TRAINING] [D loss: 0.0020] [G loss: 6.4614]\n",
      "[Epoch 2/500] [Batch 118/312] [D IS TRAINING] [D loss: 0.0018] [G loss: 6.4341]\n",
      "[Epoch 2/500] [Batch 119/312] [D IS TRAINING] [D loss: 0.0018] [G loss: 6.4607]\n",
      "[Epoch 2/500] [Batch 120/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.4983]\n",
      "[Epoch 2/500] [Batch 121/312] [D IS TRAINING] [D loss: 0.0018] [G loss: 6.5583]\n",
      "[Epoch 2/500] [Batch 122/312] [D IS TRAINING] [D loss: 0.0018] [G loss: 6.5012]\n",
      "[Epoch 2/500] [Batch 123/312] [D IS TRAINING] [D loss: 0.0015] [G loss: 6.4263]\n",
      "[Epoch 2/500] [Batch 124/312] [D IS TRAINING] [D loss: 0.0019] [G loss: 6.4153]\n",
      "[Epoch 2/500] [Batch 125/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.4797]\n",
      "[Epoch 2/500] [Batch 126/312] [D IS TRAINING] [D loss: 0.0014] [G loss: 6.5375]\n",
      "[Epoch 2/500] [Batch 127/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.6050]\n",
      "[Epoch 2/500] [Batch 128/312] [D IS TRAINING] [D loss: 0.0018] [G loss: 6.4473]\n",
      "[Epoch 2/500] [Batch 129/312] [D IS TRAINING] [D loss: 0.0018] [G loss: 6.4300]\n",
      "[Epoch 2/500] [Batch 130/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.3647]\n",
      "[Epoch 2/500] [Batch 131/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.4887]\n",
      "[Epoch 2/500] [Batch 132/312] [D IS TRAINING] [D loss: 0.0015] [G loss: 6.4592]\n",
      "[Epoch 2/500] [Batch 133/312] [D IS TRAINING] [D loss: 0.0019] [G loss: 6.4690]\n",
      "[Epoch 2/500] [Batch 134/312] [D IS TRAINING] [D loss: 0.0017] [G loss: 6.2898]\n",
      "[Epoch 2/500] [Batch 135/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.2941]\n",
      "[Epoch 2/500] [Batch 136/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.4237]\n",
      "[Epoch 2/500] [Batch 137/312] [D IS TRAINING] [D loss: 0.0014] [G loss: 6.5883]\n",
      "[Epoch 2/500] [Batch 138/312] [D IS TRAINING] [D loss: 0.0013] [G loss: 6.6832]\n",
      "[Epoch 2/500] [Batch 139/312] [D IS TRAINING] [D loss: 0.0018] [G loss: 6.5865]\n",
      "[Epoch 2/500] [Batch 140/312] [D IS TRAINING] [D loss: 0.0015] [G loss: 6.6634]\n",
      "[Epoch 2/500] [Batch 141/312] [D IS TRAINING] [D loss: 0.0018] [G loss: 6.5281]\n",
      "[Epoch 2/500] [Batch 142/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.6493]\n",
      "[Epoch 2/500] [Batch 143/312] [D IS TRAINING] [D loss: 0.0017] [G loss: 6.5568]\n",
      "[Epoch 2/500] [Batch 144/312] [D IS TRAINING] [D loss: 0.0014] [G loss: 6.6590]\n",
      "[Epoch 2/500] [Batch 145/312] [D IS TRAINING] [D loss: 0.0014] [G loss: 6.6006]\n",
      "[Epoch 2/500] [Batch 146/312] [D IS TRAINING] [D loss: 0.0013] [G loss: 6.6507]\n",
      "[Epoch 2/500] [Batch 147/312] [D IS TRAINING] [D loss: 0.0013] [G loss: 6.8631]\n",
      "[Epoch 2/500] [Batch 148/312] [D IS TRAINING] [D loss: 0.0014] [G loss: 6.9508]\n",
      "[Epoch 2/500] [Batch 149/312] [D IS TRAINING] [D loss: 0.0015] [G loss: 6.6855]\n",
      "[Epoch 2/500] [Batch 150/312] [D IS TRAINING] [D loss: 0.0014] [G loss: 6.6582]\n",
      "[Epoch 2/500] [Batch 151/312] [D IS TRAINING] [D loss: 0.0015] [G loss: 6.6107]\n",
      "[Epoch 2/500] [Batch 152/312] [D IS TRAINING] [D loss: 0.0011] [G loss: 7.1804]\n",
      "[Epoch 2/500] [Batch 153/312] [D IS TRAINING] [D loss: 0.0014] [G loss: 6.8848]\n",
      "[Epoch 2/500] [Batch 154/312] [D IS TRAINING] [D loss: 0.0013] [G loss: 6.9001]\n",
      "[Epoch 2/500] [Batch 155/312] [D IS TRAINING] [D loss: 0.0011] [G loss: 7.0154]\n",
      "[Epoch 2/500] [Batch 156/312] [D IS TRAINING] [D loss: 0.0011] [G loss: 7.2162]\n",
      "[Epoch 2/500] [Batch 157/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.5360]\n",
      "[Epoch 2/500] [Batch 158/312] [D IS TRAINING] [D loss: 0.0015] [G loss: 6.7087]\n",
      "[Epoch 2/500] [Batch 159/312] [D IS TRAINING] [D loss: 0.0012] [G loss: 7.3779]\n",
      "[Epoch 2/500] [Batch 160/312] [D IS TRAINING] [D loss: 0.0012] [G loss: 6.8662]\n",
      "[Epoch 2/500] [Batch 161/312] [D IS TRAINING] [D loss: 0.0013] [G loss: 6.7896]\n",
      "[Epoch 2/500] [Batch 162/312] [D IS TRAINING] [D loss: 0.0011] [G loss: 7.0439]\n",
      "[Epoch 2/500] [Batch 163/312] [D IS TRAINING] [D loss: 0.0015] [G loss: 6.8677]\n",
      "[Epoch 2/500] [Batch 164/312] [D IS TRAINING] [D loss: 0.0014] [G loss: 6.6323]\n",
      "[Epoch 2/500] [Batch 165/312] [D IS TRAINING] [D loss: 0.0014] [G loss: 7.3427]\n",
      "[Epoch 2/500] [Batch 166/312] [D IS TRAINING] [D loss: 0.0030] [G loss: 5.6087]\n",
      "[Epoch 2/500] [Batch 167/312] [D IS TRAINING] [D loss: 0.0009] [G loss: 8.1346]\n",
      "[Epoch 2/500] [Batch 168/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 5.8456]\n",
      "[Epoch 2/500] [Batch 169/312] [D IS TRAINING] [D loss: 0.0015] [G loss: 8.5733]\n",
      "[Epoch 2/500] [Batch 170/312] [D IS TRAINING] [D loss: 0.0031] [G loss: 5.7689]\n",
      "[Epoch 2/500] [Batch 171/312] [D IS TRAINING] [D loss: 0.0014] [G loss: 9.0599]\n",
      "[Epoch 2/500] [Batch 172/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.2752]\n",
      "[Epoch 2/500] [Batch 173/312] [D IS TRAINING] [D loss: 0.0014] [G loss: 7.3190]\n",
      "[Epoch 2/500] [Batch 174/312] [D IS TRAINING] [D loss: 0.0040] [G loss: 5.2559]\n",
      "[Epoch 2/500] [Batch 175/312] [D IS TRAINING] [D loss: 0.0031] [G loss: 11.1610]\n",
      "[Epoch 2/500] [Batch 176/312] [D IS TRAINING] [D loss: 0.0013] [G loss: 11.2880]\n",
      "[Epoch 2/500] [Batch 177/312] [D IS TRAINING] [D loss: 0.0009] [G loss: 10.4434]\n",
      "[Epoch 2/500] [Batch 178/312] [D IS TRAINING] [D loss: 0.0009] [G loss: 9.5127]\n",
      "[Epoch 2/500] [Batch 179/312] [D IS TRAINING] [D loss: 0.0008] [G loss: 8.5273]\n",
      "[Epoch 2/500] [Batch 180/312] [D IS TRAINING] [D loss: 0.0012] [G loss: 6.8330]\n",
      "[Epoch 2/500] [Batch 181/312] [D IS TRAINING] [D loss: 0.0241] [G loss: 3.3068]\n",
      "[Epoch 2/500] [Batch 182/312] [D IS TRAINING] [D loss: 2.1089] [G loss: 10.8443]\n",
      "[Epoch 2/500] [Batch 183/312] [D IS TRAINING] [D loss: 0.4877] [G loss: 3.4529]\n",
      "[Epoch 2/500] [Batch 184/312] [D IS TRAINING] [D loss: 0.4337] [G loss: 1.5480]\n",
      "[Epoch 2/500] [Batch 185/312] [D IS TRAINING] [D loss: 0.4226] [G loss: 4.4349]\n",
      "[Epoch 2/500] [Batch 186/312] [D IS TRAINING] [D loss: 0.1963] [G loss: 3.0685]\n",
      "[Epoch 2/500] [Batch 187/312] [D IS TRAINING] [D loss: 0.1070] [G loss: 2.2389]\n",
      "[Epoch 2/500] [Batch 188/312] [D IS TRAINING] [D loss: 0.1066] [G loss: 2.4231]\n",
      "[Epoch 2/500] [Batch 189/312] [D IS TRAINING] [D loss: 0.0872] [G loss: 2.4547]\n",
      "[Epoch 2/500] [Batch 190/312] [D IS TRAINING] [D loss: 0.0882] [G loss: 2.4632]\n",
      "[Epoch 2/500] [Batch 191/312] [D IS TRAINING] [D loss: 0.0847] [G loss: 2.6314]\n",
      "[Epoch 2/500] [Batch 192/312] [D IS TRAINING] [D loss: 0.0778] [G loss: 2.6042]\n",
      "[Epoch 2/500] [Batch 193/312] [D IS TRAINING] [D loss: 0.0700] [G loss: 2.7686]\n",
      "[Epoch 2/500] [Batch 194/312] [D IS TRAINING] [D loss: 0.0874] [G loss: 3.3819]\n",
      "[Epoch 2/500] [Batch 195/312] [D IS TRAINING] [D loss: 0.1188] [G loss: 2.2698]\n",
      "[Epoch 2/500] [Batch 196/312] [D IS TRAINING] [D loss: 0.2887] [G loss: 6.3049]\n",
      "[Epoch 2/500] [Batch 197/312] [D IS TRAINING] [D loss: 0.3516] [G loss: 1.0907]\n",
      "[Epoch 2/500] [Batch 198/312] [D IS TRAINING] [D loss: 0.5143] [G loss: 8.7605]\n",
      "[Epoch 2/500] [Batch 199/312] [D IS TRAINING] [D loss: 0.0577] [G loss: 7.0918]\n",
      "[Epoch 2/500] [Batch 200/312] [D IS TRAINING] [D loss: 0.0203] [G loss: 5.7627]\n",
      "[Epoch 2/500] [Batch 201/312] [D IS TRAINING] [D loss: 0.0172] [G loss: 4.7838]\n",
      "[Epoch 2/500] [Batch 202/312] [D IS TRAINING] [D loss: 0.0176] [G loss: 4.0935]\n",
      "[Epoch 2/500] [Batch 203/312] [D IS TRAINING] [D loss: 0.0146] [G loss: 4.2620]\n",
      "[Epoch 2/500] [Batch 204/312] [D IS TRAINING] [D loss: 0.0133] [G loss: 4.6852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/500] [Batch 205/312] [D IS TRAINING] [D loss: 0.0147] [G loss: 4.7388]\n",
      "[Epoch 2/500] [Batch 206/312] [D IS TRAINING] [D loss: 0.0126] [G loss: 4.3630]\n",
      "[Epoch 2/500] [Batch 207/312] [D IS TRAINING] [D loss: 0.0149] [G loss: 4.1164]\n",
      "[Epoch 2/500] [Batch 208/312] [D IS TRAINING] [D loss: 0.0113] [G loss: 4.4184]\n",
      "[Epoch 2/500] [Batch 209/312] [D IS TRAINING] [D loss: 0.0127] [G loss: 4.4726]\n",
      "[Epoch 2/500] [Batch 210/312] [D IS TRAINING] [D loss: 0.0155] [G loss: 4.4154]\n",
      "[Epoch 2/500] [Batch 211/312] [D IS TRAINING] [D loss: 0.0151] [G loss: 4.0488]\n",
      "[Epoch 2/500] [Batch 212/312] [D IS TRAINING] [D loss: 0.0128] [G loss: 4.4771]\n",
      "[Epoch 2/500] [Batch 213/312] [D IS TRAINING] [D loss: 0.0142] [G loss: 4.4891]\n",
      "[Epoch 2/500] [Batch 214/312] [D IS TRAINING] [D loss: 0.0160] [G loss: 4.2076]\n",
      "[Epoch 2/500] [Batch 215/312] [D IS TRAINING] [D loss: 0.0203] [G loss: 3.8149]\n",
      "[Epoch 2/500] [Batch 216/312] [D IS TRAINING] [D loss: 0.0189] [G loss: 4.1388]\n",
      "[Epoch 2/500] [Batch 217/312] [D IS TRAINING] [D loss: 0.0134] [G loss: 4.2523]\n",
      "[Epoch 2/500] [Batch 218/312] [D IS TRAINING] [D loss: 0.0121] [G loss: 4.4423]\n",
      "[Epoch 2/500] [Batch 219/312] [D IS TRAINING] [D loss: 0.0137] [G loss: 4.5901]\n",
      "[Epoch 2/500] [Batch 220/312] [D IS TRAINING] [D loss: 0.0107] [G loss: 4.4103]\n",
      "[Epoch 2/500] [Batch 221/312] [D IS TRAINING] [D loss: 0.0091] [G loss: 4.6035]\n",
      "[Epoch 2/500] [Batch 222/312] [D IS TRAINING] [D loss: 0.0081] [G loss: 4.8293]\n",
      "[Epoch 2/500] [Batch 223/312] [D IS TRAINING] [D loss: 0.0082] [G loss: 4.8637]\n",
      "[Epoch 2/500] [Batch 224/312] [D IS TRAINING] [D loss: 0.0085] [G loss: 4.9499]\n",
      "[Epoch 2/500] [Batch 225/312] [D IS TRAINING] [D loss: 0.0072] [G loss: 5.1380]\n",
      "[Epoch 2/500] [Batch 226/312] [D IS TRAINING] [D loss: 0.0071] [G loss: 5.3814]\n",
      "[Epoch 2/500] [Batch 227/312] [D IS TRAINING] [D loss: 0.0054] [G loss: 5.4340]\n",
      "[Epoch 2/500] [Batch 228/312] [D IS TRAINING] [D loss: 0.0067] [G loss: 5.3720]\n",
      "[Epoch 2/500] [Batch 229/312] [D IS TRAINING] [D loss: 0.0061] [G loss: 5.3302]\n",
      "[Epoch 2/500] [Batch 230/312] [D IS TRAINING] [D loss: 0.0065] [G loss: 5.3341]\n",
      "[Epoch 2/500] [Batch 231/312] [D IS TRAINING] [D loss: 0.0065] [G loss: 5.4578]\n",
      "[Epoch 2/500] [Batch 232/312] [D IS TRAINING] [D loss: 0.0072] [G loss: 5.4933]\n",
      "[Epoch 2/500] [Batch 233/312] [D IS TRAINING] [D loss: 0.0069] [G loss: 5.4868]\n",
      "[Epoch 2/500] [Batch 234/312] [D IS TRAINING] [D loss: 0.0064] [G loss: 5.4485]\n",
      "[Epoch 2/500] [Batch 235/312] [D IS TRAINING] [D loss: 0.0068] [G loss: 5.3964]\n",
      "[Epoch 2/500] [Batch 236/312] [D IS TRAINING] [D loss: 0.0074] [G loss: 5.3217]\n",
      "[Epoch 2/500] [Batch 237/312] [D IS TRAINING] [D loss: 0.0071] [G loss: 5.3515]\n",
      "[Epoch 2/500] [Batch 238/312] [D IS TRAINING] [D loss: 0.0061] [G loss: 5.3995]\n",
      "[Epoch 2/500] [Batch 239/312] [D IS TRAINING] [D loss: 0.0061] [G loss: 5.3590]\n",
      "[Epoch 2/500] [Batch 240/312] [D IS TRAINING] [D loss: 0.0064] [G loss: 5.3487]\n",
      "[Epoch 2/500] [Batch 241/312] [D IS TRAINING] [D loss: 0.0070] [G loss: 5.3113]\n",
      "[Epoch 2/500] [Batch 242/312] [D IS TRAINING] [D loss: 0.0075] [G loss: 5.3723]\n",
      "[Epoch 2/500] [Batch 243/312] [D IS TRAINING] [D loss: 0.0069] [G loss: 5.4461]\n",
      "[Epoch 2/500] [Batch 244/312] [D IS TRAINING] [D loss: 0.0059] [G loss: 5.4816]\n",
      "[Epoch 2/500] [Batch 245/312] [D IS TRAINING] [D loss: 0.0053] [G loss: 5.5023]\n",
      "[Epoch 2/500] [Batch 246/312] [D IS TRAINING] [D loss: 0.0071] [G loss: 5.5532]\n",
      "[Epoch 2/500] [Batch 247/312] [D IS TRAINING] [D loss: 0.0054] [G loss: 5.5589]\n",
      "[Epoch 2/500] [Batch 248/312] [D IS TRAINING] [D loss: 0.0066] [G loss: 5.5400]\n",
      "[Epoch 2/500] [Batch 249/312] [D IS TRAINING] [D loss: 0.0062] [G loss: 5.5095]\n",
      "[Epoch 2/500] [Batch 250/312] [D IS TRAINING] [D loss: 0.0056] [G loss: 5.5524]\n",
      "[Epoch 2/500] [Batch 251/312] [D IS TRAINING] [D loss: 0.0060] [G loss: 5.5774]\n",
      "[Epoch 2/500] [Batch 252/312] [D IS TRAINING] [D loss: 0.0055] [G loss: 5.5900]\n",
      "[Epoch 2/500] [Batch 253/312] [D IS TRAINING] [D loss: 0.0058] [G loss: 5.6196]\n",
      "[Epoch 2/500] [Batch 254/312] [D IS TRAINING] [D loss: 0.0054] [G loss: 5.6417]\n",
      "[Epoch 2/500] [Batch 255/312] [D IS TRAINING] [D loss: 0.0050] [G loss: 5.6251]\n",
      "[Epoch 2/500] [Batch 256/312] [D IS TRAINING] [D loss: 0.0058] [G loss: 5.5949]\n",
      "[Epoch 2/500] [Batch 257/312] [D IS TRAINING] [D loss: 0.0054] [G loss: 5.5617]\n",
      "[Epoch 2/500] [Batch 258/312] [D IS TRAINING] [D loss: 0.0049] [G loss: 5.5354]\n",
      "[Epoch 2/500] [Batch 259/312] [D IS TRAINING] [D loss: 0.0046] [G loss: 5.4867]\n",
      "[Epoch 2/500] [Batch 260/312] [D IS TRAINING] [D loss: 0.0046] [G loss: 5.4316]\n",
      "[Epoch 2/500] [Batch 261/312] [D IS TRAINING] [D loss: 0.0048] [G loss: 5.3126]\n",
      "[Epoch 2/500] [Batch 262/312] [D IS TRAINING] [D loss: 0.0052] [G loss: 5.2192]\n",
      "[Epoch 2/500] [Batch 263/312] [D IS TRAINING] [D loss: 0.0050] [G loss: 5.1490]\n",
      "[Epoch 2/500] [Batch 264/312] [D IS TRAINING] [D loss: 0.0053] [G loss: 5.0986]\n",
      "[Epoch 2/500] [Batch 265/312] [D IS TRAINING] [D loss: 0.0053] [G loss: 5.0713]\n",
      "[Epoch 2/500] [Batch 266/312] [D IS TRAINING] [D loss: 0.0055] [G loss: 5.0392]\n",
      "[Epoch 2/500] [Batch 267/312] [D IS TRAINING] [D loss: 0.0058] [G loss: 4.9810]\n",
      "[Epoch 2/500] [Batch 268/312] [D IS TRAINING] [D loss: 0.0058] [G loss: 4.8991]\n",
      "[Epoch 2/500] [Batch 269/312] [D IS TRAINING] [D loss: 0.0062] [G loss: 4.8465]\n",
      "[Epoch 2/500] [Batch 270/312] [D IS TRAINING] [D loss: 0.0068] [G loss: 4.8311]\n",
      "[Epoch 2/500] [Batch 271/312] [D IS TRAINING] [D loss: 0.0060] [G loss: 4.8736]\n",
      "[Epoch 2/500] [Batch 272/312] [D IS TRAINING] [D loss: 0.0055] [G loss: 4.9768]\n",
      "[Epoch 2/500] [Batch 273/312] [D IS TRAINING] [D loss: 0.0058] [G loss: 5.0846]\n",
      "[Epoch 2/500] [Batch 274/312] [D IS TRAINING] [D loss: 0.0055] [G loss: 5.1179]\n",
      "[Epoch 2/500] [Batch 275/312] [D IS TRAINING] [D loss: 0.0050] [G loss: 5.1223]\n",
      "[Epoch 2/500] [Batch 276/312] [D IS TRAINING] [D loss: 0.0049] [G loss: 5.1406]\n",
      "[Epoch 2/500] [Batch 277/312] [D IS TRAINING] [D loss: 0.0056] [G loss: 5.1211]\n",
      "[Epoch 2/500] [Batch 278/312] [D IS TRAINING] [D loss: 0.0053] [G loss: 5.0398]\n",
      "[Epoch 2/500] [Batch 279/312] [D IS TRAINING] [D loss: 0.0071] [G loss: 5.0016]\n",
      "[Epoch 2/500] [Batch 280/312] [D IS TRAINING] [D loss: 0.0070] [G loss: 4.9539]\n",
      "[Epoch 2/500] [Batch 281/312] [D IS TRAINING] [D loss: 0.0063] [G loss: 4.9840]\n",
      "[Epoch 2/500] [Batch 282/312] [D IS TRAINING] [D loss: 0.0075] [G loss: 4.9908]\n",
      "[Epoch 2/500] [Batch 283/312] [D IS TRAINING] [D loss: 0.0076] [G loss: 4.9120]\n",
      "[Epoch 2/500] [Batch 284/312] [D IS TRAINING] [D loss: 0.0074] [G loss: 4.8560]\n",
      "[Epoch 2/500] [Batch 285/312] [D IS TRAINING] [D loss: 0.0070] [G loss: 4.9080]\n",
      "[Epoch 2/500] [Batch 286/312] [D IS TRAINING] [D loss: 0.0059] [G loss: 5.0018]\n",
      "[Epoch 2/500] [Batch 287/312] [D IS TRAINING] [D loss: 0.0062] [G loss: 5.1170]\n",
      "[Epoch 2/500] [Batch 288/312] [D IS TRAINING] [D loss: 0.0054] [G loss: 5.2175]\n",
      "[Epoch 2/500] [Batch 289/312] [D IS TRAINING] [D loss: 0.0059] [G loss: 5.1706]\n",
      "[Epoch 2/500] [Batch 290/312] [D IS TRAINING] [D loss: 0.0054] [G loss: 5.1372]\n",
      "[Epoch 2/500] [Batch 291/312] [D IS TRAINING] [D loss: 0.0061] [G loss: 5.2581]\n",
      "[Epoch 2/500] [Batch 292/312] [D IS TRAINING] [D loss: 0.0051] [G loss: 5.3366]\n",
      "[Epoch 2/500] [Batch 293/312] [D IS TRAINING] [D loss: 0.0037] [G loss: 5.5820]\n",
      "[Epoch 2/500] [Batch 294/312] [D IS TRAINING] [D loss: 0.0044] [G loss: 5.8175]\n",
      "[Epoch 2/500] [Batch 295/312] [D IS TRAINING] [D loss: 0.0069] [G loss: 4.9129]\n",
      "[Epoch 2/500] [Batch 296/312] [D IS TRAINING] [D loss: 0.0036] [G loss: 5.6916]\n",
      "[Epoch 2/500] [Batch 297/312] [D IS TRAINING] [D loss: 0.0047] [G loss: 5.6100]\n",
      "[Epoch 2/500] [Batch 298/312] [D IS TRAINING] [D loss: 0.0033] [G loss: 5.9245]\n",
      "[Epoch 2/500] [Batch 299/312] [D IS TRAINING] [D loss: 0.0058] [G loss: 5.2213]\n",
      "[Epoch 2/500] [Batch 300/312] [D IS TRAINING] [D loss: 0.0022] [G loss: 6.5053]\n",
      "[Epoch 2/500] [Batch 301/312] [D IS TRAINING] [D loss: 0.0102] [G loss: 4.4179]\n",
      "[Epoch 2/500] [Batch 302/312] [D IS TRAINING] [D loss: 0.0042] [G loss: 6.6783]\n",
      "[Epoch 2/500] [Batch 303/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 6.7969]\n",
      "[Epoch 2/500] [Batch 304/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 6.5462]\n",
      "[Epoch 2/500] [Batch 305/312] [D IS TRAINING] [D loss: 0.0051] [G loss: 5.0114]\n",
      "[Epoch 2/500] [Batch 306/312] [D IS TRAINING] [D loss: 0.0018] [G loss: 7.8591]\n",
      "[Epoch 2/500] [Batch 307/312] [D IS TRAINING] [D loss: 0.0093] [G loss: 4.3643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2/500] [Batch 308/312] [D IS TRAINING] [D loss: 0.0031] [G loss: 9.2923]\n",
      "[Epoch 2/500] [Batch 309/312] [D IS TRAINING] [D loss: 0.0027] [G loss: 7.9387]\n",
      "[Epoch 2/500] [Batch 310/312] [D IS TRAINING] [D loss: 0.0039] [G loss: 6.2392]\n",
      "[Epoch 2/500] [Batch 311/312] [D IS TRAINING] [D loss: 0.0024] [G loss: 6.2856]\n",
      "[Epoch 3/500] [Batch 0/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 6.3472]\n",
      "[Epoch 3/500] [Batch 1/312] [D IS TRAINING] [D loss: 0.0030] [G loss: 6.2273]\n",
      "[Epoch 3/500] [Batch 2/312] [D IS TRAINING] [D loss: 0.0013] [G loss: 7.0148]\n",
      "[Epoch 3/500] [Batch 3/312] [D IS TRAINING] [D loss: 0.0054] [G loss: 4.9138]\n",
      "[Epoch 3/500] [Batch 4/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 9.2366]\n",
      "[Epoch 3/500] [Batch 5/312] [D IS TRAINING] [D loss: 0.0030] [G loss: 8.5077]\n",
      "[Epoch 3/500] [Batch 6/312] [D IS TRAINING] [D loss: 0.0058] [G loss: 5.0726]\n",
      "[Epoch 3/500] [Batch 7/312] [D IS TRAINING] [D loss: 0.0056] [G loss: 7.3810]\n",
      "[Epoch 3/500] [Batch 8/312] [D IS TRAINING] [D loss: 0.0048] [G loss: 5.1168]\n",
      "[Epoch 3/500] [Batch 9/312] [D IS TRAINING] [D loss: 0.0022] [G loss: 8.0613]\n",
      "[Epoch 3/500] [Batch 10/312] [D IS TRAINING] [D loss: 0.0021] [G loss: 6.4168]\n",
      "[Epoch 3/500] [Batch 11/312] [D IS TRAINING] [D loss: 0.0055] [G loss: 4.7916]\n",
      "[Epoch 3/500] [Batch 12/312] [D IS TRAINING] [D loss: 0.0017] [G loss: 8.6113]\n",
      "[Epoch 3/500] [Batch 13/312] [D IS TRAINING] [D loss: 0.0045] [G loss: 7.1656]\n",
      "[Epoch 3/500] [Batch 14/312] [D IS TRAINING] [D loss: 0.0035] [G loss: 5.4852]\n",
      "[Epoch 3/500] [Batch 15/312] [D IS TRAINING] [D loss: 0.0015] [G loss: 8.5643]\n",
      "[Epoch 3/500] [Batch 16/312] [D IS TRAINING] [D loss: 0.0035] [G loss: 5.6672]\n",
      "[Epoch 3/500] [Batch 17/312] [D IS TRAINING] [D loss: 0.0015] [G loss: 9.1058]\n",
      "[Epoch 3/500] [Batch 18/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 8.8172]\n",
      "[Epoch 3/500] [Batch 19/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.4290]\n",
      "[Epoch 3/500] [Batch 20/312] [D IS TRAINING] [D loss: 0.0037] [G loss: 5.2864]\n",
      "[Epoch 3/500] [Batch 21/312] [D IS TRAINING] [D loss: 0.0028] [G loss: 8.4409]\n",
      "[Epoch 3/500] [Batch 22/312] [D IS TRAINING] [D loss: 0.0029] [G loss: 5.7069]\n",
      "[Epoch 3/500] [Batch 23/312] [D IS TRAINING] [D loss: 0.0011] [G loss: 9.1518]\n",
      "[Epoch 3/500] [Batch 24/312] [D IS TRAINING] [D loss: 0.0013] [G loss: 8.6116]\n",
      "[Epoch 3/500] [Batch 25/312] [D IS TRAINING] [D loss: 0.0025] [G loss: 5.8729]\n",
      "[Epoch 3/500] [Batch 26/312] [D IS TRAINING] [D loss: 0.0010] [G loss: 7.5271]\n",
      "[Epoch 3/500] [Batch 27/312] [D IS TRAINING] [D loss: 0.0023] [G loss: 5.7045]\n",
      "[Epoch 3/500] [Batch 28/312] [D IS TRAINING] [D loss: 0.0017] [G loss: 10.0014]\n",
      "[Epoch 3/500] [Batch 29/312] [D IS TRAINING] [D loss: 0.0010] [G loss: 10.0144]\n",
      "[Epoch 3/500] [Batch 30/312] [D IS TRAINING] [D loss: 0.0007] [G loss: 8.7700]\n",
      "[Epoch 3/500] [Batch 31/312] [D IS TRAINING] [D loss: 0.0016] [G loss: 6.8857]\n",
      "[Epoch 3/500] [Batch 32/312] [D IS TRAINING] [D loss: 0.0100] [G loss: 4.1392]\n",
      "[Epoch 3/500] [Batch 33/312] [D IS TRAINING] [D loss: 0.0264] [G loss: 10.3306]\n",
      "[Epoch 3/500] [Batch 34/312] [D IS TRAINING] [D loss: 0.0278] [G loss: 9.6502]\n",
      "[Epoch 3/500] [Batch 35/312] [D IS TRAINING] [D loss: 0.1689] [G loss: 1.6118]\n",
      "[Epoch 3/500] [Batch 36/312] [D IS TRAINING] [D loss: 1.8662] [G loss: 10.4208]\n",
      "[Epoch 3/500] [Batch 37/312] [D IS TRAINING] [D loss: 0.3705] [G loss: 6.1704]\n",
      "[Epoch 3/500] [Batch 38/312] [D IS TRAINING] [D loss: 0.1713] [G loss: 3.2949]\n",
      "[Epoch 3/500] [Batch 39/312] [D IS TRAINING] [D loss: 0.1174] [G loss: 2.0933]\n",
      "[Epoch 3/500] [Batch 40/312] [D IS TRAINING] [D loss: 0.0763] [G loss: 2.5039]\n",
      "[Epoch 3/500] [Batch 41/312] [D IS TRAINING] [D loss: 0.0478] [G loss: 3.1058]\n",
      "[Epoch 3/500] [Batch 42/312] [D IS TRAINING] [D loss: 0.0363] [G loss: 3.6497]\n",
      "[Epoch 3/500] [Batch 43/312] [D IS TRAINING] [D loss: 0.0505] [G loss: 3.6375]\n",
      "[Epoch 3/500] [Batch 44/312] [D IS TRAINING] [D loss: 0.0350] [G loss: 3.2546]\n",
      "[Epoch 3/500] [Batch 45/312] [D IS TRAINING] [D loss: 0.0341] [G loss: 3.2500]\n",
      "[Epoch 3/500] [Batch 46/312] [D IS TRAINING] [D loss: 0.0343] [G loss: 3.5579]\n",
      "[Epoch 3/500] [Batch 47/312] [D IS TRAINING] [D loss: 0.0303] [G loss: 3.6238]\n",
      "[Epoch 3/500] [Batch 48/312] [D IS TRAINING] [D loss: 0.0208] [G loss: 3.6671]\n",
      "[Epoch 3/500] [Batch 49/312] [D IS TRAINING] [D loss: 0.0328] [G loss: 3.8252]\n",
      "[Epoch 3/500] [Batch 50/312] [D IS TRAINING] [D loss: 0.0294] [G loss: 3.3533]\n",
      "[Epoch 3/500] [Batch 51/312] [D IS TRAINING] [D loss: 0.0491] [G loss: 2.9249]\n",
      "[Epoch 3/500] [Batch 52/312] [D IS TRAINING] [D loss: 0.0567] [G loss: 2.7066]\n",
      "[Epoch 3/500] [Batch 53/312] [D IS TRAINING] [D loss: 0.0770] [G loss: 3.3741]\n",
      "[Epoch 3/500] [Batch 54/312] [D IS TRAINING] [D loss: 0.2444] [G loss: 1.4259]\n",
      "[Epoch 3/500] [Batch 55/312] [D IS TRAINING] [D loss: 0.8747] [G loss: 8.5972]\n",
      "[Epoch 3/500] [Batch 56/312] [D IS TRAINING] [D loss: 0.1556] [G loss: 2.5471]\n",
      "[Epoch 3/500] [Batch 57/312] [D IS TRAINING] [D loss: 0.2734] [G loss: 1.2306]\n",
      "[Epoch 3/500] [Batch 58/312] [D IS TRAINING] [D loss: 0.1144] [G loss: 4.3938]\n",
      "[Epoch 3/500] [Batch 59/312] [D IS TRAINING] [D loss: 0.0831] [G loss: 4.8580]\n",
      "[Epoch 3/500] [Batch 60/312] [D IS TRAINING] [D loss: 0.0428] [G loss: 3.9460]\n",
      "[Epoch 3/500] [Batch 61/312] [D IS TRAINING] [D loss: 0.0487] [G loss: 3.0083]\n",
      "[Epoch 3/500] [Batch 62/312] [D IS TRAINING] [D loss: 0.0489] [G loss: 3.0466]\n",
      "[Epoch 3/500] [Batch 63/312] [D IS TRAINING] [D loss: 0.0368] [G loss: 3.3738]\n",
      "[Epoch 3/500] [Batch 64/312] [D IS TRAINING] [D loss: 0.0407] [G loss: 3.5680]\n",
      "[Epoch 3/500] [Batch 65/312] [D IS TRAINING] [D loss: 0.0456] [G loss: 3.1635]\n",
      "[Epoch 3/500] [Batch 66/312] [D IS TRAINING] [D loss: 0.0517] [G loss: 2.8978]\n",
      "[Epoch 3/500] [Batch 67/312] [D IS TRAINING] [D loss: 0.0506] [G loss: 3.2068]\n",
      "[Epoch 3/500] [Batch 68/312] [D IS TRAINING] [D loss: 0.0483] [G loss: 3.1762]\n",
      "[Epoch 3/500] [Batch 69/312] [D IS TRAINING] [D loss: 0.0444] [G loss: 3.2468]\n",
      "[Epoch 3/500] [Batch 70/312] [D IS TRAINING] [D loss: 0.0358] [G loss: 3.5000]\n",
      "[Epoch 3/500] [Batch 71/312] [D IS TRAINING] [D loss: 0.0323] [G loss: 3.7400]\n",
      "[Epoch 3/500] [Batch 72/312] [D IS TRAINING] [D loss: 0.0301] [G loss: 3.6257]\n",
      "[Epoch 3/500] [Batch 73/312] [D IS TRAINING] [D loss: 0.0262] [G loss: 3.8186]\n",
      "[Epoch 3/500] [Batch 74/312] [D IS TRAINING] [D loss: 0.0244] [G loss: 3.8231]\n",
      "[Epoch 3/500] [Batch 75/312] [D IS TRAINING] [D loss: 0.0236] [G loss: 3.8727]\n",
      "[Epoch 3/500] [Batch 76/312] [D IS TRAINING] [D loss: 0.0291] [G loss: 3.9319]\n",
      "[Epoch 3/500] [Batch 77/312] [D IS TRAINING] [D loss: 0.0547] [G loss: 3.1968]\n",
      "[Epoch 3/500] [Batch 78/312] [D IS TRAINING] [D loss: 0.0241] [G loss: 3.9710]\n",
      "[Epoch 3/500] [Batch 79/312] [D IS TRAINING] [D loss: 0.0287] [G loss: 3.9214]\n",
      "[Epoch 3/500] [Batch 80/312] [D IS TRAINING] [D loss: 0.0235] [G loss: 4.0479]\n",
      "[Epoch 3/500] [Batch 81/312] [D IS TRAINING] [D loss: 0.0336] [G loss: 4.5425]\n",
      "[Epoch 3/500] [Batch 82/312] [D IS TRAINING] [D loss: 0.1199] [G loss: 1.8276]\n",
      "[Epoch 3/500] [Batch 83/312] [D IS TRAINING] [D loss: 0.8761] [G loss: 9.2874]\n",
      "[Epoch 3/500] [Batch 84/312] [D IS TRAINING] [D loss: 0.1048] [G loss: 2.6450]\n",
      "[Epoch 3/500] [Batch 85/312] [D IS TRAINING] [D loss: 0.1179] [G loss: 2.1793]\n",
      "[Epoch 3/500] [Batch 86/312] [D IS TRAINING] [D loss: 0.0481] [G loss: 3.8103]\n",
      "[Epoch 3/500] [Batch 87/312] [D IS TRAINING] [D loss: 0.0355] [G loss: 4.3034]\n",
      "[Epoch 3/500] [Batch 88/312] [D IS TRAINING] [D loss: 0.0303] [G loss: 4.2461]\n",
      "[Epoch 3/500] [Batch 89/312] [D IS TRAINING] [D loss: 0.0264] [G loss: 4.0171]\n",
      "[Epoch 3/500] [Batch 90/312] [D IS TRAINING] [D loss: 0.0240] [G loss: 3.8086]\n",
      "[Epoch 3/500] [Batch 91/312] [D IS TRAINING] [D loss: 0.0246] [G loss: 3.6622]\n",
      "[Epoch 3/500] [Batch 92/312] [D IS TRAINING] [D loss: 0.0244] [G loss: 3.7499]\n",
      "[Epoch 3/500] [Batch 93/312] [D IS TRAINING] [D loss: 0.0283] [G loss: 3.7637]\n",
      "[Epoch 3/500] [Batch 94/312] [D IS TRAINING] [D loss: 0.0227] [G loss: 3.7022]\n",
      "[Epoch 3/500] [Batch 95/312] [D IS TRAINING] [D loss: 0.0305] [G loss: 3.7092]\n",
      "[Epoch 3/500] [Batch 96/312] [D IS TRAINING] [D loss: 0.0261] [G loss: 3.6922]\n",
      "[Epoch 3/500] [Batch 97/312] [D IS TRAINING] [D loss: 0.0358] [G loss: 3.6077]\n",
      "[Epoch 3/500] [Batch 98/312] [D IS TRAINING] [D loss: 0.0272] [G loss: 3.7128]\n",
      "[Epoch 3/500] [Batch 99/312] [D IS TRAINING] [D loss: 0.0335] [G loss: 3.7342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/500] [Batch 100/312] [D IS TRAINING] [D loss: 0.0286] [G loss: 3.6990]\n",
      "[Epoch 3/500] [Batch 101/312] [D IS TRAINING] [D loss: 0.0315] [G loss: 3.7956]\n",
      "[Epoch 3/500] [Batch 102/312] [D IS TRAINING] [D loss: 0.0285] [G loss: 3.5029]\n",
      "[Epoch 3/500] [Batch 103/312] [D IS TRAINING] [D loss: 0.0225] [G loss: 3.8876]\n",
      "[Epoch 3/500] [Batch 104/312] [D IS TRAINING] [D loss: 0.0357] [G loss: 4.1689]\n",
      "[Epoch 3/500] [Batch 105/312] [D IS TRAINING] [D loss: 0.0417] [G loss: 3.7975]\n",
      "[Epoch 3/500] [Batch 106/312] [D IS TRAINING] [D loss: 0.0383] [G loss: 3.2304]\n",
      "[Epoch 3/500] [Batch 107/312] [D IS TRAINING] [D loss: 0.0185] [G loss: 4.5629]\n",
      "[Epoch 3/500] [Batch 108/312] [D IS TRAINING] [D loss: 0.0268] [G loss: 4.2992]\n",
      "[Epoch 3/500] [Batch 109/312] [D IS TRAINING] [D loss: 0.0338] [G loss: 3.9430]\n",
      "[Epoch 3/500] [Batch 110/312] [D IS TRAINING] [D loss: 0.0360] [G loss: 3.5454]\n",
      "[Epoch 3/500] [Batch 111/312] [D IS TRAINING] [D loss: 0.0380] [G loss: 3.5126]\n",
      "[Epoch 3/500] [Batch 112/312] [D IS TRAINING] [D loss: 0.0260] [G loss: 3.8490]\n",
      "[Epoch 3/500] [Batch 113/312] [D IS TRAINING] [D loss: 0.0234] [G loss: 4.0563]\n",
      "[Epoch 3/500] [Batch 114/312] [D IS TRAINING] [D loss: 0.0223] [G loss: 4.1821]\n",
      "[Epoch 3/500] [Batch 115/312] [D IS TRAINING] [D loss: 0.0235] [G loss: 4.2037]\n",
      "[Epoch 3/500] [Batch 116/312] [D IS TRAINING] [D loss: 0.0169] [G loss: 4.1476]\n",
      "[Epoch 3/500] [Batch 117/312] [D IS TRAINING] [D loss: 0.0204] [G loss: 4.6233]\n",
      "[Epoch 3/500] [Batch 118/312] [D IS TRAINING] [D loss: 0.0155] [G loss: 4.1660]\n",
      "[Epoch 3/500] [Batch 119/312] [D IS TRAINING] [D loss: 0.0116] [G loss: 4.7238]\n",
      "[Epoch 3/500] [Batch 120/312] [D IS TRAINING] [D loss: 0.0124] [G loss: 4.6460]\n",
      "[Epoch 3/500] [Batch 121/312] [D IS TRAINING] [D loss: 0.0161] [G loss: 4.5703]\n",
      "[Epoch 3/500] [Batch 122/312] [D IS TRAINING] [D loss: 0.0178] [G loss: 4.4558]\n",
      "[Epoch 3/500] [Batch 123/312] [D IS TRAINING] [D loss: 0.0132] [G loss: 4.7234]\n",
      "[Epoch 3/500] [Batch 124/312] [D IS TRAINING] [D loss: 0.0101] [G loss: 4.9718]\n",
      "[Epoch 3/500] [Batch 125/312] [D IS TRAINING] [D loss: 0.0116] [G loss: 4.7920]\n",
      "[Epoch 3/500] [Batch 126/312] [D IS TRAINING] [D loss: 0.0075] [G loss: 5.1229]\n",
      "[Epoch 3/500] [Batch 127/312] [D IS TRAINING] [D loss: 0.0354] [G loss: 5.1771]\n",
      "[Epoch 3/500] [Batch 128/312] [D IS TRAINING] [D loss: 0.1444] [G loss: 1.9939]\n",
      "[Epoch 3/500] [Batch 129/312] [D IS TRAINING] [D loss: 0.7614] [G loss: 9.5253]\n",
      "[Epoch 3/500] [Batch 130/312] [D IS TRAINING] [D loss: 0.0944] [G loss: 7.7206]\n",
      "[Epoch 3/500] [Batch 131/312] [D IS TRAINING] [D loss: 0.0246] [G loss: 5.1496]\n",
      "[Epoch 3/500] [Batch 132/312] [D IS TRAINING] [D loss: 0.0557] [G loss: 3.8345]\n",
      "[Epoch 3/500] [Batch 133/312] [D IS TRAINING] [D loss: 0.0356] [G loss: 3.2444]\n",
      "[Epoch 3/500] [Batch 134/312] [D IS TRAINING] [D loss: 0.0246] [G loss: 3.5920]\n",
      "[Epoch 3/500] [Batch 135/312] [D IS TRAINING] [D loss: 0.0216] [G loss: 3.9292]\n",
      "[Epoch 3/500] [Batch 136/312] [D IS TRAINING] [D loss: 0.0188] [G loss: 4.0994]\n",
      "[Epoch 3/500] [Batch 137/312] [D IS TRAINING] [D loss: 0.0267] [G loss: 4.2634]\n",
      "[Epoch 3/500] [Batch 138/312] [D IS TRAINING] [D loss: 0.0188] [G loss: 4.0442]\n",
      "[Epoch 3/500] [Batch 139/312] [D IS TRAINING] [D loss: 0.0269] [G loss: 3.9572]\n",
      "[Epoch 3/500] [Batch 140/312] [D IS TRAINING] [D loss: 0.0214] [G loss: 3.9583]\n",
      "[Epoch 3/500] [Batch 141/312] [D IS TRAINING] [D loss: 0.0213] [G loss: 3.9406]\n",
      "[Epoch 3/500] [Batch 142/312] [D IS TRAINING] [D loss: 0.0235] [G loss: 4.0238]\n",
      "[Epoch 3/500] [Batch 143/312] [D IS TRAINING] [D loss: 0.0217] [G loss: 3.9214]\n",
      "[Epoch 3/500] [Batch 144/312] [D IS TRAINING] [D loss: 0.0217] [G loss: 4.1091]\n",
      "[Epoch 3/500] [Batch 145/312] [D IS TRAINING] [D loss: 0.0519] [G loss: 3.4977]\n",
      "[Epoch 3/500] [Batch 146/312] [D IS TRAINING] [D loss: 0.0390] [G loss: 3.3932]\n",
      "[Epoch 3/500] [Batch 147/312] [D IS TRAINING] [D loss: 0.0468] [G loss: 4.0607]\n",
      "[Epoch 3/500] [Batch 148/312] [D IS TRAINING] [D loss: 0.0332] [G loss: 3.7261]\n",
      "[Epoch 3/500] [Batch 149/312] [D IS TRAINING] [D loss: 0.0149] [G loss: 4.9507]\n",
      "[Epoch 3/500] [Batch 150/312] [D IS TRAINING] [D loss: 0.0228] [G loss: 4.9617]\n",
      "[Epoch 3/500] [Batch 151/312] [D IS TRAINING] [D loss: 0.0225] [G loss: 3.9536]\n",
      "[Epoch 3/500] [Batch 152/312] [D IS TRAINING] [D loss: 0.0226] [G loss: 4.3412]\n",
      "[Epoch 3/500] [Batch 153/312] [D IS TRAINING] [D loss: 0.0276] [G loss: 3.7919]\n",
      "[Epoch 3/500] [Batch 154/312] [D IS TRAINING] [D loss: 0.0325] [G loss: 3.8511]\n",
      "[Epoch 3/500] [Batch 155/312] [D IS TRAINING] [D loss: 0.1029] [G loss: 2.0772]\n",
      "[Epoch 3/500] [Batch 156/312] [D IS TRAINING] [D loss: 2.4077] [G loss: 9.7989]\n",
      "[Epoch 3/500] [Batch 157/312] [D IS TRAINING] [D loss: 2.9562] [G loss: 0.5464]\n",
      "[Epoch 3/500] [Batch 158/312] [D IS TRAINING] [D loss: 0.3327] [G loss: 4.2402]\n",
      "[Epoch 3/500] [Batch 159/312] [D IS TRAINING] [D loss: 0.3058] [G loss: 4.7133]\n",
      "[Epoch 3/500] [Batch 160/312] [D IS TRAINING] [D loss: 0.1399] [G loss: 3.7672]\n",
      "[Epoch 3/500] [Batch 161/312] [D IS TRAINING] [D loss: 0.0916] [G loss: 2.9917]\n",
      "[Epoch 3/500] [Batch 162/312] [D IS TRAINING] [D loss: 0.0971] [G loss: 2.3643]\n",
      "[Epoch 3/500] [Batch 163/312] [D IS TRAINING] [D loss: 0.1564] [G loss: 1.7690]\n",
      "[Epoch 3/500] [Batch 164/312] [D IS TRAINING] [D loss: 0.1872] [G loss: 1.6246]\n",
      "[Epoch 3/500] [Batch 165/312] [D IS TRAINING] [D loss: 0.2227] [G loss: 2.0127]\n",
      "[Epoch 3/500] [Batch 166/312] [D IS TRAINING] [D loss: 0.3230] [G loss: 2.1811]\n",
      "[Epoch 3/500] [Batch 167/312] [D IS TRAINING] [D loss: 0.4631] [G loss: 1.8542]\n",
      "[Epoch 3/500] [Batch 168/312] [D IS TRAINING] [D loss: 0.8760] [G loss: 3.8703]\n",
      "[Epoch 3/500] [Batch 169/312] [D IS TRAINING] [D loss: 0.1701] [G loss: 2.6400]\n",
      "[Epoch 3/500] [Batch 170/312] [D IS TRAINING] [D loss: 0.2121] [G loss: 2.0687]\n",
      "[Epoch 3/500] [Batch 171/312] [D IS TRAINING] [D loss: 0.1635] [G loss: 1.7812]\n",
      "[Epoch 3/500] [Batch 172/312] [D IS TRAINING] [D loss: 0.1698] [G loss: 2.0419]\n",
      "[Epoch 3/500] [Batch 173/312] [D IS TRAINING] [D loss: 0.1637] [G loss: 2.3108]\n",
      "[Epoch 3/500] [Batch 174/312] [D IS TRAINING] [D loss: 0.1774] [G loss: 1.8237]\n",
      "[Epoch 3/500] [Batch 175/312] [D IS TRAINING] [D loss: 0.2372] [G loss: 1.4731]\n",
      "[Epoch 3/500] [Batch 176/312] [D IS TRAINING] [D loss: 0.2492] [G loss: 2.4221]\n",
      "[Epoch 3/500] [Batch 177/312] [D IS TRAINING] [D loss: 0.3046] [G loss: 1.2006]\n",
      "[Epoch 3/500] [Batch 178/312] [D IS TRAINING] [D loss: 0.3708] [G loss: 3.4285]\n",
      "[Epoch 3/500] [Batch 179/312] [D IS TRAINING] [D loss: 0.2629] [G loss: 1.6179]\n",
      "[Epoch 3/500] [Batch 180/312] [D IS TRAINING] [D loss: 0.2149] [G loss: 2.3643]\n",
      "[Epoch 3/500] [Batch 181/312] [D IS TRAINING] [D loss: 0.1957] [G loss: 1.7359]\n",
      "[Epoch 3/500] [Batch 182/312] [D IS TRAINING] [D loss: 0.3172] [G loss: 3.6673]\n",
      "[Epoch 3/500] [Batch 183/312] [D IS TRAINING] [D loss: 0.3995] [G loss: 0.8112]\n",
      "[Epoch 3/500] [Batch 184/312] [D IS TRAINING] [D loss: 0.7760] [G loss: 4.9915]\n",
      "[Epoch 3/500] [Batch 185/312] [D IS TRAINING] [D loss: 0.3389] [G loss: 1.7038]\n",
      "[Epoch 3/500] [Batch 186/312] [D IS TRAINING] [D loss: 0.2630] [G loss: 2.4976]\n",
      "[Epoch 3/500] [Batch 187/312] [D IS TRAINING] [D loss: 0.1971] [G loss: 2.3538]\n",
      "[Epoch 3/500] [Batch 188/312] [D IS TRAINING] [D loss: 0.2017] [G loss: 2.0594]\n",
      "[Epoch 3/500] [Batch 189/312] [D IS TRAINING] [D loss: 0.2057] [G loss: 1.8479]\n",
      "[Epoch 3/500] [Batch 190/312] [D IS TRAINING] [D loss: 0.2721] [G loss: 2.8139]\n",
      "[Epoch 3/500] [Batch 191/312] [D IS TRAINING] [D loss: 0.3475] [G loss: 1.4349]\n",
      "[Epoch 3/500] [Batch 192/312] [D IS TRAINING] [D loss: 0.5604] [G loss: 4.4551]\n",
      "[Epoch 3/500] [Batch 193/312] [D IS TRAINING] [D loss: 0.2272] [G loss: 1.9503]\n",
      "[Epoch 3/500] [Batch 194/312] [D IS TRAINING] [D loss: 0.2455] [G loss: 1.6749]\n",
      "[Epoch 3/500] [Batch 195/312] [D IS TRAINING] [D loss: 0.2550] [G loss: 3.1799]\n",
      "[Epoch 3/500] [Batch 196/312] [D IS TRAINING] [D loss: 0.2270] [G loss: 1.4965]\n",
      "[Epoch 3/500] [Batch 197/312] [D IS TRAINING] [D loss: 0.2532] [G loss: 2.5694]\n",
      "[Epoch 3/500] [Batch 198/312] [D IS TRAINING] [D loss: 0.2741] [G loss: 1.3946]\n",
      "[Epoch 3/500] [Batch 199/312] [D IS TRAINING] [D loss: 0.3162] [G loss: 2.5733]\n",
      "[Epoch 3/500] [Batch 200/312] [D IS TRAINING] [D loss: 0.3280] [G loss: 1.0289]\n",
      "[Epoch 3/500] [Batch 201/312] [D IS TRAINING] [D loss: 0.6720] [G loss: 3.7764]\n",
      "[Epoch 3/500] [Batch 202/312] [D IS TRAINING] [D loss: 0.9217] [G loss: 0.7239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/500] [Batch 203/312] [D IS TRAINING] [D loss: 0.7967] [G loss: 3.4320]\n",
      "[Epoch 3/500] [Batch 204/312] [D IS TRAINING] [D loss: 0.4164] [G loss: 1.6460]\n",
      "[Epoch 3/500] [Batch 205/312] [D IS TRAINING] [D loss: 0.5197] [G loss: 0.8816]\n",
      "[Epoch 3/500] [Batch 206/312] [D IS TRAINING] [D loss: 0.3726] [G loss: 1.6189]\n",
      "[Epoch 3/500] [Batch 207/312] [D IS TRAINING] [D loss: 0.3627] [G loss: 1.9340]\n",
      "[Epoch 3/500] [Batch 208/312] [D IS TRAINING] [D loss: 0.3324] [G loss: 1.1598]\n",
      "[Epoch 3/500] [Batch 209/312] [D IS TRAINING] [D loss: 0.2540] [G loss: 1.9078]\n",
      "[Epoch 3/500] [Batch 210/312] [D IS TRAINING] [D loss: 0.1992] [G loss: 1.7783]\n",
      "[Epoch 3/500] [Batch 211/312] [D IS TRAINING] [D loss: 0.2148] [G loss: 1.7258]\n",
      "[Epoch 3/500] [Batch 212/312] [D IS TRAINING] [D loss: 0.2003] [G loss: 2.2144]\n",
      "[Epoch 3/500] [Batch 213/312] [D IS TRAINING] [D loss: 0.2235] [G loss: 1.5379]\n",
      "[Epoch 3/500] [Batch 214/312] [D IS TRAINING] [D loss: 0.2610] [G loss: 2.6001]\n",
      "[Epoch 3/500] [Batch 215/312] [D IS TRAINING] [D loss: 0.2770] [G loss: 1.1481]\n",
      "[Epoch 3/500] [Batch 216/312] [D IS TRAINING] [D loss: 0.5120] [G loss: 3.3483]\n",
      "[Epoch 3/500] [Batch 217/312] [D IS TRAINING] [D loss: 0.5732] [G loss: 0.5191]\n",
      "[Epoch 3/500] [Batch 218/312] [D IS TRAINING] [D loss: 0.8262] [G loss: 3.7678]\n",
      "[Epoch 3/500] [Batch 219/312] [D IS TRAINING] [D loss: 0.4066] [G loss: 1.1486]\n",
      "[Epoch 3/500] [Batch 220/312] [D IS TRAINING] [D loss: 0.4214] [G loss: 1.0868]\n",
      "[Epoch 3/500] [Batch 221/312] [D IS TRAINING] [D loss: 0.3424] [G loss: 1.6485]\n",
      "[Epoch 3/500] [Batch 222/312] [D IS TRAINING] [D loss: 0.3351] [G loss: 1.3830]\n",
      "[Epoch 3/500] [Batch 223/312] [D IS TRAINING] [D loss: 0.3084] [G loss: 1.6723]\n",
      "[Epoch 3/500] [Batch 224/312] [D IS TRAINING] [D loss: 0.3241] [G loss: 1.4434]\n",
      "[Epoch 3/500] [Batch 225/312] [D IS TRAINING] [D loss: 0.2947] [G loss: 2.2877]\n",
      "[Epoch 3/500] [Batch 226/312] [D IS TRAINING] [D loss: 0.3155] [G loss: 1.1745]\n",
      "[Epoch 3/500] [Batch 227/312] [D IS TRAINING] [D loss: 0.3290] [G loss: 2.7854]\n",
      "[Epoch 3/500] [Batch 228/312] [D IS TRAINING] [D loss: 0.4215] [G loss: 1.0351]\n",
      "[Epoch 3/500] [Batch 229/312] [D IS TRAINING] [D loss: 0.3764] [G loss: 2.7651]\n",
      "[Epoch 3/500] [Batch 230/312] [D IS TRAINING] [D loss: 0.5723] [G loss: 0.6225]\n",
      "[Epoch 3/500] [Batch 231/312] [D IS TRAINING] [D loss: 0.9491] [G loss: 4.4337]\n",
      "[Epoch 3/500] [Batch 232/312] [D IS TRAINING] [D loss: 0.3459] [G loss: 1.5708]\n",
      "[Epoch 3/500] [Batch 233/312] [D IS TRAINING] [D loss: 0.3974] [G loss: 0.8398]\n",
      "[Epoch 3/500] [Batch 234/312] [D IS TRAINING] [D loss: 0.2637] [G loss: 2.0048]\n",
      "[Epoch 3/500] [Batch 235/312] [D IS TRAINING] [D loss: 0.2359] [G loss: 1.9610]\n",
      "[Epoch 3/500] [Batch 236/312] [D IS TRAINING] [D loss: 0.3069] [G loss: 1.2662]\n",
      "[Epoch 3/500] [Batch 237/312] [D IS TRAINING] [D loss: 0.2372] [G loss: 1.8194]\n",
      "[Epoch 3/500] [Batch 238/312] [D IS TRAINING] [D loss: 0.2428] [G loss: 1.6883]\n",
      "[Epoch 3/500] [Batch 239/312] [D IS TRAINING] [D loss: 0.2570] [G loss: 1.6371]\n",
      "[Epoch 3/500] [Batch 240/312] [D IS TRAINING] [D loss: 0.2730] [G loss: 1.2867]\n",
      "[Epoch 3/500] [Batch 241/312] [D IS TRAINING] [D loss: 0.2970] [G loss: 2.5087]\n",
      "[Epoch 3/500] [Batch 242/312] [D IS TRAINING] [D loss: 0.3208] [G loss: 1.0345]\n",
      "[Epoch 3/500] [Batch 243/312] [D IS TRAINING] [D loss: 0.4212] [G loss: 2.9949]\n",
      "[Epoch 3/500] [Batch 244/312] [D IS TRAINING] [D loss: 0.2457] [G loss: 1.5418]\n",
      "[Epoch 3/500] [Batch 245/312] [D IS TRAINING] [D loss: 0.2770] [G loss: 1.4212]\n",
      "[Epoch 3/500] [Batch 246/312] [D IS TRAINING] [D loss: 0.3273] [G loss: 3.0429]\n",
      "[Epoch 3/500] [Batch 247/312] [D IS TRAINING] [D loss: 0.2501] [G loss: 1.2774]\n",
      "[Epoch 3/500] [Batch 248/312] [D IS TRAINING] [D loss: 0.2780] [G loss: 2.7075]\n",
      "[Epoch 3/500] [Batch 249/312] [D IS TRAINING] [D loss: 0.2372] [G loss: 1.4406]\n",
      "[Epoch 3/500] [Batch 250/312] [D IS TRAINING] [D loss: 0.1440] [G loss: 3.1897]\n",
      "[Epoch 3/500] [Batch 251/312] [D IS TRAINING] [D loss: 0.1270] [G loss: 3.1446]\n",
      "[Epoch 3/500] [Batch 252/312] [D IS TRAINING] [D loss: 0.2529] [G loss: 1.3019]\n",
      "[Epoch 3/500] [Batch 253/312] [D IS TRAINING] [D loss: 0.3097] [G loss: 3.5389]\n",
      "[Epoch 3/500] [Batch 254/312] [D IS TRAINING] [D loss: 0.2631] [G loss: 1.3269]\n",
      "[Epoch 3/500] [Batch 255/312] [D IS TRAINING] [D loss: 0.2325] [G loss: 3.5776]\n",
      "[Epoch 3/500] [Batch 256/312] [D IS TRAINING] [D loss: 0.1980] [G loss: 1.9606]\n",
      "[Epoch 3/500] [Batch 257/312] [D IS TRAINING] [D loss: 0.1584] [G loss: 2.8657]\n",
      "[Epoch 3/500] [Batch 258/312] [D IS TRAINING] [D loss: 0.2781] [G loss: 1.1704]\n",
      "[Epoch 3/500] [Batch 259/312] [D IS TRAINING] [D loss: 0.3624] [G loss: 3.2949]\n",
      "[Epoch 3/500] [Batch 260/312] [D IS TRAINING] [D loss: 0.1502] [G loss: 1.8161]\n",
      "[Epoch 3/500] [Batch 261/312] [D IS TRAINING] [D loss: 0.1548] [G loss: 1.9335]\n",
      "[Epoch 3/500] [Batch 262/312] [D IS TRAINING] [D loss: 0.1860] [G loss: 3.1188]\n",
      "[Epoch 3/500] [Batch 263/312] [D IS TRAINING] [D loss: 0.0740] [G loss: 3.1389]\n",
      "[Epoch 3/500] [Batch 264/312] [D IS TRAINING] [D loss: 0.3205] [G loss: 1.0830]\n",
      "[Epoch 3/500] [Batch 265/312] [D IS TRAINING] [D loss: 0.8857] [G loss: 6.2483]\n",
      "[Epoch 3/500] [Batch 266/312] [D IS TRAINING] [D loss: 0.2840] [G loss: 1.9752]\n",
      "[Epoch 3/500] [Batch 267/312] [D IS TRAINING] [D loss: 0.2384] [G loss: 3.0637]\n",
      "[Epoch 3/500] [Batch 268/312] [D IS TRAINING] [D loss: 0.1941] [G loss: 1.5814]\n",
      "[Epoch 3/500] [Batch 269/312] [D IS TRAINING] [D loss: 0.1359] [G loss: 3.6867]\n",
      "[Epoch 3/500] [Batch 270/312] [D IS TRAINING] [D loss: 0.1748] [G loss: 2.1568]\n",
      "[Epoch 3/500] [Batch 271/312] [D IS TRAINING] [D loss: 0.1161] [G loss: 2.3641]\n",
      "[Epoch 3/500] [Batch 272/312] [D IS TRAINING] [D loss: 0.1412] [G loss: 3.8012]\n",
      "[Epoch 3/500] [Batch 273/312] [D IS TRAINING] [D loss: 0.1688] [G loss: 1.8809]\n",
      "[Epoch 3/500] [Batch 274/312] [D IS TRAINING] [D loss: 0.2844] [G loss: 3.8410]\n",
      "[Epoch 3/500] [Batch 275/312] [D IS TRAINING] [D loss: 0.0851] [G loss: 4.2095]\n",
      "[Epoch 3/500] [Batch 276/312] [D IS TRAINING] [D loss: 0.8849] [G loss: 0.4178]\n",
      "[Epoch 3/500] [Batch 277/312] [D IS TRAINING] [D loss: 1.5523] [G loss: 5.6669]\n",
      "[Epoch 3/500] [Batch 278/312] [D IS TRAINING] [D loss: 0.6622] [G loss: 0.8226]\n",
      "[Epoch 3/500] [Batch 279/312] [D IS TRAINING] [D loss: 0.5264] [G loss: 1.4585]\n",
      "[Epoch 3/500] [Batch 280/312] [D IS TRAINING] [D loss: 0.4096] [G loss: 1.2218]\n",
      "[Epoch 3/500] [Batch 281/312] [D IS TRAINING] [D loss: 0.3582] [G loss: 1.4346]\n",
      "[Epoch 3/500] [Batch 282/312] [D IS TRAINING] [D loss: 0.3825] [G loss: 1.3636]\n",
      "[Epoch 3/500] [Batch 283/312] [D IS TRAINING] [D loss: 0.3264] [G loss: 1.3487]\n",
      "[Epoch 3/500] [Batch 284/312] [D IS TRAINING] [D loss: 0.3022] [G loss: 1.4701]\n",
      "[Epoch 3/500] [Batch 285/312] [D IS TRAINING] [D loss: 0.2929] [G loss: 1.8709]\n",
      "[Epoch 3/500] [Batch 286/312] [D IS TRAINING] [D loss: 0.3272] [G loss: 1.1947]\n",
      "[Epoch 3/500] [Batch 287/312] [D IS TRAINING] [D loss: 0.3077] [G loss: 2.9495]\n",
      "[Epoch 3/500] [Batch 288/312] [D IS TRAINING] [D loss: 0.4061] [G loss: 0.8768]\n",
      "[Epoch 3/500] [Batch 289/312] [D IS TRAINING] [D loss: 0.5533] [G loss: 3.2739]\n",
      "[Epoch 3/500] [Batch 290/312] [D IS TRAINING] [D loss: 0.4056] [G loss: 0.8895]\n",
      "[Epoch 3/500] [Batch 291/312] [D IS TRAINING] [D loss: 0.3557] [G loss: 2.1944]\n",
      "[Epoch 3/500] [Batch 292/312] [D IS TRAINING] [D loss: 0.2276] [G loss: 1.8101]\n",
      "[Epoch 3/500] [Batch 293/312] [D IS TRAINING] [D loss: 0.3229] [G loss: 1.3786]\n",
      "[Epoch 3/500] [Batch 294/312] [D IS TRAINING] [D loss: 0.2407] [G loss: 2.0065]\n",
      "[Epoch 3/500] [Batch 295/312] [D IS TRAINING] [D loss: 0.1668] [G loss: 1.8492]\n",
      "[Epoch 3/500] [Batch 296/312] [D IS TRAINING] [D loss: 0.1183] [G loss: 2.5864]\n",
      "[Epoch 3/500] [Batch 297/312] [D IS TRAINING] [D loss: 0.1395] [G loss: 2.0087]\n",
      "[Epoch 3/500] [Batch 298/312] [D IS TRAINING] [D loss: 0.1124] [G loss: 3.0153]\n",
      "[Epoch 3/500] [Batch 299/312] [D IS TRAINING] [D loss: 0.0783] [G loss: 2.7139]\n",
      "[Epoch 3/500] [Batch 300/312] [D IS TRAINING] [D loss: 0.0866] [G loss: 2.2652]\n",
      "[Epoch 3/500] [Batch 301/312] [D IS TRAINING] [D loss: 0.1205] [G loss: 2.8340]\n",
      "[Epoch 3/500] [Batch 302/312] [D IS TRAINING] [D loss: 0.1209] [G loss: 3.1586]\n",
      "[Epoch 3/500] [Batch 303/312] [D IS TRAINING] [D loss: 0.3005] [G loss: 1.4149]\n",
      "[Epoch 3/500] [Batch 304/312] [D IS TRAINING] [D loss: 0.7717] [G loss: 6.5212]\n",
      "[Epoch 3/500] [Batch 305/312] [D IS TRAINING] [D loss: 0.5461] [G loss: 1.1152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/500] [Batch 306/312] [D IS TRAINING] [D loss: 0.3541] [G loss: 2.0346]\n",
      "[Epoch 3/500] [Batch 307/312] [D IS TRAINING] [D loss: 0.1864] [G loss: 2.3596]\n",
      "[Epoch 3/500] [Batch 308/312] [D IS TRAINING] [D loss: 0.2584] [G loss: 1.5113]\n",
      "[Epoch 3/500] [Batch 309/312] [D IS TRAINING] [D loss: 0.3233] [G loss: 2.4232]\n",
      "[Epoch 3/500] [Batch 310/312] [D IS TRAINING] [D loss: 0.4013] [G loss: 0.9767]\n",
      "[Epoch 3/500] [Batch 311/312] [D IS TRAINING] [D loss: 0.7492] [G loss: 3.9867]\n",
      "[Epoch 4/500] [Batch 0/312] [D IS TRAINING] [D loss: 0.5755] [G loss: 0.5779]\n",
      "[Epoch 4/500] [Batch 1/312] [D IS TRAINING] [D loss: 0.4593] [G loss: 3.0301]\n",
      "[Epoch 4/500] [Batch 2/312] [D IS TRAINING] [D loss: 0.2435] [G loss: 1.7149]\n",
      "[Epoch 4/500] [Batch 3/312] [D IS TRAINING] [D loss: 0.2700] [G loss: 1.3409]\n",
      "[Epoch 4/500] [Batch 4/312] [D IS TRAINING] [D loss: 0.2824] [G loss: 1.9324]\n",
      "[Epoch 4/500] [Batch 5/312] [D IS TRAINING] [D loss: 0.2636] [G loss: 1.7743]\n",
      "[Epoch 4/500] [Batch 6/312] [D IS TRAINING] [D loss: 0.2313] [G loss: 1.6716]\n",
      "[Epoch 4/500] [Batch 7/312] [D IS TRAINING] [D loss: 0.2315] [G loss: 1.9212]\n",
      "[Epoch 4/500] [Batch 8/312] [D IS TRAINING] [D loss: 0.2471] [G loss: 2.0469]\n",
      "[Epoch 4/500] [Batch 9/312] [D IS TRAINING] [D loss: 0.3145] [G loss: 1.2889]\n",
      "[Epoch 4/500] [Batch 10/312] [D IS TRAINING] [D loss: 0.3961] [G loss: 2.9341]\n",
      "[Epoch 4/500] [Batch 11/312] [D IS TRAINING] [D loss: 0.6965] [G loss: 0.3907]\n",
      "[Epoch 4/500] [Batch 12/312] [D IS TRAINING] [D loss: 1.4999] [G loss: 6.9075]\n",
      "[Epoch 4/500] [Batch 13/312] [D IS TRAINING] [D loss: 0.3539] [G loss: 3.7013]\n",
      "[Epoch 4/500] [Batch 14/312] [D IS TRAINING] [D loss: 0.4015] [G loss: 1.1402]\n",
      "[Epoch 4/500] [Batch 15/312] [D IS TRAINING] [D loss: 0.2595] [G loss: 1.3154]\n",
      "[Epoch 4/500] [Batch 16/312] [D IS TRAINING] [D loss: 0.1951] [G loss: 2.2628]\n",
      "[Epoch 4/500] [Batch 17/312] [D IS TRAINING] [D loss: 0.1903] [G loss: 2.8263]\n",
      "[Epoch 4/500] [Batch 18/312] [D IS TRAINING] [D loss: 0.1953] [G loss: 1.7978]\n",
      "[Epoch 4/500] [Batch 19/312] [D IS TRAINING] [D loss: 0.2258] [G loss: 1.5140]\n",
      "[Epoch 4/500] [Batch 20/312] [D IS TRAINING] [D loss: 0.2650] [G loss: 1.9582]\n",
      "[Epoch 4/500] [Batch 21/312] [D IS TRAINING] [D loss: 0.2299] [G loss: 1.8734]\n",
      "[Epoch 4/500] [Batch 22/312] [D IS TRAINING] [D loss: 0.2850] [G loss: 1.4055]\n",
      "[Epoch 4/500] [Batch 23/312] [D IS TRAINING] [D loss: 0.4896] [G loss: 2.6560]\n",
      "[Epoch 4/500] [Batch 24/312] [D IS TRAINING] [D loss: 0.5553] [G loss: 0.8983]\n",
      "[Epoch 4/500] [Batch 25/312] [D IS TRAINING] [D loss: 0.5593] [G loss: 2.4934]\n",
      "[Epoch 4/500] [Batch 26/312] [D IS TRAINING] [D loss: 0.3061] [G loss: 1.3284]\n",
      "[Epoch 4/500] [Batch 27/312] [D IS TRAINING] [D loss: 0.2711] [G loss: 1.8881]\n",
      "[Epoch 4/500] [Batch 28/312] [D IS TRAINING] [D loss: 0.2161] [G loss: 1.7388]\n",
      "[Epoch 4/500] [Batch 29/312] [D IS TRAINING] [D loss: 0.2196] [G loss: 2.0497]\n",
      "[Epoch 4/500] [Batch 30/312] [D IS TRAINING] [D loss: 0.1699] [G loss: 2.1753]\n",
      "[Epoch 4/500] [Batch 31/312] [D IS TRAINING] [D loss: 0.1453] [G loss: 1.9801]\n",
      "[Epoch 4/500] [Batch 32/312] [D IS TRAINING] [D loss: 0.1723] [G loss: 2.2553]\n",
      "[Epoch 4/500] [Batch 33/312] [D IS TRAINING] [D loss: 0.1327] [G loss: 2.5580]\n",
      "[Epoch 4/500] [Batch 34/312] [D IS TRAINING] [D loss: 0.2151] [G loss: 1.3745]\n",
      "[Epoch 4/500] [Batch 35/312] [D IS TRAINING] [D loss: 0.3899] [G loss: 4.7392]\n",
      "[Epoch 4/500] [Batch 36/312] [D IS TRAINING] [D loss: 0.3633] [G loss: 0.8886]\n",
      "[Epoch 4/500] [Batch 37/312] [D IS TRAINING] [D loss: 0.5373] [G loss: 4.3787]\n",
      "[Epoch 4/500] [Batch 38/312] [D IS TRAINING] [D loss: 0.2732] [G loss: 1.1539]\n",
      "[Epoch 4/500] [Batch 39/312] [D IS TRAINING] [D loss: 0.2087] [G loss: 2.6860]\n",
      "[Epoch 4/500] [Batch 40/312] [D IS TRAINING] [D loss: 0.1851] [G loss: 1.9724]\n",
      "[Epoch 4/500] [Batch 41/312] [D IS TRAINING] [D loss: 0.1431] [G loss: 2.3915]\n",
      "[Epoch 4/500] [Batch 42/312] [D IS TRAINING] [D loss: 0.1368] [G loss: 2.1910]\n",
      "[Epoch 4/500] [Batch 43/312] [D IS TRAINING] [D loss: 0.1341] [G loss: 2.9762]\n",
      "[Epoch 4/500] [Batch 44/312] [D IS TRAINING] [D loss: 0.0915] [G loss: 2.7981]\n",
      "[Epoch 4/500] [Batch 45/312] [D IS TRAINING] [D loss: 0.1300] [G loss: 1.9113]\n",
      "[Epoch 4/500] [Batch 46/312] [D IS TRAINING] [D loss: 0.1260] [G loss: 2.6666]\n",
      "[Epoch 4/500] [Batch 47/312] [D IS TRAINING] [D loss: 0.0748] [G loss: 5.3709]\n",
      "[Epoch 4/500] [Batch 48/312] [D IS TRAINING] [D loss: 0.0537] [G loss: 2.9837]\n",
      "[Epoch 4/500] [Batch 49/312] [D IS TRAINING] [D loss: 0.0464] [G loss: 2.9136]\n",
      "[Epoch 4/500] [Batch 50/312] [D IS TRAINING] [D loss: 0.1192] [G loss: 1.9508]\n",
      "[Epoch 4/500] [Batch 51/312] [D IS TRAINING] [D loss: 0.1099] [G loss: 4.0271]\n",
      "[Epoch 4/500] [Batch 52/312] [D IS TRAINING] [D loss: 0.0339] [G loss: 3.9460]\n",
      "[Epoch 4/500] [Batch 53/312] [D IS TRAINING] [D loss: 0.0796] [G loss: 2.3383]\n",
      "[Epoch 4/500] [Batch 54/312] [D IS TRAINING] [D loss: 0.0277] [G loss: 4.8852]\n",
      "[Epoch 4/500] [Batch 55/312] [D IS TRAINING] [D loss: 0.0373] [G loss: 4.2764]\n",
      "[Epoch 4/500] [Batch 56/312] [D IS TRAINING] [D loss: 0.0210] [G loss: 5.3996]\n",
      "[Epoch 4/500] [Batch 57/312] [D IS TRAINING] [D loss: 0.0519] [G loss: 2.9266]\n",
      "[Epoch 4/500] [Batch 58/312] [D IS TRAINING] [D loss: 0.0097] [G loss: 6.6769]\n",
      "[Epoch 4/500] [Batch 59/312] [D IS TRAINING] [D loss: 0.0103] [G loss: 6.7696]\n",
      "[Epoch 4/500] [Batch 60/312] [D IS TRAINING] [D loss: 0.0125] [G loss: 5.4869]\n",
      "[Epoch 4/500] [Batch 61/312] [D IS TRAINING] [D loss: 0.0125] [G loss: 4.9538]\n",
      "[Epoch 4/500] [Batch 62/312] [D IS TRAINING] [D loss: 0.0245] [G loss: 4.0865]\n",
      "[Epoch 4/500] [Batch 63/312] [D IS TRAINING] [D loss: 0.0286] [G loss: 3.7435]\n",
      "[Epoch 4/500] [Batch 64/312] [D IS TRAINING] [D loss: 0.0420] [G loss: 3.4678]\n",
      "[Epoch 4/500] [Batch 65/312] [D IS TRAINING] [D loss: 0.0340] [G loss: 3.5524]\n",
      "[Epoch 4/500] [Batch 66/312] [D IS TRAINING] [D loss: 0.0327] [G loss: 3.9656]\n",
      "[Epoch 4/500] [Batch 67/312] [D IS TRAINING] [D loss: 0.0404] [G loss: 4.2160]\n",
      "[Epoch 4/500] [Batch 68/312] [D IS TRAINING] [D loss: 0.0362] [G loss: 3.7521]\n",
      "[Epoch 4/500] [Batch 69/312] [D IS TRAINING] [D loss: 0.0250] [G loss: 3.8946]\n",
      "[Epoch 4/500] [Batch 70/312] [D IS TRAINING] [D loss: 0.0304] [G loss: 3.7275]\n",
      "[Epoch 4/500] [Batch 71/312] [D IS TRAINING] [D loss: 0.0391] [G loss: 3.9866]\n",
      "[Epoch 4/500] [Batch 72/312] [D IS TRAINING] [D loss: 0.0494] [G loss: 3.8064]\n",
      "[Epoch 4/500] [Batch 73/312] [D IS TRAINING] [D loss: 0.0455] [G loss: 3.6507]\n",
      "[Epoch 4/500] [Batch 74/312] [D IS TRAINING] [D loss: 0.0244] [G loss: 4.8825]\n",
      "[Epoch 4/500] [Batch 75/312] [D IS TRAINING] [D loss: 0.0195] [G loss: 4.5546]\n",
      "[Epoch 4/500] [Batch 76/312] [D IS TRAINING] [D loss: 0.0912] [G loss: 2.6647]\n",
      "[Epoch 4/500] [Batch 77/312] [D IS TRAINING] [D loss: 0.1615] [G loss: 5.4147]\n",
      "[Epoch 4/500] [Batch 78/312] [D IS TRAINING] [D loss: 0.2408] [G loss: 5.6045]\n",
      "[Epoch 4/500] [Batch 79/312] [D IS TRAINING] [D loss: 0.1143] [G loss: 2.1271]\n",
      "[Epoch 4/500] [Batch 80/312] [D IS TRAINING] [D loss: 0.1258] [G loss: 4.4202]\n",
      "[Epoch 4/500] [Batch 81/312] [D IS TRAINING] [D loss: 0.0319] [G loss: 3.9387]\n",
      "[Epoch 4/500] [Batch 82/312] [D IS TRAINING] [D loss: 0.0799] [G loss: 2.7580]\n",
      "[Epoch 4/500] [Batch 83/312] [D IS TRAINING] [D loss: 0.0507] [G loss: 5.6432]\n",
      "[Epoch 4/500] [Batch 84/312] [D IS TRAINING] [D loss: 0.0974] [G loss: 2.3461]\n",
      "[Epoch 4/500] [Batch 85/312] [D IS TRAINING] [D loss: 0.0780] [G loss: 6.5874]\n",
      "[Epoch 4/500] [Batch 86/312] [D IS TRAINING] [D loss: 0.0738] [G loss: 2.6559]\n",
      "[Epoch 4/500] [Batch 87/312] [D IS TRAINING] [D loss: 0.0394] [G loss: 5.6039]\n",
      "[Epoch 4/500] [Batch 88/312] [D IS TRAINING] [D loss: 0.0539] [G loss: 2.8188]\n",
      "[Epoch 4/500] [Batch 89/312] [D IS TRAINING] [D loss: 0.0224] [G loss: 8.5770]\n",
      "[Epoch 4/500] [Batch 90/312] [D IS TRAINING] [D loss: 0.0108] [G loss: 6.2459]\n",
      "[Epoch 4/500] [Batch 91/312] [D IS TRAINING] [D loss: 0.0136] [G loss: 5.5806]\n",
      "[Epoch 4/500] [Batch 92/312] [D IS TRAINING] [D loss: 0.0102] [G loss: 4.8820]\n",
      "[Epoch 4/500] [Batch 93/312] [D IS TRAINING] [D loss: 0.0319] [G loss: 3.2673]\n",
      "[Epoch 4/500] [Batch 94/312] [D IS TRAINING] [D loss: 0.0146] [G loss: 5.8068]\n",
      "[Epoch 4/500] [Batch 95/312] [D IS TRAINING] [D loss: 0.0933] [G loss: 2.1796]\n",
      "[Epoch 4/500] [Batch 96/312] [D IS TRAINING] [D loss: 0.4132] [G loss: 9.5980]\n",
      "[Epoch 4/500] [Batch 97/312] [D IS TRAINING] [D loss: 1.0415] [G loss: 0.8867]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/500] [Batch 98/312] [D IS TRAINING] [D loss: 1.0287] [G loss: 7.8682]\n",
      "[Epoch 4/500] [Batch 99/312] [D IS TRAINING] [D loss: 0.1515] [G loss: 2.7176]\n",
      "[Epoch 4/500] [Batch 100/312] [D IS TRAINING] [D loss: 0.2926] [G loss: 1.2567]\n",
      "[Epoch 4/500] [Batch 101/312] [D IS TRAINING] [D loss: 0.4529] [G loss: 2.1703]\n",
      "[Epoch 4/500] [Batch 102/312] [D IS TRAINING] [D loss: 0.4961] [G loss: 1.6770]\n",
      "[Epoch 4/500] [Batch 103/312] [D IS TRAINING] [D loss: 0.4414] [G loss: 1.1766]\n",
      "[Epoch 4/500] [Batch 104/312] [D IS TRAINING] [D loss: 0.4514] [G loss: 1.8836]\n",
      "[Epoch 4/500] [Batch 105/312] [D IS TRAINING] [D loss: 0.4180] [G loss: 1.0823]\n",
      "[Epoch 4/500] [Batch 106/312] [D IS TRAINING] [D loss: 0.4002] [G loss: 2.7874]\n",
      "[Epoch 4/500] [Batch 107/312] [D IS TRAINING] [D loss: 0.2837] [G loss: 1.3114]\n",
      "[Epoch 4/500] [Batch 108/312] [D IS TRAINING] [D loss: 0.2394] [G loss: 1.8976]\n",
      "[Epoch 4/500] [Batch 109/312] [D IS TRAINING] [D loss: 0.2877] [G loss: 1.4502]\n",
      "[Epoch 4/500] [Batch 110/312] [D IS TRAINING] [D loss: 0.2969] [G loss: 2.6006]\n",
      "[Epoch 4/500] [Batch 111/312] [D IS TRAINING] [D loss: 0.2271] [G loss: 1.5428]\n",
      "[Epoch 4/500] [Batch 112/312] [D IS TRAINING] [D loss: 0.2067] [G loss: 2.2279]\n",
      "[Epoch 4/500] [Batch 113/312] [D IS TRAINING] [D loss: 0.2528] [G loss: 2.4940]\n",
      "[Epoch 4/500] [Batch 114/312] [D IS TRAINING] [D loss: 0.3861] [G loss: 0.9430]\n",
      "[Epoch 4/500] [Batch 115/312] [D IS TRAINING] [D loss: 0.8304] [G loss: 4.9849]\n",
      "[Epoch 4/500] [Batch 116/312] [D IS TRAINING] [D loss: 0.4455] [G loss: 0.7998]\n",
      "[Epoch 4/500] [Batch 117/312] [D IS TRAINING] [D loss: 0.1965] [G loss: 2.6760]\n",
      "[Epoch 4/500] [Batch 118/312] [D IS TRAINING] [D loss: 0.2088] [G loss: 2.6671]\n",
      "[Epoch 4/500] [Batch 119/312] [D IS TRAINING] [D loss: 0.2783] [G loss: 1.1005]\n",
      "[Epoch 4/500] [Batch 120/312] [D IS TRAINING] [D loss: 0.2136] [G loss: 2.6795]\n",
      "[Epoch 4/500] [Batch 121/312] [D IS TRAINING] [D loss: 0.2485] [G loss: 2.1233]\n",
      "[Epoch 4/500] [Batch 122/312] [D IS TRAINING] [D loss: 0.5241] [G loss: 0.6008]\n",
      "[Epoch 4/500] [Batch 123/312] [D IS TRAINING] [D loss: 0.8902] [G loss: 4.2517]\n",
      "[Epoch 4/500] [Batch 124/312] [D IS TRAINING] [D loss: 0.3990] [G loss: 1.2792]\n",
      "[Epoch 4/500] [Batch 125/312] [D IS TRAINING] [D loss: 0.4472] [G loss: 0.8007]\n",
      "[Epoch 4/500] [Batch 126/312] [D IS TRAINING] [D loss: 0.3773] [G loss: 2.9585]\n",
      "[Epoch 4/500] [Batch 127/312] [D IS TRAINING] [D loss: 0.1976] [G loss: 2.4892]\n",
      "[Epoch 4/500] [Batch 128/312] [D IS TRAINING] [D loss: 0.2254] [G loss: 1.6609]\n",
      "[Epoch 4/500] [Batch 129/312] [D IS TRAINING] [D loss: 0.1418] [G loss: 2.9859]\n",
      "[Epoch 4/500] [Batch 130/312] [D IS TRAINING] [D loss: 0.1229] [G loss: 3.1265]\n",
      "[Epoch 4/500] [Batch 131/312] [D IS TRAINING] [D loss: 0.1156] [G loss: 2.7186]\n",
      "[Epoch 4/500] [Batch 132/312] [D IS TRAINING] [D loss: 0.1319] [G loss: 2.9397]\n",
      "[Epoch 4/500] [Batch 133/312] [D IS TRAINING] [D loss: 0.1602] [G loss: 1.8944]\n",
      "[Epoch 4/500] [Batch 134/312] [D IS TRAINING] [D loss: 0.2467] [G loss: 2.1277]\n",
      "[Epoch 4/500] [Batch 135/312] [D IS TRAINING] [D loss: 0.4148] [G loss: 2.3840]\n",
      "[Epoch 4/500] [Batch 136/312] [D IS TRAINING] [D loss: 0.5937] [G loss: 1.6639]\n",
      "[Epoch 4/500] [Batch 137/312] [D IS TRAINING] [D loss: 0.4859] [G loss: 3.9783]\n",
      "[Epoch 4/500] [Batch 138/312] [D IS TRAINING] [D loss: 0.2424] [G loss: 1.9313]\n",
      "[Epoch 4/500] [Batch 139/312] [D IS TRAINING] [D loss: 0.2237] [G loss: 2.7136]\n",
      "[Epoch 4/500] [Batch 140/312] [D IS TRAINING] [D loss: 0.1576] [G loss: 1.9209]\n",
      "[Epoch 4/500] [Batch 141/312] [D IS TRAINING] [D loss: 0.2023] [G loss: 2.1682]\n",
      "[Epoch 4/500] [Batch 142/312] [D IS TRAINING] [D loss: 0.1436] [G loss: 2.8080]\n",
      "[Epoch 4/500] [Batch 143/312] [D IS TRAINING] [D loss: 0.3850] [G loss: 0.8307]\n",
      "[Epoch 4/500] [Batch 144/312] [D IS TRAINING] [D loss: 0.8505] [G loss: 5.1834]\n",
      "[Epoch 4/500] [Batch 145/312] [D IS TRAINING] [D loss: 0.6058] [G loss: 0.6829]\n",
      "[Epoch 4/500] [Batch 146/312] [D IS TRAINING] [D loss: 0.8506] [G loss: 4.4420]\n",
      "[Epoch 4/500] [Batch 147/312] [D IS TRAINING] [D loss: 0.4029] [G loss: 1.4581]\n",
      "[Epoch 4/500] [Batch 148/312] [D IS TRAINING] [D loss: 0.3879] [G loss: 1.1415]\n",
      "[Epoch 4/500] [Batch 149/312] [D IS TRAINING] [D loss: 0.3688] [G loss: 2.4663]\n",
      "[Epoch 4/500] [Batch 150/312] [D IS TRAINING] [D loss: 0.2761] [G loss: 1.6343]\n",
      "[Epoch 4/500] [Batch 151/312] [D IS TRAINING] [D loss: 0.2474] [G loss: 1.4966]\n",
      "[Epoch 4/500] [Batch 152/312] [D IS TRAINING] [D loss: 0.2412] [G loss: 2.2197]\n",
      "[Epoch 4/500] [Batch 153/312] [D IS TRAINING] [D loss: 0.2458] [G loss: 1.6316]\n",
      "[Epoch 4/500] [Batch 154/312] [D IS TRAINING] [D loss: 0.2761] [G loss: 1.3231]\n",
      "[Epoch 4/500] [Batch 155/312] [D IS TRAINING] [D loss: 0.2269] [G loss: 2.3699]\n",
      "[Epoch 4/500] [Batch 156/312] [D IS TRAINING] [D loss: 0.2671] [G loss: 1.7400]\n",
      "[Epoch 4/500] [Batch 157/312] [D IS TRAINING] [D loss: 0.2188] [G loss: 1.9285]\n",
      "[Epoch 4/500] [Batch 158/312] [D IS TRAINING] [D loss: 0.1685] [G loss: 1.7910]\n",
      "[Epoch 4/500] [Batch 159/312] [D IS TRAINING] [D loss: 0.1324] [G loss: 2.7927]\n",
      "[Epoch 4/500] [Batch 160/312] [D IS TRAINING] [D loss: 0.1010] [G loss: 2.6218]\n",
      "[Epoch 4/500] [Batch 161/312] [D IS TRAINING] [D loss: 0.0896] [G loss: 2.7741]\n",
      "[Epoch 4/500] [Batch 162/312] [D IS TRAINING] [D loss: 0.1018] [G loss: 2.4229]\n",
      "[Epoch 4/500] [Batch 163/312] [D IS TRAINING] [D loss: 0.1117] [G loss: 2.8530]\n",
      "[Epoch 4/500] [Batch 164/312] [D IS TRAINING] [D loss: 0.1208] [G loss: 2.8600]\n",
      "[Epoch 4/500] [Batch 165/312] [D IS TRAINING] [D loss: 0.1634] [G loss: 2.1924]\n",
      "[Epoch 4/500] [Batch 166/312] [D IS TRAINING] [D loss: 0.2348] [G loss: 2.2916]\n",
      "[Epoch 4/500] [Batch 167/312] [D IS TRAINING] [D loss: 0.3880] [G loss: 1.1012]\n",
      "[Epoch 4/500] [Batch 168/312] [D IS TRAINING] [D loss: 0.8951] [G loss: 5.5518]\n",
      "[Epoch 4/500] [Batch 169/312] [D IS TRAINING] [D loss: 1.4562] [G loss: 0.2610]\n",
      "[Epoch 4/500] [Batch 170/312] [D IS TRAINING] [D loss: 0.7233] [G loss: 3.2941]\n",
      "[Epoch 4/500] [Batch 171/312] [D IS TRAINING] [D loss: 0.4042] [G loss: 1.4562]\n",
      "[Epoch 4/500] [Batch 172/312] [D IS TRAINING] [D loss: 0.3255] [G loss: 1.5553]\n",
      "[Epoch 4/500] [Batch 173/312] [D IS TRAINING] [D loss: 0.3598] [G loss: 1.3019]\n",
      "[Epoch 4/500] [Batch 174/312] [D IS TRAINING] [D loss: 0.3029] [G loss: 1.5540]\n",
      "[Epoch 4/500] [Batch 175/312] [D IS TRAINING] [D loss: 0.3278] [G loss: 1.6651]\n",
      "[Epoch 4/500] [Batch 176/312] [D IS TRAINING] [D loss: 0.2936] [G loss: 1.8317]\n",
      "[Epoch 4/500] [Batch 177/312] [D IS TRAINING] [D loss: 0.5362] [G loss: 0.5844]\n",
      "[Epoch 4/500] [Batch 178/312] [D IS TRAINING] [D loss: 0.8167] [G loss: 4.4941]\n",
      "[Epoch 4/500] [Batch 179/312] [D IS TRAINING] [D loss: 0.3340] [G loss: 1.3455]\n",
      "[Epoch 4/500] [Batch 180/312] [D IS TRAINING] [D loss: 0.2291] [G loss: 1.4807]\n",
      "[Epoch 4/500] [Batch 181/312] [D IS TRAINING] [D loss: 0.2093] [G loss: 2.5923]\n",
      "[Epoch 4/500] [Batch 182/312] [D IS TRAINING] [D loss: 0.2156] [G loss: 1.7343]\n",
      "[Epoch 4/500] [Batch 183/312] [D IS TRAINING] [D loss: 0.2510] [G loss: 1.5805]\n",
      "[Epoch 4/500] [Batch 184/312] [D IS TRAINING] [D loss: 0.3353] [G loss: 1.8800]\n",
      "[Epoch 4/500] [Batch 185/312] [D IS TRAINING] [D loss: 0.4236] [G loss: 0.8861]\n",
      "[Epoch 4/500] [Batch 186/312] [D IS TRAINING] [D loss: 0.3297] [G loss: 2.6186]\n",
      "[Epoch 4/500] [Batch 187/312] [D IS TRAINING] [D loss: 0.2679] [G loss: 1.5041]\n",
      "[Epoch 4/500] [Batch 188/312] [D IS TRAINING] [D loss: 0.2093] [G loss: 2.5535]\n",
      "[Epoch 4/500] [Batch 189/312] [D IS TRAINING] [D loss: 0.1706] [G loss: 1.7844]\n",
      "[Epoch 4/500] [Batch 190/312] [D IS TRAINING] [D loss: 0.2510] [G loss: 1.6322]\n",
      "[Epoch 4/500] [Batch 191/312] [D IS TRAINING] [D loss: 0.2615] [G loss: 2.2962]\n",
      "[Epoch 4/500] [Batch 192/312] [D IS TRAINING] [D loss: 0.3676] [G loss: 1.0708]\n",
      "[Epoch 4/500] [Batch 193/312] [D IS TRAINING] [D loss: 0.4390] [G loss: 3.8936]\n",
      "[Epoch 4/500] [Batch 194/312] [D IS TRAINING] [D loss: 0.2568] [G loss: 1.4508]\n",
      "[Epoch 4/500] [Batch 195/312] [D IS TRAINING] [D loss: 0.1942] [G loss: 1.8123]\n",
      "[Epoch 4/500] [Batch 196/312] [D IS TRAINING] [D loss: 0.1387] [G loss: 2.9364]\n",
      "[Epoch 4/500] [Batch 197/312] [D IS TRAINING] [D loss: 0.1374] [G loss: 2.8834]\n",
      "[Epoch 4/500] [Batch 198/312] [D IS TRAINING] [D loss: 0.2247] [G loss: 1.4929]\n",
      "[Epoch 4/500] [Batch 199/312] [D IS TRAINING] [D loss: 0.2310] [G loss: 2.7833]\n",
      "[Epoch 4/500] [Batch 200/312] [D IS TRAINING] [D loss: 0.3039] [G loss: 1.0743]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/500] [Batch 201/312] [D IS TRAINING] [D loss: 0.3242] [G loss: 3.4459]\n",
      "[Epoch 4/500] [Batch 202/312] [D IS TRAINING] [D loss: 0.2788] [G loss: 1.2094]\n",
      "[Epoch 4/500] [Batch 203/312] [D IS TRAINING] [D loss: 0.4106] [G loss: 1.7305]\n",
      "[Epoch 4/500] [Batch 204/312] [D IS TRAINING] [D loss: 0.3147] [G loss: 1.0826]\n",
      "[Epoch 4/500] [Batch 205/312] [D IS TRAINING] [D loss: 0.3237] [G loss: 2.9534]\n",
      "[Epoch 4/500] [Batch 206/312] [D IS TRAINING] [D loss: 0.2789] [G loss: 1.3983]\n",
      "[Epoch 4/500] [Batch 207/312] [D IS TRAINING] [D loss: 0.3042] [G loss: 2.1837]\n",
      "[Epoch 4/500] [Batch 208/312] [D IS TRAINING] [D loss: 0.3346] [G loss: 1.0526]\n",
      "[Epoch 4/500] [Batch 209/312] [D IS TRAINING] [D loss: 0.6520] [G loss: 5.2116]\n",
      "[Epoch 4/500] [Batch 210/312] [D IS TRAINING] [D loss: 0.4671] [G loss: 0.6888]\n",
      "[Epoch 4/500] [Batch 211/312] [D IS TRAINING] [D loss: 0.2258] [G loss: 3.9333]\n",
      "[Epoch 4/500] [Batch 212/312] [D IS TRAINING] [D loss: 0.1111] [G loss: 3.2731]\n",
      "[Epoch 4/500] [Batch 213/312] [D IS TRAINING] [D loss: 0.1271] [G loss: 2.0933]\n",
      "[Epoch 4/500] [Batch 214/312] [D IS TRAINING] [D loss: 0.2112] [G loss: 1.4107]\n",
      "[Epoch 4/500] [Batch 215/312] [D IS TRAINING] [D loss: 0.2351] [G loss: 3.2753]\n",
      "[Epoch 4/500] [Batch 216/312] [D IS TRAINING] [D loss: 0.1601] [G loss: 1.9476]\n",
      "[Epoch 4/500] [Batch 217/312] [D IS TRAINING] [D loss: 0.2006] [G loss: 1.6139]\n",
      "[Epoch 4/500] [Batch 218/312] [D IS TRAINING] [D loss: 0.2479] [G loss: 3.3792]\n",
      "[Epoch 4/500] [Batch 219/312] [D IS TRAINING] [D loss: 0.6012] [G loss: 0.5365]\n",
      "[Epoch 4/500] [Batch 220/312] [D IS TRAINING] [D loss: 0.7905] [G loss: 4.8276]\n",
      "[Epoch 4/500] [Batch 221/312] [D IS TRAINING] [D loss: 0.2302] [G loss: 2.3472]\n",
      "[Epoch 4/500] [Batch 222/312] [D IS TRAINING] [D loss: 0.4307] [G loss: 0.7648]\n",
      "[Epoch 4/500] [Batch 223/312] [D IS TRAINING] [D loss: 0.3800] [G loss: 3.8144]\n",
      "[Epoch 4/500] [Batch 224/312] [D IS TRAINING] [D loss: 0.1720] [G loss: 2.0893]\n",
      "[Epoch 4/500] [Batch 225/312] [D IS TRAINING] [D loss: 0.3436] [G loss: 1.1136]\n",
      "[Epoch 4/500] [Batch 226/312] [D IS TRAINING] [D loss: 0.2234] [G loss: 3.2611]\n",
      "[Epoch 4/500] [Batch 227/312] [D IS TRAINING] [D loss: 0.1501] [G loss: 2.8455]\n",
      "[Epoch 4/500] [Batch 228/312] [D IS TRAINING] [D loss: 0.1190] [G loss: 2.1357]\n",
      "[Epoch 4/500] [Batch 229/312] [D IS TRAINING] [D loss: 0.1750] [G loss: 1.5840]\n",
      "[Epoch 4/500] [Batch 230/312] [D IS TRAINING] [D loss: 0.2082] [G loss: 2.8668]\n",
      "[Epoch 4/500] [Batch 231/312] [D IS TRAINING] [D loss: 0.1029] [G loss: 3.5352]\n",
      "[Epoch 4/500] [Batch 232/312] [D IS TRAINING] [D loss: 0.2271] [G loss: 1.3465]\n",
      "[Epoch 4/500] [Batch 233/312] [D IS TRAINING] [D loss: 0.1213] [G loss: 3.3960]\n",
      "[Epoch 4/500] [Batch 234/312] [D IS TRAINING] [D loss: 0.1813] [G loss: 1.8087]\n",
      "[Epoch 4/500] [Batch 235/312] [D IS TRAINING] [D loss: 0.1367] [G loss: 4.0759]\n",
      "[Epoch 4/500] [Batch 236/312] [D IS TRAINING] [D loss: 0.2682] [G loss: 1.1663]\n",
      "[Epoch 4/500] [Batch 237/312] [D IS TRAINING] [D loss: 0.1887] [G loss: 3.8751]\n",
      "[Epoch 4/500] [Batch 238/312] [D IS TRAINING] [D loss: 0.1409] [G loss: 2.6537]\n",
      "[Epoch 4/500] [Batch 239/312] [D IS TRAINING] [D loss: 0.1295] [G loss: 1.9148]\n",
      "[Epoch 4/500] [Batch 240/312] [D IS TRAINING] [D loss: 0.0843] [G loss: 3.1355]\n",
      "[Epoch 4/500] [Batch 241/312] [D IS TRAINING] [D loss: 0.0774] [G loss: 3.2867]\n",
      "[Epoch 4/500] [Batch 242/312] [D IS TRAINING] [D loss: 0.0821] [G loss: 3.0366]\n",
      "[Epoch 4/500] [Batch 243/312] [D IS TRAINING] [D loss: 0.1173] [G loss: 1.9954]\n",
      "[Epoch 4/500] [Batch 244/312] [D IS TRAINING] [D loss: 0.1003] [G loss: 3.4049]\n",
      "[Epoch 4/500] [Batch 245/312] [D IS TRAINING] [D loss: 0.0943] [G loss: 2.7902]\n",
      "[Epoch 4/500] [Batch 246/312] [D IS TRAINING] [D loss: 0.1580] [G loss: 1.8613]\n",
      "[Epoch 4/500] [Batch 247/312] [D IS TRAINING] [D loss: 0.1425] [G loss: 3.1115]\n",
      "[Epoch 4/500] [Batch 248/312] [D IS TRAINING] [D loss: 0.1333] [G loss: 2.1607]\n",
      "[Epoch 4/500] [Batch 249/312] [D IS TRAINING] [D loss: 0.1052] [G loss: 3.4216]\n",
      "[Epoch 4/500] [Batch 250/312] [D IS TRAINING] [D loss: 0.2351] [G loss: 1.2209]\n",
      "[Epoch 4/500] [Batch 251/312] [D IS TRAINING] [D loss: 0.5024] [G loss: 5.8759]\n",
      "[Epoch 4/500] [Batch 252/312] [D IS TRAINING] [D loss: 0.5518] [G loss: 1.0143]\n",
      "[Epoch 4/500] [Batch 253/312] [D IS TRAINING] [D loss: 0.3237] [G loss: 3.3886]\n",
      "[Epoch 4/500] [Batch 254/312] [D IS TRAINING] [D loss: 0.1618] [G loss: 2.4716]\n",
      "[Epoch 4/500] [Batch 255/312] [D IS TRAINING] [D loss: 0.1442] [G loss: 2.0502]\n",
      "[Epoch 4/500] [Batch 256/312] [D IS TRAINING] [D loss: 0.2099] [G loss: 2.1467]\n",
      "[Epoch 4/500] [Batch 257/312] [D IS TRAINING] [D loss: 0.1564] [G loss: 2.9850]\n",
      "[Epoch 4/500] [Batch 258/312] [D IS TRAINING] [D loss: 0.0948] [G loss: 2.8276]\n",
      "[Epoch 4/500] [Batch 259/312] [D IS TRAINING] [D loss: 0.3513] [G loss: 1.0688]\n",
      "[Epoch 4/500] [Batch 260/312] [D IS TRAINING] [D loss: 0.8246] [G loss: 5.7245]\n",
      "[Epoch 4/500] [Batch 261/312] [D IS TRAINING] [D loss: 0.1696] [G loss: 2.0809]\n",
      "[Epoch 4/500] [Batch 262/312] [D IS TRAINING] [D loss: 0.3174] [G loss: 1.3697]\n",
      "[Epoch 4/500] [Batch 263/312] [D IS TRAINING] [D loss: 0.3653] [G loss: 3.7414]\n",
      "[Epoch 4/500] [Batch 264/312] [D IS TRAINING] [D loss: 0.1545] [G loss: 2.7112]\n",
      "[Epoch 4/500] [Batch 265/312] [D IS TRAINING] [D loss: 0.0674] [G loss: 2.9377]\n",
      "[Epoch 4/500] [Batch 266/312] [D IS TRAINING] [D loss: 0.2182] [G loss: 1.4704]\n",
      "[Epoch 4/500] [Batch 267/312] [D IS TRAINING] [D loss: 0.3792] [G loss: 4.1632]\n",
      "[Epoch 4/500] [Batch 268/312] [D IS TRAINING] [D loss: 0.2146] [G loss: 1.4480]\n",
      "[Epoch 4/500] [Batch 269/312] [D IS TRAINING] [D loss: 0.1870] [G loss: 1.7891]\n",
      "[Epoch 4/500] [Batch 270/312] [D IS TRAINING] [D loss: 0.2842] [G loss: 3.5650]\n",
      "[Epoch 4/500] [Batch 271/312] [D IS TRAINING] [D loss: 0.3037] [G loss: 1.0104]\n",
      "[Epoch 4/500] [Batch 272/312] [D IS TRAINING] [D loss: 0.3075] [G loss: 3.2706]\n",
      "[Epoch 4/500] [Batch 273/312] [D IS TRAINING] [D loss: 0.1257] [G loss: 3.1585]\n",
      "[Epoch 4/500] [Batch 274/312] [D IS TRAINING] [D loss: 0.7117] [G loss: 0.3671]\n",
      "[Epoch 4/500] [Batch 275/312] [D IS TRAINING] [D loss: 1.0588] [G loss: 7.0823]\n",
      "[Epoch 4/500] [Batch 276/312] [D IS TRAINING] [D loss: 0.2329] [G loss: 3.6979]\n",
      "[Epoch 4/500] [Batch 277/312] [D IS TRAINING] [D loss: 0.4059] [G loss: 0.9634]\n",
      "[Epoch 4/500] [Batch 278/312] [D IS TRAINING] [D loss: 0.2666] [G loss: 1.5637]\n",
      "[Epoch 4/500] [Batch 279/312] [D IS TRAINING] [D loss: 0.3147] [G loss: 3.3280]\n",
      "[Epoch 4/500] [Batch 280/312] [D IS TRAINING] [D loss: 0.2959] [G loss: 1.3665]\n",
      "[Epoch 4/500] [Batch 281/312] [D IS TRAINING] [D loss: 0.2390] [G loss: 1.8419]\n",
      "[Epoch 4/500] [Batch 282/312] [D IS TRAINING] [D loss: 0.2814] [G loss: 2.4597]\n",
      "[Epoch 4/500] [Batch 283/312] [D IS TRAINING] [D loss: 0.3803] [G loss: 1.1702]\n",
      "[Epoch 4/500] [Batch 284/312] [D IS TRAINING] [D loss: 0.3300] [G loss: 2.5872]\n",
      "[Epoch 4/500] [Batch 285/312] [D IS TRAINING] [D loss: 0.2296] [G loss: 1.8180]\n",
      "[Epoch 4/500] [Batch 286/312] [D IS TRAINING] [D loss: 0.2631] [G loss: 1.3810]\n",
      "[Epoch 4/500] [Batch 287/312] [D IS TRAINING] [D loss: 0.2470] [G loss: 3.0083]\n",
      "[Epoch 4/500] [Batch 288/312] [D IS TRAINING] [D loss: 0.2428] [G loss: 1.3979]\n",
      "[Epoch 4/500] [Batch 289/312] [D IS TRAINING] [D loss: 0.2110] [G loss: 3.2355]\n",
      "[Epoch 4/500] [Batch 290/312] [D IS TRAINING] [D loss: 0.2005] [G loss: 1.5183]\n",
      "[Epoch 4/500] [Batch 291/312] [D IS TRAINING] [D loss: 0.3053] [G loss: 1.4818]\n",
      "[Epoch 4/500] [Batch 292/312] [D IS TRAINING] [D loss: 0.4368] [G loss: 3.2720]\n",
      "[Epoch 4/500] [Batch 293/312] [D IS TRAINING] [D loss: 0.5944] [G loss: 0.6930]\n",
      "[Epoch 4/500] [Batch 294/312] [D IS TRAINING] [D loss: 0.5167] [G loss: 3.7981]\n",
      "[Epoch 4/500] [Batch 295/312] [D IS TRAINING] [D loss: 0.2045] [G loss: 1.8671]\n",
      "[Epoch 4/500] [Batch 296/312] [D IS TRAINING] [D loss: 0.2564] [G loss: 1.6507]\n",
      "[Epoch 4/500] [Batch 297/312] [D IS TRAINING] [D loss: 0.1910] [G loss: 1.7668]\n",
      "[Epoch 4/500] [Batch 298/312] [D IS TRAINING] [D loss: 0.1818] [G loss: 3.1444]\n",
      "[Epoch 4/500] [Batch 299/312] [D IS TRAINING] [D loss: 0.1038] [G loss: 2.4948]\n",
      "[Epoch 4/500] [Batch 300/312] [D IS TRAINING] [D loss: 0.3225] [G loss: 1.0043]\n",
      "[Epoch 4/500] [Batch 301/312] [D IS TRAINING] [D loss: 0.2538] [G loss: 4.3446]\n",
      "[Epoch 4/500] [Batch 302/312] [D IS TRAINING] [D loss: 0.0661] [G loss: 4.9517]\n",
      "[Epoch 4/500] [Batch 303/312] [D IS TRAINING] [D loss: 0.0828] [G loss: 2.5433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/500] [Batch 304/312] [D IS TRAINING] [D loss: 0.1642] [G loss: 1.5392]\n",
      "[Epoch 4/500] [Batch 305/312] [D IS TRAINING] [D loss: 0.1774] [G loss: 2.0675]\n",
      "[Epoch 4/500] [Batch 306/312] [D IS TRAINING] [D loss: 0.2210] [G loss: 3.3525]\n",
      "[Epoch 4/500] [Batch 307/312] [D IS TRAINING] [D loss: 0.1417] [G loss: 1.9628]\n",
      "[Epoch 4/500] [Batch 308/312] [D IS TRAINING] [D loss: 0.2809] [G loss: 1.1312]\n",
      "[Epoch 4/500] [Batch 309/312] [D IS TRAINING] [D loss: 0.3395] [G loss: 3.6875]\n",
      "[Epoch 4/500] [Batch 310/312] [D IS TRAINING] [D loss: 0.0714] [G loss: 3.1304]\n",
      "[Epoch 4/500] [Batch 311/312] [D IS TRAINING] [D loss: 0.6963] [G loss: 0.4017]\n",
      "[Epoch 5/500] [Batch 0/312] [D IS TRAINING] [D loss: 0.9461] [G loss: 6.2176]\n",
      "[Epoch 5/500] [Batch 1/312] [D IS TRAINING] [D loss: 0.1647] [G loss: 4.7789]\n",
      "[Epoch 5/500] [Batch 2/312] [D IS TRAINING] [D loss: 0.5091] [G loss: 0.7766]\n",
      "[Epoch 5/500] [Batch 3/312] [D IS TRAINING] [D loss: 0.2070] [G loss: 2.4163]\n",
      "[Epoch 5/500] [Batch 4/312] [D IS TRAINING] [D loss: 0.2059] [G loss: 3.2721]\n",
      "[Epoch 5/500] [Batch 5/312] [D IS TRAINING] [D loss: 0.0951] [G loss: 2.8229]\n",
      "[Epoch 5/500] [Batch 6/312] [D IS TRAINING] [D loss: 0.3299] [G loss: 1.0649]\n",
      "[Epoch 5/500] [Batch 7/312] [D IS TRAINING] [D loss: 0.2346] [G loss: 2.3482]\n",
      "[Epoch 5/500] [Batch 8/312] [D IS TRAINING] [D loss: 0.2352] [G loss: 2.2367]\n",
      "[Epoch 5/500] [Batch 9/312] [D IS TRAINING] [D loss: 0.2540] [G loss: 1.3323]\n",
      "[Epoch 5/500] [Batch 10/312] [D IS TRAINING] [D loss: 0.1719] [G loss: 3.1092]\n",
      "[Epoch 5/500] [Batch 11/312] [D IS TRAINING] [D loss: 0.1514] [G loss: 1.8846]\n",
      "[Epoch 5/500] [Batch 12/312] [D IS TRAINING] [D loss: 0.1350] [G loss: 2.4529]\n",
      "[Epoch 5/500] [Batch 13/312] [D IS TRAINING] [D loss: 0.1662] [G loss: 2.5670]\n",
      "[Epoch 5/500] [Batch 14/312] [D IS TRAINING] [D loss: 0.1984] [G loss: 1.6830]\n",
      "[Epoch 5/500] [Batch 15/312] [D IS TRAINING] [D loss: 0.1635] [G loss: 2.8045]\n",
      "[Epoch 5/500] [Batch 16/312] [D IS TRAINING] [D loss: 0.1335] [G loss: 2.7377]\n",
      "[Epoch 5/500] [Batch 17/312] [D IS TRAINING] [D loss: 0.4324] [G loss: 0.7193]\n",
      "[Epoch 5/500] [Batch 18/312] [D IS TRAINING] [D loss: 0.6312] [G loss: 5.8751]\n",
      "[Epoch 5/500] [Batch 19/312] [D IS TRAINING] [D loss: 0.1713] [G loss: 3.3505]\n",
      "[Epoch 5/500] [Batch 20/312] [D IS TRAINING] [D loss: 0.3122] [G loss: 1.1725]\n",
      "[Epoch 5/500] [Batch 21/312] [D IS TRAINING] [D loss: 0.2477] [G loss: 2.6515]\n",
      "[Epoch 5/500] [Batch 22/312] [D IS TRAINING] [D loss: 0.1749] [G loss: 2.8755]\n",
      "[Epoch 5/500] [Batch 23/312] [D IS TRAINING] [D loss: 0.1827] [G loss: 2.0186]\n",
      "[Epoch 5/500] [Batch 24/312] [D IS TRAINING] [D loss: 0.1452] [G loss: 2.5165]\n",
      "[Epoch 5/500] [Batch 25/312] [D IS TRAINING] [D loss: 0.1199] [G loss: 2.3416]\n",
      "[Epoch 5/500] [Batch 26/312] [D IS TRAINING] [D loss: 0.0923] [G loss: 3.1848]\n",
      "[Epoch 5/500] [Batch 27/312] [D IS TRAINING] [D loss: 0.1564] [G loss: 1.9334]\n",
      "[Epoch 5/500] [Batch 28/312] [D IS TRAINING] [D loss: 0.1126] [G loss: 2.9694]\n",
      "[Epoch 5/500] [Batch 29/312] [D IS TRAINING] [D loss: 0.1368] [G loss: 2.2236]\n",
      "[Epoch 5/500] [Batch 30/312] [D IS TRAINING] [D loss: 0.1135] [G loss: 2.5623]\n",
      "[Epoch 5/500] [Batch 31/312] [D IS TRAINING] [D loss: 0.4567] [G loss: 0.7567]\n",
      "[Epoch 5/500] [Batch 32/312] [D IS TRAINING] [D loss: 1.1161] [G loss: 5.6467]\n",
      "[Epoch 5/500] [Batch 33/312] [D IS TRAINING] [D loss: 0.1577] [G loss: 3.3394]\n",
      "[Epoch 5/500] [Batch 34/312] [D IS TRAINING] [D loss: 0.3489] [G loss: 1.1196]\n",
      "[Epoch 5/500] [Batch 35/312] [D IS TRAINING] [D loss: 0.1708] [G loss: 2.1235]\n",
      "[Epoch 5/500] [Batch 36/312] [D IS TRAINING] [D loss: 0.2510] [G loss: 4.2202]\n",
      "[Epoch 5/500] [Batch 37/312] [D IS TRAINING] [D loss: 0.3358] [G loss: 1.2484]\n",
      "[Epoch 5/500] [Batch 38/312] [D IS TRAINING] [D loss: 0.2257] [G loss: 3.1870]\n",
      "[Epoch 5/500] [Batch 39/312] [D IS TRAINING] [D loss: 0.2212] [G loss: 1.5055]\n",
      "[Epoch 5/500] [Batch 40/312] [D IS TRAINING] [D loss: 0.1650] [G loss: 3.0728]\n",
      "[Epoch 5/500] [Batch 41/312] [D IS TRAINING] [D loss: 0.1131] [G loss: 3.7263]\n",
      "[Epoch 5/500] [Batch 42/312] [D IS TRAINING] [D loss: 0.4823] [G loss: 0.7134]\n",
      "[Epoch 5/500] [Batch 43/312] [D IS TRAINING] [D loss: 0.6895] [G loss: 6.0129]\n",
      "[Epoch 5/500] [Batch 44/312] [D IS TRAINING] [D loss: 0.0845] [G loss: 3.0809]\n",
      "[Epoch 5/500] [Batch 45/312] [D IS TRAINING] [D loss: 0.5014] [G loss: 0.6619]\n",
      "[Epoch 5/500] [Batch 46/312] [D IS TRAINING] [D loss: 0.6197] [G loss: 5.7170]\n",
      "[Epoch 5/500] [Batch 47/312] [D IS TRAINING] [D loss: 0.1796] [G loss: 2.6720]\n",
      "[Epoch 5/500] [Batch 48/312] [D IS TRAINING] [D loss: 0.3477] [G loss: 1.0223]\n",
      "[Epoch 5/500] [Batch 49/312] [D IS TRAINING] [D loss: 0.1996] [G loss: 2.8068]\n",
      "[Epoch 5/500] [Batch 50/312] [D IS TRAINING] [D loss: 0.2245] [G loss: 3.0208]\n",
      "[Epoch 5/500] [Batch 51/312] [D IS TRAINING] [D loss: 0.2893] [G loss: 1.1227]\n",
      "[Epoch 5/500] [Batch 52/312] [D IS TRAINING] [D loss: 0.2570] [G loss: 2.1081]\n",
      "[Epoch 5/500] [Batch 53/312] [D IS TRAINING] [D loss: 0.2098] [G loss: 2.1819]\n",
      "[Epoch 5/500] [Batch 54/312] [D IS TRAINING] [D loss: 0.3112] [G loss: 1.3686]\n",
      "[Epoch 5/500] [Batch 55/312] [D IS TRAINING] [D loss: 0.2733] [G loss: 2.0060]\n",
      "[Epoch 5/500] [Batch 56/312] [D IS TRAINING] [D loss: 0.2382] [G loss: 1.9095]\n",
      "[Epoch 5/500] [Batch 57/312] [D IS TRAINING] [D loss: 0.2108] [G loss: 2.0447]\n",
      "[Epoch 5/500] [Batch 58/312] [D IS TRAINING] [D loss: 0.2091] [G loss: 1.9279]\n",
      "[Epoch 5/500] [Batch 59/312] [D IS TRAINING] [D loss: 0.2189] [G loss: 1.9544]\n",
      "[Epoch 5/500] [Batch 60/312] [D IS TRAINING] [D loss: 0.2407] [G loss: 2.0185]\n",
      "[Epoch 5/500] [Batch 61/312] [D IS TRAINING] [D loss: 0.2093] [G loss: 2.0126]\n",
      "[Epoch 5/500] [Batch 62/312] [D IS TRAINING] [D loss: 0.1683] [G loss: 2.4888]\n",
      "[Epoch 5/500] [Batch 63/312] [D IS TRAINING] [D loss: 0.1749] [G loss: 1.8060]\n",
      "[Epoch 5/500] [Batch 64/312] [D IS TRAINING] [D loss: 0.2436] [G loss: 1.7742]\n",
      "[Epoch 5/500] [Batch 65/312] [D IS TRAINING] [D loss: 0.2483] [G loss: 3.2663]\n",
      "[Epoch 5/500] [Batch 66/312] [D IS TRAINING] [D loss: 0.0752] [G loss: 3.4053]\n",
      "[Epoch 5/500] [Batch 67/312] [D IS TRAINING] [D loss: 0.3882] [G loss: 0.7815]\n",
      "[Epoch 5/500] [Batch 68/312] [D IS TRAINING] [D loss: 0.3013] [G loss: 6.1528]\n",
      "[Epoch 5/500] [Batch 69/312] [D IS TRAINING] [D loss: 0.1096] [G loss: 2.3412]\n",
      "[Epoch 5/500] [Batch 70/312] [D IS TRAINING] [D loss: 0.3006] [G loss: 1.2515]\n",
      "[Epoch 5/500] [Batch 71/312] [D IS TRAINING] [D loss: 0.5781] [G loss: 5.1757]\n",
      "[Epoch 5/500] [Batch 72/312] [D IS TRAINING] [D loss: 0.2125] [G loss: 1.5785]\n",
      "[Epoch 5/500] [Batch 73/312] [D IS TRAINING] [D loss: 0.1556] [G loss: 1.8169]\n",
      "[Epoch 5/500] [Batch 74/312] [D IS TRAINING] [D loss: 0.2098] [G loss: 2.8826]\n",
      "[Epoch 5/500] [Batch 75/312] [D IS TRAINING] [D loss: 0.1407] [G loss: 2.0232]\n",
      "[Epoch 5/500] [Batch 76/312] [D IS TRAINING] [D loss: 0.1062] [G loss: 2.3634]\n",
      "[Epoch 5/500] [Batch 77/312] [D IS TRAINING] [D loss: 0.1114] [G loss: 2.6627]\n",
      "[Epoch 5/500] [Batch 78/312] [D IS TRAINING] [D loss: 0.1227] [G loss: 3.0328]\n",
      "[Epoch 5/500] [Batch 79/312] [D IS TRAINING] [D loss: 0.4201] [G loss: 0.7805]\n",
      "[Epoch 5/500] [Batch 80/312] [D IS TRAINING] [D loss: 0.6042] [G loss: 5.6229]\n",
      "[Epoch 5/500] [Batch 81/312] [D IS TRAINING] [D loss: 0.1585] [G loss: 2.7681]\n",
      "[Epoch 5/500] [Batch 82/312] [D IS TRAINING] [D loss: 0.4827] [G loss: 0.8056]\n",
      "[Epoch 5/500] [Batch 83/312] [D IS TRAINING] [D loss: 0.8062] [G loss: 6.7340]\n",
      "[Epoch 5/500] [Batch 84/312] [D IS TRAINING] [D loss: 0.1751] [G loss: 3.6804]\n",
      "[Epoch 5/500] [Batch 85/312] [D IS TRAINING] [D loss: 0.4067] [G loss: 0.9001]\n",
      "[Epoch 5/500] [Batch 86/312] [D IS TRAINING] [D loss: 0.1102] [G loss: 2.9239]\n",
      "[Epoch 5/500] [Batch 87/312] [D IS TRAINING] [D loss: 0.2035] [G loss: 3.3395]\n",
      "[Epoch 5/500] [Batch 88/312] [D IS TRAINING] [D loss: 0.0927] [G loss: 2.6835]\n",
      "[Epoch 5/500] [Batch 89/312] [D IS TRAINING] [D loss: 0.1414] [G loss: 1.9551]\n",
      "[Epoch 5/500] [Batch 90/312] [D IS TRAINING] [D loss: 0.1318] [G loss: 2.6251]\n",
      "[Epoch 5/500] [Batch 91/312] [D IS TRAINING] [D loss: 0.1491] [G loss: 2.3768]\n",
      "[Epoch 5/500] [Batch 92/312] [D IS TRAINING] [D loss: 0.1979] [G loss: 1.8357]\n",
      "[Epoch 5/500] [Batch 93/312] [D IS TRAINING] [D loss: 0.2349] [G loss: 2.6515]\n",
      "[Epoch 5/500] [Batch 94/312] [D IS TRAINING] [D loss: 0.2504] [G loss: 1.2117]\n",
      "[Epoch 5/500] [Batch 95/312] [D IS TRAINING] [D loss: 0.3866] [G loss: 3.0711]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/500] [Batch 96/312] [D IS TRAINING] [D loss: 0.4963] [G loss: 0.7964]\n",
      "[Epoch 5/500] [Batch 97/312] [D IS TRAINING] [D loss: 0.7008] [G loss: 4.5359]\n",
      "[Epoch 5/500] [Batch 98/312] [D IS TRAINING] [D loss: 0.1714] [G loss: 2.1315]\n",
      "[Epoch 5/500] [Batch 99/312] [D IS TRAINING] [D loss: 0.5590] [G loss: 0.5358]\n",
      "[Epoch 5/500] [Batch 100/312] [D IS TRAINING] [D loss: 0.9470] [G loss: 5.6109]\n",
      "[Epoch 5/500] [Batch 101/312] [D IS TRAINING] [D loss: 0.3041] [G loss: 1.5995]\n",
      "[Epoch 5/500] [Batch 102/312] [D IS TRAINING] [D loss: 0.5616] [G loss: 0.5983]\n",
      "[Epoch 5/500] [Batch 103/312] [D IS TRAINING] [D loss: 0.5861] [G loss: 4.1209]\n",
      "[Epoch 5/500] [Batch 104/312] [D IS TRAINING] [D loss: 0.2371] [G loss: 1.9436]\n",
      "[Epoch 5/500] [Batch 105/312] [D IS TRAINING] [D loss: 0.3674] [G loss: 1.0786]\n",
      "[Epoch 5/500] [Batch 106/312] [D IS TRAINING] [D loss: 0.3260] [G loss: 1.7710]\n",
      "[Epoch 5/500] [Batch 107/312] [D IS TRAINING] [D loss: 0.2087] [G loss: 2.2473]\n",
      "[Epoch 5/500] [Batch 108/312] [D IS TRAINING] [D loss: 0.2292] [G loss: 1.9480]\n",
      "[Epoch 5/500] [Batch 109/312] [D IS TRAINING] [D loss: 0.2192] [G loss: 1.7127]\n",
      "[Epoch 5/500] [Batch 110/312] [D IS TRAINING] [D loss: 0.3092] [G loss: 1.4756]\n",
      "[Epoch 5/500] [Batch 111/312] [D IS TRAINING] [D loss: 0.3094] [G loss: 2.2264]\n",
      "[Epoch 5/500] [Batch 112/312] [D IS TRAINING] [D loss: 0.1696] [G loss: 2.0627]\n",
      "[Epoch 5/500] [Batch 113/312] [D IS TRAINING] [D loss: 0.1165] [G loss: 2.2601]\n",
      "[Epoch 5/500] [Batch 114/312] [D IS TRAINING] [D loss: 0.1155] [G loss: 2.4433]\n",
      "[Epoch 5/500] [Batch 115/312] [D IS TRAINING] [D loss: 0.1251] [G loss: 2.3763]\n",
      "[Epoch 5/500] [Batch 116/312] [D IS TRAINING] [D loss: 0.1795] [G loss: 1.6332]\n",
      "[Epoch 5/500] [Batch 117/312] [D IS TRAINING] [D loss: 0.1954] [G loss: 2.3848]\n",
      "[Epoch 5/500] [Batch 118/312] [D IS TRAINING] [D loss: 0.1450] [G loss: 2.1412]\n",
      "[Epoch 5/500] [Batch 119/312] [D IS TRAINING] [D loss: 0.1974] [G loss: 2.2153]\n",
      "[Epoch 5/500] [Batch 120/312] [D IS TRAINING] [D loss: 0.1986] [G loss: 1.6200]\n",
      "[Epoch 5/500] [Batch 121/312] [D IS TRAINING] [D loss: 0.2136] [G loss: 2.3298]\n",
      "[Epoch 5/500] [Batch 122/312] [D IS TRAINING] [D loss: 0.1689] [G loss: 2.1771]\n",
      "[Epoch 5/500] [Batch 123/312] [D IS TRAINING] [D loss: 0.1545] [G loss: 1.8981]\n",
      "[Epoch 5/500] [Batch 124/312] [D IS TRAINING] [D loss: 0.1807] [G loss: 2.2945]\n",
      "[Epoch 5/500] [Batch 125/312] [D IS TRAINING] [D loss: 0.1292] [G loss: 2.2937]\n",
      "[Epoch 5/500] [Batch 126/312] [D IS TRAINING] [D loss: 0.1228] [G loss: 2.2990]\n",
      "[Epoch 5/500] [Batch 127/312] [D IS TRAINING] [D loss: 0.1524] [G loss: 2.3651]\n",
      "[Epoch 5/500] [Batch 128/312] [D IS TRAINING] [D loss: 0.1503] [G loss: 2.2524]\n",
      "[Epoch 5/500] [Batch 129/312] [D IS TRAINING] [D loss: 0.1204] [G loss: 2.5029]\n",
      "[Epoch 5/500] [Batch 130/312] [D IS TRAINING] [D loss: 0.1257] [G loss: 2.2225]\n",
      "[Epoch 5/500] [Batch 131/312] [D IS TRAINING] [D loss: 0.1339] [G loss: 2.2023]\n",
      "[Epoch 5/500] [Batch 132/312] [D IS TRAINING] [D loss: 0.1269] [G loss: 2.2689]\n",
      "[Epoch 5/500] [Batch 133/312] [D IS TRAINING] [D loss: 0.1423] [G loss: 3.3491]\n",
      "[Epoch 5/500] [Batch 134/312] [D IS TRAINING] [D loss: 0.1269] [G loss: 2.1877]\n",
      "[Epoch 5/500] [Batch 135/312] [D IS TRAINING] [D loss: 0.3019] [G loss: 1.0570]\n",
      "[Epoch 5/500] [Batch 136/312] [D IS TRAINING] [D loss: 0.4804] [G loss: 4.4901]\n",
      "[Epoch 5/500] [Batch 137/312] [D IS TRAINING] [D loss: 0.0869] [G loss: 3.1779]\n",
      "[Epoch 5/500] [Batch 138/312] [D IS TRAINING] [D loss: 0.6541] [G loss: 0.4111]\n",
      "[Epoch 5/500] [Batch 139/312] [D IS TRAINING] [D loss: 0.5426] [G loss: 6.2049]\n",
      "[Epoch 5/500] [Batch 140/312] [D IS TRAINING] [D loss: 0.1700] [G loss: 5.5012]\n",
      "[Epoch 5/500] [Batch 141/312] [D IS TRAINING] [D loss: 0.1883] [G loss: 1.7249]\n",
      "[Epoch 5/500] [Batch 142/312] [D IS TRAINING] [D loss: 0.0765] [G loss: 2.5614]\n",
      "[Epoch 5/500] [Batch 143/312] [D IS TRAINING] [D loss: 0.2405] [G loss: 1.6020]\n",
      "[Epoch 5/500] [Batch 144/312] [D IS TRAINING] [D loss: 0.2501] [G loss: 3.5399]\n",
      "[Epoch 5/500] [Batch 145/312] [D IS TRAINING] [D loss: 0.3653] [G loss: 1.2264]\n",
      "[Epoch 5/500] [Batch 146/312] [D IS TRAINING] [D loss: 0.2972] [G loss: 2.9659]\n",
      "[Epoch 5/500] [Batch 147/312] [D IS TRAINING] [D loss: 0.1780] [G loss: 2.4177]\n",
      "[Epoch 5/500] [Batch 148/312] [D IS TRAINING] [D loss: 0.2170] [G loss: 1.4654]\n",
      "[Epoch 5/500] [Batch 149/312] [D IS TRAINING] [D loss: 0.1829] [G loss: 3.0863]\n",
      "[Epoch 5/500] [Batch 150/312] [D IS TRAINING] [D loss: 0.1075] [G loss: 3.5817]\n",
      "[Epoch 5/500] [Batch 151/312] [D IS TRAINING] [D loss: 0.2792] [G loss: 1.1121]\n",
      "[Epoch 5/500] [Batch 152/312] [D IS TRAINING] [D loss: 0.2275] [G loss: 3.7795]\n",
      "[Epoch 5/500] [Batch 153/312] [D IS TRAINING] [D loss: 0.1088] [G loss: 3.0664]\n",
      "[Epoch 5/500] [Batch 154/312] [D IS TRAINING] [D loss: 0.4408] [G loss: 0.8227]\n",
      "[Epoch 5/500] [Batch 155/312] [D IS TRAINING] [D loss: 0.5481] [G loss: 5.2315]\n",
      "[Epoch 5/500] [Batch 156/312] [D IS TRAINING] [D loss: 0.3977] [G loss: 0.9637]\n",
      "[Epoch 5/500] [Batch 157/312] [D IS TRAINING] [D loss: 0.3728] [G loss: 2.7795]\n",
      "[Epoch 5/500] [Batch 158/312] [D IS TRAINING] [D loss: 0.2402] [G loss: 1.9619]\n",
      "[Epoch 5/500] [Batch 159/312] [D IS TRAINING] [D loss: 0.1005] [G loss: 3.3001]\n",
      "[Epoch 5/500] [Batch 160/312] [D IS TRAINING] [D loss: 0.3535] [G loss: 0.9901]\n",
      "[Epoch 5/500] [Batch 161/312] [D IS TRAINING] [D loss: 0.2777] [G loss: 3.6573]\n",
      "[Epoch 5/500] [Batch 162/312] [D IS TRAINING] [D loss: 0.0903] [G loss: 5.0707]\n",
      "[Epoch 5/500] [Batch 163/312] [D IS TRAINING] [D loss: 0.2869] [G loss: 1.1936]\n",
      "[Epoch 5/500] [Batch 164/312] [D IS TRAINING] [D loss: 0.1730] [G loss: 4.0246]\n",
      "[Epoch 5/500] [Batch 165/312] [D IS TRAINING] [D loss: 0.1881] [G loss: 1.9858]\n",
      "[Epoch 5/500] [Batch 166/312] [D IS TRAINING] [D loss: 0.1260] [G loss: 2.3784]\n",
      "[Epoch 5/500] [Batch 167/312] [D IS TRAINING] [D loss: 0.1377] [G loss: 3.1644]\n",
      "[Epoch 5/500] [Batch 168/312] [D IS TRAINING] [D loss: 0.1329] [G loss: 2.4512]\n",
      "[Epoch 5/500] [Batch 169/312] [D IS TRAINING] [D loss: 0.1551] [G loss: 1.8848]\n",
      "[Epoch 5/500] [Batch 170/312] [D IS TRAINING] [D loss: 0.2224] [G loss: 2.2829]\n",
      "[Epoch 5/500] [Batch 171/312] [D IS TRAINING] [D loss: 0.1182] [G loss: 3.4704]\n",
      "[Epoch 5/500] [Batch 172/312] [D IS TRAINING] [D loss: 0.5418] [G loss: 0.7498]\n",
      "[Epoch 5/500] [Batch 173/312] [D IS TRAINING] [D loss: 0.6831] [G loss: 6.2762]\n",
      "[Epoch 5/500] [Batch 174/312] [D IS TRAINING] [D loss: 0.1764] [G loss: 3.6082]\n",
      "[Epoch 5/500] [Batch 175/312] [D IS TRAINING] [D loss: 0.4929] [G loss: 0.9915]\n",
      "[Epoch 5/500] [Batch 176/312] [D IS TRAINING] [D loss: 0.2850] [G loss: 4.5036]\n",
      "[Epoch 5/500] [Batch 177/312] [D IS TRAINING] [D loss: 0.2022] [G loss: 2.6212]\n",
      "[Epoch 5/500] [Batch 178/312] [D IS TRAINING] [D loss: 0.1256] [G loss: 2.8304]\n",
      "[Epoch 5/500] [Batch 179/312] [D IS TRAINING] [D loss: 0.3590] [G loss: 0.9602]\n",
      "[Epoch 5/500] [Batch 180/312] [D IS TRAINING] [D loss: 0.2352] [G loss: 3.4251]\n",
      "[Epoch 5/500] [Batch 181/312] [D IS TRAINING] [D loss: 0.1383] [G loss: 3.5302]\n",
      "[Epoch 5/500] [Batch 182/312] [D IS TRAINING] [D loss: 0.3736] [G loss: 0.9625]\n",
      "[Epoch 5/500] [Batch 183/312] [D IS TRAINING] [D loss: 0.1391] [G loss: 3.2625]\n",
      "[Epoch 5/500] [Batch 184/312] [D IS TRAINING] [D loss: 0.2415] [G loss: 2.2511]\n",
      "[Epoch 5/500] [Batch 185/312] [D IS TRAINING] [D loss: 0.2762] [G loss: 1.3955]\n",
      "[Epoch 5/500] [Batch 186/312] [D IS TRAINING] [D loss: 0.1852] [G loss: 2.2243]\n",
      "[Epoch 5/500] [Batch 187/312] [D IS TRAINING] [D loss: 0.1669] [G loss: 3.0211]\n",
      "[Epoch 5/500] [Batch 188/312] [D IS TRAINING] [D loss: 0.1769] [G loss: 1.7533]\n",
      "[Epoch 5/500] [Batch 189/312] [D IS TRAINING] [D loss: 0.2247] [G loss: 1.6852]\n",
      "[Epoch 5/500] [Batch 190/312] [D IS TRAINING] [D loss: 0.1930] [G loss: 2.4194]\n",
      "[Epoch 5/500] [Batch 191/312] [D IS TRAINING] [D loss: 0.1458] [G loss: 2.6582]\n",
      "[Epoch 5/500] [Batch 192/312] [D IS TRAINING] [D loss: 0.2300] [G loss: 1.5822]\n",
      "[Epoch 5/500] [Batch 193/312] [D IS TRAINING] [D loss: 0.1936] [G loss: 2.0395]\n",
      "[Epoch 5/500] [Batch 194/312] [D IS TRAINING] [D loss: 0.1442] [G loss: 2.8250]\n",
      "[Epoch 5/500] [Batch 195/312] [D IS TRAINING] [D loss: 0.2535] [G loss: 1.2419]\n",
      "[Epoch 5/500] [Batch 196/312] [D IS TRAINING] [D loss: 0.2530] [G loss: 2.6692]\n",
      "[Epoch 5/500] [Batch 197/312] [D IS TRAINING] [D loss: 0.1079] [G loss: 2.8268]\n",
      "[Epoch 5/500] [Batch 198/312] [D IS TRAINING] [D loss: 0.3584] [G loss: 0.9635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/500] [Batch 199/312] [D IS TRAINING] [D loss: 0.3808] [G loss: 3.6971]\n",
      "[Epoch 5/500] [Batch 200/312] [D IS TRAINING] [D loss: 0.0980] [G loss: 3.2275]\n",
      "[Epoch 5/500] [Batch 201/312] [D IS TRAINING] [D loss: 0.4341] [G loss: 0.7447]\n",
      "[Epoch 5/500] [Batch 202/312] [D IS TRAINING] [D loss: 0.4032] [G loss: 3.5998]\n",
      "[Epoch 5/500] [Batch 203/312] [D IS TRAINING] [D loss: 0.1402] [G loss: 2.9935]\n",
      "[Epoch 5/500] [Batch 204/312] [D IS TRAINING] [D loss: 0.1276] [G loss: 2.1932]\n",
      "[Epoch 5/500] [Batch 205/312] [D IS TRAINING] [D loss: 0.3264] [G loss: 0.9297]\n",
      "[Epoch 5/500] [Batch 206/312] [D IS TRAINING] [D loss: 0.5108] [G loss: 4.6178]\n",
      "[Epoch 5/500] [Batch 207/312] [D IS TRAINING] [D loss: 0.2194] [G loss: 1.6488]\n",
      "[Epoch 5/500] [Batch 208/312] [D IS TRAINING] [D loss: 0.1861] [G loss: 1.7428]\n",
      "[Epoch 5/500] [Batch 209/312] [D IS TRAINING] [D loss: 0.1489] [G loss: 2.5919]\n",
      "[Epoch 5/500] [Batch 210/312] [D IS TRAINING] [D loss: 0.1163] [G loss: 3.0627]\n",
      "[Epoch 5/500] [Batch 211/312] [D IS TRAINING] [D loss: 0.1976] [G loss: 1.4753]\n",
      "[Epoch 5/500] [Batch 212/312] [D IS TRAINING] [D loss: 0.1599] [G loss: 2.7107]\n",
      "[Epoch 5/500] [Batch 213/312] [D IS TRAINING] [D loss: 0.0959] [G loss: 3.6290]\n",
      "[Epoch 5/500] [Batch 214/312] [D IS TRAINING] [D loss: 0.1658] [G loss: 1.7390]\n",
      "[Epoch 5/500] [Batch 215/312] [D IS TRAINING] [D loss: 0.1882] [G loss: 2.2276]\n",
      "[Epoch 5/500] [Batch 216/312] [D IS TRAINING] [D loss: 0.1514] [G loss: 2.9974]\n",
      "[Epoch 5/500] [Batch 217/312] [D IS TRAINING] [D loss: 0.1503] [G loss: 1.8246]\n",
      "[Epoch 5/500] [Batch 218/312] [D IS TRAINING] [D loss: 0.1155] [G loss: 3.0010]\n",
      "[Epoch 5/500] [Batch 219/312] [D IS TRAINING] [D loss: 0.1110] [G loss: 3.2654]\n",
      "[Epoch 5/500] [Batch 220/312] [D IS TRAINING] [D loss: 0.1344] [G loss: 2.0859]\n",
      "[Epoch 5/500] [Batch 221/312] [D IS TRAINING] [D loss: 0.1725] [G loss: 1.9948]\n",
      "[Epoch 5/500] [Batch 222/312] [D IS TRAINING] [D loss: 0.1730] [G loss: 3.1842]\n",
      "[Epoch 5/500] [Batch 223/312] [D IS TRAINING] [D loss: 0.1694] [G loss: 1.7339]\n",
      "[Epoch 5/500] [Batch 224/312] [D IS TRAINING] [D loss: 0.1533] [G loss: 2.9171]\n",
      "[Epoch 5/500] [Batch 225/312] [D IS TRAINING] [D loss: 0.1093] [G loss: 2.6788]\n",
      "[Epoch 5/500] [Batch 226/312] [D IS TRAINING] [D loss: 0.1087] [G loss: 2.9448]\n",
      "[Epoch 5/500] [Batch 227/312] [D IS TRAINING] [D loss: 0.1406] [G loss: 2.1029]\n",
      "[Epoch 5/500] [Batch 228/312] [D IS TRAINING] [D loss: 0.1391] [G loss: 3.0718]\n",
      "[Epoch 5/500] [Batch 229/312] [D IS TRAINING] [D loss: 0.1328] [G loss: 2.2896]\n",
      "[Epoch 5/500] [Batch 230/312] [D IS TRAINING] [D loss: 0.1317] [G loss: 2.2169]\n",
      "[Epoch 5/500] [Batch 231/312] [D IS TRAINING] [D loss: 0.1520] [G loss: 2.1403]\n",
      "[Epoch 5/500] [Batch 232/312] [D IS TRAINING] [D loss: 0.2438] [G loss: 4.1995]\n",
      "[Epoch 5/500] [Batch 233/312] [D IS TRAINING] [D loss: 0.7612] [G loss: 0.4347]\n",
      "[Epoch 5/500] [Batch 234/312] [D IS TRAINING] [D loss: 0.9917] [G loss: 5.9437]\n",
      "[Epoch 5/500] [Batch 235/312] [D IS TRAINING] [D loss: 0.3116] [G loss: 1.9295]\n",
      "[Epoch 5/500] [Batch 236/312] [D IS TRAINING] [D loss: 0.2528] [G loss: 1.3116]\n",
      "[Epoch 5/500] [Batch 237/312] [D IS TRAINING] [D loss: 0.2390] [G loss: 2.2043]\n",
      "[Epoch 5/500] [Batch 238/312] [D IS TRAINING] [D loss: 0.3017] [G loss: 3.0911]\n",
      "[Epoch 5/500] [Batch 239/312] [D IS TRAINING] [D loss: 0.2104] [G loss: 1.6391]\n",
      "[Epoch 5/500] [Batch 240/312] [D IS TRAINING] [D loss: 0.2110] [G loss: 1.9245]\n",
      "[Epoch 5/500] [Batch 241/312] [D IS TRAINING] [D loss: 0.2102] [G loss: 2.3610]\n",
      "[Epoch 5/500] [Batch 242/312] [D IS TRAINING] [D loss: 0.2120] [G loss: 2.1265]\n",
      "[Epoch 5/500] [Batch 243/312] [D IS TRAINING] [D loss: 0.2052] [G loss: 1.6359]\n",
      "[Epoch 5/500] [Batch 244/312] [D IS TRAINING] [D loss: 0.2565] [G loss: 2.6082]\n",
      "[Epoch 5/500] [Batch 245/312] [D IS TRAINING] [D loss: 0.2769] [G loss: 1.3443]\n",
      "[Epoch 5/500] [Batch 246/312] [D IS TRAINING] [D loss: 0.2778] [G loss: 2.0630]\n",
      "[Epoch 5/500] [Batch 247/312] [D IS TRAINING] [D loss: 0.2849] [G loss: 1.4389]\n",
      "[Epoch 5/500] [Batch 248/312] [D IS TRAINING] [D loss: 0.2373] [G loss: 2.4975]\n",
      "[Epoch 5/500] [Batch 249/312] [D IS TRAINING] [D loss: 0.2072] [G loss: 1.6839]\n",
      "[Epoch 5/500] [Batch 250/312] [D IS TRAINING] [D loss: 0.2442] [G loss: 2.3441]\n",
      "[Epoch 5/500] [Batch 251/312] [D IS TRAINING] [D loss: 0.2077] [G loss: 1.8978]\n",
      "[Epoch 5/500] [Batch 252/312] [D IS TRAINING] [D loss: 0.1686] [G loss: 2.1049]\n",
      "[Epoch 5/500] [Batch 253/312] [D IS TRAINING] [D loss: 0.1688] [G loss: 2.9717]\n",
      "[Epoch 5/500] [Batch 254/312] [D IS TRAINING] [D loss: 0.1750] [G loss: 1.5838]\n",
      "[Epoch 5/500] [Batch 255/312] [D IS TRAINING] [D loss: 0.2115] [G loss: 2.1259]\n",
      "[Epoch 5/500] [Batch 256/312] [D IS TRAINING] [D loss: 0.1834] [G loss: 2.1054]\n",
      "[Epoch 5/500] [Batch 257/312] [D IS TRAINING] [D loss: 0.1319] [G loss: 3.0787]\n",
      "[Epoch 5/500] [Batch 258/312] [D IS TRAINING] [D loss: 0.3382] [G loss: 1.0077]\n",
      "[Epoch 5/500] [Batch 259/312] [D IS TRAINING] [D loss: 0.4868] [G loss: 4.4121]\n",
      "[Epoch 5/500] [Batch 260/312] [D IS TRAINING] [D loss: 0.3470] [G loss: 1.7875]\n",
      "[Epoch 5/500] [Batch 261/312] [D IS TRAINING] [D loss: 0.2889] [G loss: 2.4243]\n",
      "[Epoch 5/500] [Batch 262/312] [D IS TRAINING] [D loss: 0.2736] [G loss: 3.0961]\n",
      "[Epoch 5/500] [Batch 263/312] [D IS TRAINING] [D loss: 0.2109] [G loss: 1.6228]\n",
      "[Epoch 5/500] [Batch 264/312] [D IS TRAINING] [D loss: 0.2166] [G loss: 3.5378]\n",
      "[Epoch 5/500] [Batch 265/312] [D IS TRAINING] [D loss: 0.2152] [G loss: 1.5669]\n",
      "[Epoch 5/500] [Batch 266/312] [D IS TRAINING] [D loss: 0.2649] [G loss: 2.8704]\n",
      "[Epoch 5/500] [Batch 267/312] [D IS TRAINING] [D loss: 0.1921] [G loss: 1.8616]\n",
      "[Epoch 5/500] [Batch 268/312] [D IS TRAINING] [D loss: 0.1430] [G loss: 2.6569]\n",
      "[Epoch 5/500] [Batch 269/312] [D IS TRAINING] [D loss: 0.1965] [G loss: 2.2359]\n",
      "[Epoch 5/500] [Batch 270/312] [D IS TRAINING] [D loss: 0.1880] [G loss: 1.8812]\n",
      "[Epoch 5/500] [Batch 271/312] [D IS TRAINING] [D loss: 0.1543] [G loss: 2.8244]\n",
      "[Epoch 5/500] [Batch 272/312] [D IS TRAINING] [D loss: 0.1287] [G loss: 2.4226]\n",
      "[Epoch 5/500] [Batch 273/312] [D IS TRAINING] [D loss: 0.1509] [G loss: 2.3238]\n",
      "[Epoch 5/500] [Batch 274/312] [D IS TRAINING] [D loss: 0.2050] [G loss: 1.8689]\n",
      "[Epoch 5/500] [Batch 275/312] [D IS TRAINING] [D loss: 0.1518] [G loss: 2.6758]\n",
      "[Epoch 5/500] [Batch 276/312] [D IS TRAINING] [D loss: 0.2037] [G loss: 1.6198]\n",
      "[Epoch 5/500] [Batch 277/312] [D IS TRAINING] [D loss: 0.3030] [G loss: 3.3375]\n",
      "[Epoch 5/500] [Batch 278/312] [D IS TRAINING] [D loss: 0.4082] [G loss: 0.8211]\n",
      "[Epoch 5/500] [Batch 279/312] [D IS TRAINING] [D loss: 0.2367] [G loss: 4.4931]\n",
      "[Epoch 5/500] [Batch 280/312] [D IS TRAINING] [D loss: 0.0925] [G loss: 3.4019]\n",
      "[Epoch 5/500] [Batch 281/312] [D IS TRAINING] [D loss: 0.1515] [G loss: 1.9419]\n",
      "[Epoch 5/500] [Batch 282/312] [D IS TRAINING] [D loss: 0.2509] [G loss: 1.4861]\n",
      "[Epoch 5/500] [Batch 283/312] [D IS TRAINING] [D loss: 0.4350] [G loss: 3.4403]\n",
      "[Epoch 5/500] [Batch 284/312] [D IS TRAINING] [D loss: 0.5365] [G loss: 0.7257]\n",
      "[Epoch 5/500] [Batch 285/312] [D IS TRAINING] [D loss: 0.5011] [G loss: 4.4736]\n",
      "[Epoch 5/500] [Batch 286/312] [D IS TRAINING] [D loss: 0.3638] [G loss: 1.2356]\n",
      "[Epoch 5/500] [Batch 287/312] [D IS TRAINING] [D loss: 0.1962] [G loss: 2.7149]\n",
      "[Epoch 5/500] [Batch 288/312] [D IS TRAINING] [D loss: 0.1761] [G loss: 2.0798]\n",
      "[Epoch 5/500] [Batch 289/312] [D IS TRAINING] [D loss: 0.2480] [G loss: 2.3022]\n",
      "[Epoch 5/500] [Batch 290/312] [D IS TRAINING] [D loss: 0.2252] [G loss: 1.7436]\n",
      "[Epoch 5/500] [Batch 291/312] [D IS TRAINING] [D loss: 0.2457] [G loss: 2.4093]\n",
      "[Epoch 5/500] [Batch 292/312] [D IS TRAINING] [D loss: 0.2617] [G loss: 1.4477]\n",
      "[Epoch 5/500] [Batch 293/312] [D IS TRAINING] [D loss: 0.3037] [G loss: 3.7211]\n",
      "[Epoch 5/500] [Batch 294/312] [D IS TRAINING] [D loss: 0.3366] [G loss: 0.9511]\n",
      "[Epoch 5/500] [Batch 295/312] [D IS TRAINING] [D loss: 0.3198] [G loss: 5.2399]\n",
      "[Epoch 5/500] [Batch 296/312] [D IS TRAINING] [D loss: 0.1028] [G loss: 2.4236]\n",
      "[Epoch 5/500] [Batch 297/312] [D IS TRAINING] [D loss: 0.2601] [G loss: 1.4350]\n",
      "[Epoch 5/500] [Batch 298/312] [D IS TRAINING] [D loss: 0.2208] [G loss: 4.3382]\n",
      "[Epoch 5/500] [Batch 299/312] [D IS TRAINING] [D loss: 0.1209] [G loss: 2.5109]\n",
      "[Epoch 5/500] [Batch 300/312] [D IS TRAINING] [D loss: 0.1762] [G loss: 1.8230]\n",
      "[Epoch 5/500] [Batch 301/312] [D IS TRAINING] [D loss: 0.1136] [G loss: 2.3613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/500] [Batch 302/312] [D IS TRAINING] [D loss: 0.1836] [G loss: 2.6554]\n",
      "[Epoch 5/500] [Batch 303/312] [D IS TRAINING] [D loss: 0.3362] [G loss: 1.1151]\n",
      "[Epoch 5/500] [Batch 304/312] [D IS TRAINING] [D loss: 0.4531] [G loss: 2.9784]\n",
      "[Epoch 5/500] [Batch 305/312] [D IS TRAINING] [D loss: 0.3305] [G loss: 1.0173]\n",
      "[Epoch 5/500] [Batch 306/312] [D IS TRAINING] [D loss: 0.1226] [G loss: 3.1340]\n",
      "[Epoch 5/500] [Batch 307/312] [D IS TRAINING] [D loss: 0.1004] [G loss: 3.1319]\n",
      "[Epoch 5/500] [Batch 308/312] [D IS TRAINING] [D loss: 0.1151] [G loss: 2.6986]\n",
      "[Epoch 5/500] [Batch 309/312] [D IS TRAINING] [D loss: 0.1198] [G loss: 2.0494]\n",
      "[Epoch 5/500] [Batch 310/312] [D IS TRAINING] [D loss: 0.1333] [G loss: 2.0016]\n",
      "[Epoch 5/500] [Batch 311/312] [D IS TRAINING] [D loss: 0.1986] [G loss: 2.9603]\n",
      "[Epoch 6/500] [Batch 0/312] [D IS TRAINING] [D loss: 0.2289] [G loss: 1.6097]\n",
      "[Epoch 6/500] [Batch 1/312] [D IS TRAINING] [D loss: 0.1973] [G loss: 1.9668]\n",
      "[Epoch 6/500] [Batch 2/312] [D IS TRAINING] [D loss: 0.2758] [G loss: 1.9135]\n",
      "[Epoch 6/500] [Batch 3/312] [D IS TRAINING] [D loss: 0.2523] [G loss: 1.7325]\n",
      "[Epoch 6/500] [Batch 4/312] [D IS TRAINING] [D loss: 0.2143] [G loss: 2.4875]\n",
      "[Epoch 6/500] [Batch 5/312] [D IS TRAINING] [D loss: 0.3144] [G loss: 1.6858]\n",
      "[Epoch 6/500] [Batch 6/312] [D IS TRAINING] [D loss: 0.2672] [G loss: 1.8030]\n",
      "[Epoch 6/500] [Batch 7/312] [D IS TRAINING] [D loss: 0.2803] [G loss: 2.4539]\n",
      "[Epoch 6/500] [Batch 8/312] [D IS TRAINING] [D loss: 0.3291] [G loss: 1.2166]\n",
      "[Epoch 6/500] [Batch 9/312] [D IS TRAINING] [D loss: 0.5506] [G loss: 4.5558]\n",
      "[Epoch 6/500] [Batch 10/312] [D IS TRAINING] [D loss: 0.6287] [G loss: 0.4989]\n",
      "[Epoch 6/500] [Batch 11/312] [D IS TRAINING] [D loss: 0.8009] [G loss: 5.7218]\n",
      "[Epoch 6/500] [Batch 12/312] [D IS TRAINING] [D loss: 0.2369] [G loss: 1.7067]\n",
      "[Epoch 6/500] [Batch 13/312] [D IS TRAINING] [D loss: 0.1996] [G loss: 1.5525]\n",
      "[Epoch 6/500] [Batch 14/312] [D IS TRAINING] [D loss: 0.2938] [G loss: 2.9120]\n",
      "[Epoch 6/500] [Batch 15/312] [D IS TRAINING] [D loss: 0.2366] [G loss: 1.5424]\n",
      "[Epoch 6/500] [Batch 16/312] [D IS TRAINING] [D loss: 0.1977] [G loss: 2.9548]\n",
      "[Epoch 6/500] [Batch 17/312] [D IS TRAINING] [D loss: 0.1882] [G loss: 1.7288]\n",
      "[Epoch 6/500] [Batch 18/312] [D IS TRAINING] [D loss: 0.1959] [G loss: 2.1124]\n",
      "[Epoch 6/500] [Batch 19/312] [D IS TRAINING] [D loss: 0.1877] [G loss: 2.4812]\n",
      "[Epoch 6/500] [Batch 20/312] [D IS TRAINING] [D loss: 0.2192] [G loss: 1.5069]\n",
      "[Epoch 6/500] [Batch 21/312] [D IS TRAINING] [D loss: 0.1997] [G loss: 2.5238]\n",
      "[Epoch 6/500] [Batch 22/312] [D IS TRAINING] [D loss: 0.1519] [G loss: 2.2492]\n",
      "[Epoch 6/500] [Batch 23/312] [D IS TRAINING] [D loss: 0.2031] [G loss: 1.8196]\n",
      "[Epoch 6/500] [Batch 24/312] [D IS TRAINING] [D loss: 0.1599] [G loss: 2.2280]\n",
      "[Epoch 6/500] [Batch 25/312] [D IS TRAINING] [D loss: 0.1566] [G loss: 2.0263]\n",
      "[Epoch 6/500] [Batch 26/312] [D IS TRAINING] [D loss: 0.2196] [G loss: 2.2405]\n",
      "[Epoch 6/500] [Batch 27/312] [D IS TRAINING] [D loss: 0.2005] [G loss: 1.5144]\n",
      "[Epoch 6/500] [Batch 28/312] [D IS TRAINING] [D loss: 0.2037] [G loss: 2.3500]\n",
      "[Epoch 6/500] [Batch 29/312] [D IS TRAINING] [D loss: 0.2589] [G loss: 1.6421]\n",
      "[Epoch 6/500] [Batch 30/312] [D IS TRAINING] [D loss: 0.3982] [G loss: 3.1000]\n",
      "[Epoch 6/500] [Batch 31/312] [D IS TRAINING] [D loss: 0.8020] [G loss: 0.4349]\n",
      "[Epoch 6/500] [Batch 32/312] [D IS TRAINING] [D loss: 1.0396] [G loss: 6.2305]\n",
      "[Epoch 6/500] [Batch 33/312] [D IS TRAINING] [D loss: 0.2350] [G loss: 1.8462]\n",
      "[Epoch 6/500] [Batch 34/312] [D IS TRAINING] [D loss: 0.2849] [G loss: 1.1687]\n",
      "[Epoch 6/500] [Batch 35/312] [D IS TRAINING] [D loss: 0.3082] [G loss: 3.4131]\n",
      "[Epoch 6/500] [Batch 36/312] [D IS TRAINING] [D loss: 0.2667] [G loss: 1.8607]\n",
      "[Epoch 6/500] [Batch 37/312] [D IS TRAINING] [D loss: 0.3680] [G loss: 0.9275]\n",
      "[Epoch 6/500] [Batch 38/312] [D IS TRAINING] [D loss: 0.4808] [G loss: 2.7777]\n",
      "[Epoch 6/500] [Batch 39/312] [D IS TRAINING] [D loss: 0.3540] [G loss: 1.0497]\n",
      "[Epoch 6/500] [Batch 40/312] [D IS TRAINING] [D loss: 0.3585] [G loss: 2.4378]\n",
      "[Epoch 6/500] [Batch 41/312] [D IS TRAINING] [D loss: 0.2829] [G loss: 1.6218]\n",
      "[Epoch 6/500] [Batch 42/312] [D IS TRAINING] [D loss: 0.1805] [G loss: 2.1447]\n",
      "[Epoch 6/500] [Batch 43/312] [D IS TRAINING] [D loss: 0.1474] [G loss: 3.2162]\n",
      "[Epoch 6/500] [Batch 44/312] [D IS TRAINING] [D loss: 0.1560] [G loss: 2.1974]\n",
      "[Epoch 6/500] [Batch 45/312] [D IS TRAINING] [D loss: 0.1520] [G loss: 1.9641]\n",
      "[Epoch 6/500] [Batch 46/312] [D IS TRAINING] [D loss: 0.1630] [G loss: 3.0978]\n",
      "[Epoch 6/500] [Batch 47/312] [D IS TRAINING] [D loss: 0.1571] [G loss: 1.7569]\n",
      "[Epoch 6/500] [Batch 48/312] [D IS TRAINING] [D loss: 0.1242] [G loss: 2.4664]\n",
      "[Epoch 6/500] [Batch 49/312] [D IS TRAINING] [D loss: 0.2480] [G loss: 2.1409]\n",
      "[Epoch 6/500] [Batch 50/312] [D IS TRAINING] [D loss: 0.2954] [G loss: 1.2576]\n",
      "[Epoch 6/500] [Batch 51/312] [D IS TRAINING] [D loss: 0.2654] [G loss: 3.3306]\n",
      "[Epoch 6/500] [Batch 52/312] [D IS TRAINING] [D loss: 0.3178] [G loss: 1.0183]\n",
      "[Epoch 6/500] [Batch 53/312] [D IS TRAINING] [D loss: 0.3761] [G loss: 4.0715]\n",
      "[Epoch 6/500] [Batch 54/312] [D IS TRAINING] [D loss: 0.2996] [G loss: 1.2377]\n",
      "[Epoch 6/500] [Batch 55/312] [D IS TRAINING] [D loss: 0.1950] [G loss: 2.5852]\n",
      "[Epoch 6/500] [Batch 56/312] [D IS TRAINING] [D loss: 0.2107] [G loss: 2.8232]\n",
      "[Epoch 6/500] [Batch 57/312] [D IS TRAINING] [D loss: 0.2426] [G loss: 1.5768]\n",
      "[Epoch 6/500] [Batch 58/312] [D IS TRAINING] [D loss: 0.1934] [G loss: 2.1820]\n",
      "[Epoch 6/500] [Batch 59/312] [D IS TRAINING] [D loss: 0.1761] [G loss: 2.4053]\n",
      "[Epoch 6/500] [Batch 60/312] [D IS TRAINING] [D loss: 0.1985] [G loss: 2.1591]\n",
      "[Epoch 6/500] [Batch 61/312] [D IS TRAINING] [D loss: 0.2141] [G loss: 1.7325]\n",
      "[Epoch 6/500] [Batch 62/312] [D IS TRAINING] [D loss: 0.1278] [G loss: 2.5570]\n",
      "[Epoch 6/500] [Batch 63/312] [D IS TRAINING] [D loss: 0.1057] [G loss: 2.7377]\n",
      "[Epoch 6/500] [Batch 64/312] [D IS TRAINING] [D loss: 0.1000] [G loss: 2.5635]\n",
      "[Epoch 6/500] [Batch 65/312] [D IS TRAINING] [D loss: 0.1300] [G loss: 2.2790]\n",
      "[Epoch 6/500] [Batch 66/312] [D IS TRAINING] [D loss: 0.1335] [G loss: 2.2597]\n",
      "[Epoch 6/500] [Batch 67/312] [D IS TRAINING] [D loss: 0.1506] [G loss: 2.1199]\n",
      "[Epoch 6/500] [Batch 68/312] [D IS TRAINING] [D loss: 0.1812] [G loss: 2.4461]\n",
      "[Epoch 6/500] [Batch 69/312] [D IS TRAINING] [D loss: 0.1736] [G loss: 1.7877]\n",
      "[Epoch 6/500] [Batch 70/312] [D IS TRAINING] [D loss: 0.2953] [G loss: 2.4114]\n",
      "[Epoch 6/500] [Batch 71/312] [D IS TRAINING] [D loss: 0.4882] [G loss: 0.6167]\n",
      "[Epoch 6/500] [Batch 72/312] [D IS TRAINING] [D loss: 0.7179] [G loss: 4.5269]\n",
      "[Epoch 6/500] [Batch 73/312] [D IS TRAINING] [D loss: 0.2426] [G loss: 1.8575]\n",
      "[Epoch 6/500] [Batch 74/312] [D IS TRAINING] [D loss: 0.2600] [G loss: 1.3369]\n",
      "[Epoch 6/500] [Batch 75/312] [D IS TRAINING] [D loss: 0.2599] [G loss: 3.0295]\n",
      "[Epoch 6/500] [Batch 76/312] [D IS TRAINING] [D loss: 0.2900] [G loss: 2.0185]\n",
      "[Epoch 6/500] [Batch 77/312] [D IS TRAINING] [D loss: 0.2438] [G loss: 1.6195]\n",
      "[Epoch 6/500] [Batch 78/312] [D IS TRAINING] [D loss: 0.2033] [G loss: 2.5024]\n",
      "[Epoch 6/500] [Batch 79/312] [D IS TRAINING] [D loss: 0.2862] [G loss: 1.5226]\n",
      "[Epoch 6/500] [Batch 80/312] [D IS TRAINING] [D loss: 0.3150] [G loss: 2.3379]\n",
      "[Epoch 6/500] [Batch 81/312] [D IS TRAINING] [D loss: 0.2412] [G loss: 1.5101]\n",
      "[Epoch 6/500] [Batch 82/312] [D IS TRAINING] [D loss: 0.1854] [G loss: 2.4136]\n",
      "[Epoch 6/500] [Batch 83/312] [D IS TRAINING] [D loss: 0.1541] [G loss: 1.9735]\n",
      "[Epoch 6/500] [Batch 84/312] [D IS TRAINING] [D loss: 0.1982] [G loss: 1.8191]\n",
      "[Epoch 6/500] [Batch 85/312] [D IS TRAINING] [D loss: 0.3188] [G loss: 1.9848]\n",
      "[Epoch 6/500] [Batch 86/312] [D IS TRAINING] [D loss: 0.3531] [G loss: 1.1689]\n",
      "[Epoch 6/500] [Batch 87/312] [D IS TRAINING] [D loss: 0.4164] [G loss: 3.2982]\n",
      "[Epoch 6/500] [Batch 88/312] [D IS TRAINING] [D loss: 0.6016] [G loss: 0.6138]\n",
      "[Epoch 6/500] [Batch 89/312] [D IS TRAINING] [D loss: 0.9012] [G loss: 5.3280]\n",
      "[Epoch 6/500] [Batch 90/312] [D IS TRAINING] [D loss: 0.4059] [G loss: 1.2384]\n",
      "[Epoch 6/500] [Batch 91/312] [D IS TRAINING] [D loss: 0.1892] [G loss: 2.0511]\n",
      "[Epoch 6/500] [Batch 92/312] [D IS TRAINING] [D loss: 0.1853] [G loss: 3.0785]\n",
      "[Epoch 6/500] [Batch 93/312] [D IS TRAINING] [D loss: 0.1951] [G loss: 1.7447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/500] [Batch 94/312] [D IS TRAINING] [D loss: 0.1976] [G loss: 2.1765]\n",
      "[Epoch 6/500] [Batch 95/312] [D IS TRAINING] [D loss: 0.1859] [G loss: 1.8444]\n",
      "[Epoch 6/500] [Batch 96/312] [D IS TRAINING] [D loss: 0.2848] [G loss: 2.3678]\n",
      "[Epoch 6/500] [Batch 97/312] [D IS TRAINING] [D loss: 0.3076] [G loss: 1.2647]\n",
      "[Epoch 6/500] [Batch 98/312] [D IS TRAINING] [D loss: 0.3844] [G loss: 2.8131]\n",
      "[Epoch 6/500] [Batch 99/312] [D IS TRAINING] [D loss: 0.1821] [G loss: 2.1608]\n",
      "[Epoch 6/500] [Batch 100/312] [D IS TRAINING] [D loss: 0.1682] [G loss: 2.0255]\n",
      "[Epoch 6/500] [Batch 101/312] [D IS TRAINING] [D loss: 0.2272] [G loss: 2.5403]\n",
      "[Epoch 6/500] [Batch 102/312] [D IS TRAINING] [D loss: 0.1853] [G loss: 2.2560]\n",
      "[Epoch 6/500] [Batch 103/312] [D IS TRAINING] [D loss: 0.2550] [G loss: 1.7685]\n",
      "[Epoch 6/500] [Batch 104/312] [D IS TRAINING] [D loss: 0.2656] [G loss: 1.8109]\n",
      "[Epoch 6/500] [Batch 105/312] [D IS TRAINING] [D loss: 0.2639] [G loss: 3.0709]\n",
      "[Epoch 6/500] [Batch 106/312] [D IS TRAINING] [D loss: 0.3195] [G loss: 1.0440]\n",
      "[Epoch 6/500] [Batch 107/312] [D IS TRAINING] [D loss: 0.2890] [G loss: 3.3464]\n",
      "[Epoch 6/500] [Batch 108/312] [D IS TRAINING] [D loss: 0.2790] [G loss: 1.4580]\n",
      "[Epoch 6/500] [Batch 109/312] [D IS TRAINING] [D loss: 0.2193] [G loss: 2.8171]\n",
      "[Epoch 6/500] [Batch 110/312] [D IS TRAINING] [D loss: 0.2596] [G loss: 1.2719]\n",
      "[Epoch 6/500] [Batch 111/312] [D IS TRAINING] [D loss: 0.2918] [G loss: 3.9415]\n",
      "[Epoch 6/500] [Batch 112/312] [D IS TRAINING] [D loss: 0.2020] [G loss: 1.4689]\n",
      "[Epoch 6/500] [Batch 113/312] [D IS TRAINING] [D loss: 0.1481] [G loss: 2.7311]\n",
      "[Epoch 6/500] [Batch 114/312] [D IS TRAINING] [D loss: 0.1518] [G loss: 2.6328]\n",
      "[Epoch 6/500] [Batch 115/312] [D IS TRAINING] [D loss: 0.1607] [G loss: 1.8446]\n",
      "[Epoch 6/500] [Batch 116/312] [D IS TRAINING] [D loss: 0.2264] [G loss: 2.1099]\n",
      "[Epoch 6/500] [Batch 117/312] [D IS TRAINING] [D loss: 0.2273] [G loss: 2.6666]\n",
      "[Epoch 6/500] [Batch 118/312] [D IS TRAINING] [D loss: 0.6741] [G loss: 0.5407]\n",
      "[Epoch 6/500] [Batch 119/312] [D IS TRAINING] [D loss: 1.0598] [G loss: 5.6087]\n",
      "[Epoch 6/500] [Batch 120/312] [D IS TRAINING] [D loss: 0.2439] [G loss: 1.7670]\n",
      "[Epoch 6/500] [Batch 121/312] [D IS TRAINING] [D loss: 0.2463] [G loss: 1.4103]\n",
      "[Epoch 6/500] [Batch 122/312] [D IS TRAINING] [D loss: 0.3101] [G loss: 3.3379]\n",
      "[Epoch 6/500] [Batch 123/312] [D IS TRAINING] [D loss: 0.2057] [G loss: 1.7329]\n",
      "[Epoch 6/500] [Batch 124/312] [D IS TRAINING] [D loss: 0.2260] [G loss: 1.4175]\n",
      "[Epoch 6/500] [Batch 125/312] [D IS TRAINING] [D loss: 0.2835] [G loss: 2.2068]\n",
      "[Epoch 6/500] [Batch 126/312] [D IS TRAINING] [D loss: 0.2701] [G loss: 1.8903]\n",
      "[Epoch 6/500] [Batch 127/312] [D IS TRAINING] [D loss: 0.4033] [G loss: 1.1939]\n",
      "[Epoch 6/500] [Batch 128/312] [D IS TRAINING] [D loss: 0.3181] [G loss: 2.1974]\n",
      "[Epoch 6/500] [Batch 129/312] [D IS TRAINING] [D loss: 0.2779] [G loss: 1.3629]\n",
      "[Epoch 6/500] [Batch 130/312] [D IS TRAINING] [D loss: 0.2695] [G loss: 3.1405]\n",
      "[Epoch 6/500] [Batch 131/312] [D IS TRAINING] [D loss: 0.1648] [G loss: 1.9893]\n",
      "[Epoch 6/500] [Batch 132/312] [D IS TRAINING] [D loss: 0.1834] [G loss: 1.9213]\n",
      "[Epoch 6/500] [Batch 133/312] [D IS TRAINING] [D loss: 0.1985] [G loss: 1.9724]\n",
      "[Epoch 6/500] [Batch 134/312] [D IS TRAINING] [D loss: 0.1824] [G loss: 2.1467]\n",
      "[Epoch 6/500] [Batch 135/312] [D IS TRAINING] [D loss: 0.3099] [G loss: 1.6351]\n",
      "[Epoch 6/500] [Batch 136/312] [D IS TRAINING] [D loss: 0.2951] [G loss: 2.1861]\n",
      "[Epoch 6/500] [Batch 137/312] [D IS TRAINING] [D loss: 0.5104] [G loss: 1.0047]\n",
      "[Epoch 6/500] [Batch 138/312] [D IS TRAINING] [D loss: 0.6997] [G loss: 4.7154]\n",
      "[Epoch 6/500] [Batch 139/312] [D IS TRAINING] [D loss: 0.3618] [G loss: 1.7200]\n",
      "[Epoch 6/500] [Batch 140/312] [D IS TRAINING] [D loss: 0.1857] [G loss: 2.5692]\n",
      "[Epoch 6/500] [Batch 141/312] [D IS TRAINING] [D loss: 0.1827] [G loss: 2.8669]\n",
      "[Epoch 6/500] [Batch 142/312] [D IS TRAINING] [D loss: 0.1665] [G loss: 2.3619]\n",
      "[Epoch 6/500] [Batch 143/312] [D IS TRAINING] [D loss: 0.2066] [G loss: 1.5777]\n",
      "[Epoch 6/500] [Batch 144/312] [D IS TRAINING] [D loss: 0.2078] [G loss: 2.6579]\n",
      "[Epoch 6/500] [Batch 145/312] [D IS TRAINING] [D loss: 0.3162] [G loss: 1.3435]\n",
      "[Epoch 6/500] [Batch 146/312] [D IS TRAINING] [D loss: 0.4116] [G loss: 2.8212]\n",
      "[Epoch 6/500] [Batch 147/312] [D IS TRAINING] [D loss: 0.3901] [G loss: 0.9912]\n",
      "[Epoch 6/500] [Batch 148/312] [D IS TRAINING] [D loss: 0.2540] [G loss: 2.5066]\n",
      "[Epoch 6/500] [Batch 149/312] [D IS TRAINING] [D loss: 0.3340] [G loss: 2.2745]\n",
      "[Epoch 6/500] [Batch 150/312] [D IS TRAINING] [D loss: 0.4837] [G loss: 0.7947]\n",
      "[Epoch 6/500] [Batch 151/312] [D IS TRAINING] [D loss: 0.5929] [G loss: 4.1088]\n",
      "[Epoch 6/500] [Batch 152/312] [D IS TRAINING] [D loss: 0.2703] [G loss: 1.4708]\n",
      "[Epoch 6/500] [Batch 153/312] [D IS TRAINING] [D loss: 0.1722] [G loss: 1.9105]\n",
      "[Epoch 6/500] [Batch 154/312] [D IS TRAINING] [D loss: 0.2293] [G loss: 2.3852]\n",
      "[Epoch 6/500] [Batch 155/312] [D IS TRAINING] [D loss: 0.1469] [G loss: 2.1082]\n",
      "[Epoch 6/500] [Batch 156/312] [D IS TRAINING] [D loss: 0.2279] [G loss: 1.6548]\n",
      "[Epoch 6/500] [Batch 157/312] [D IS TRAINING] [D loss: 0.1772] [G loss: 2.3223]\n",
      "[Epoch 6/500] [Batch 158/312] [D IS TRAINING] [D loss: 0.2362] [G loss: 1.4200]\n",
      "[Epoch 6/500] [Batch 159/312] [D IS TRAINING] [D loss: 0.3134] [G loss: 1.4683]\n",
      "[Epoch 6/500] [Batch 160/312] [D IS TRAINING] [D loss: 0.4238] [G loss: 3.0758]\n",
      "[Epoch 6/500] [Batch 161/312] [D IS TRAINING] [D loss: 0.3211] [G loss: 1.4863]\n",
      "[Epoch 6/500] [Batch 162/312] [D IS TRAINING] [D loss: 0.2604] [G loss: 1.8003]\n",
      "[Epoch 6/500] [Batch 163/312] [D IS TRAINING] [D loss: 0.2852] [G loss: 2.7667]\n",
      "[Epoch 6/500] [Batch 164/312] [D IS TRAINING] [D loss: 0.2363] [G loss: 1.7401]\n",
      "[Epoch 6/500] [Batch 165/312] [D IS TRAINING] [D loss: 0.2180] [G loss: 1.5761]\n",
      "[Epoch 6/500] [Batch 166/312] [D IS TRAINING] [D loss: 0.1979] [G loss: 2.5508]\n",
      "[Epoch 6/500] [Batch 167/312] [D IS TRAINING] [D loss: 0.1742] [G loss: 2.1859]\n",
      "[Epoch 6/500] [Batch 168/312] [D IS TRAINING] [D loss: 0.1842] [G loss: 1.9311]\n",
      "[Epoch 6/500] [Batch 169/312] [D IS TRAINING] [D loss: 0.1752] [G loss: 1.9451]\n",
      "[Epoch 6/500] [Batch 170/312] [D IS TRAINING] [D loss: 0.1537] [G loss: 3.2049]\n",
      "[Epoch 6/500] [Batch 171/312] [D IS TRAINING] [D loss: 0.1792] [G loss: 1.9643]\n",
      "[Epoch 6/500] [Batch 172/312] [D IS TRAINING] [D loss: 0.2509] [G loss: 1.5009]\n",
      "[Epoch 6/500] [Batch 173/312] [D IS TRAINING] [D loss: 0.2196] [G loss: 2.7994]\n",
      "[Epoch 6/500] [Batch 174/312] [D IS TRAINING] [D loss: 0.2172] [G loss: 2.0921]\n",
      "[Epoch 6/500] [Batch 175/312] [D IS TRAINING] [D loss: 0.2675] [G loss: 1.5416]\n",
      "[Epoch 6/500] [Batch 176/312] [D IS TRAINING] [D loss: 0.2211] [G loss: 2.0737]\n",
      "[Epoch 6/500] [Batch 177/312] [D IS TRAINING] [D loss: 0.2863] [G loss: 2.2573]\n",
      "[Epoch 6/500] [Batch 178/312] [D IS TRAINING] [D loss: 0.2868] [G loss: 1.3432]\n",
      "[Epoch 6/500] [Batch 179/312] [D IS TRAINING] [D loss: 0.2487] [G loss: 2.9793]\n",
      "[Epoch 6/500] [Batch 180/312] [D IS TRAINING] [D loss: 0.2199] [G loss: 1.6846]\n",
      "[Epoch 6/500] [Batch 181/312] [D IS TRAINING] [D loss: 0.2432] [G loss: 2.1168]\n",
      "[Epoch 6/500] [Batch 182/312] [D IS TRAINING] [D loss: 0.1800] [G loss: 2.6994]\n",
      "[Epoch 6/500] [Batch 183/312] [D IS TRAINING] [D loss: 0.1172] [G loss: 2.4066]\n",
      "[Epoch 6/500] [Batch 184/312] [D IS TRAINING] [D loss: 0.3443] [G loss: 0.9916]\n",
      "[Epoch 6/500] [Batch 185/312] [D IS TRAINING] [D loss: 0.6049] [G loss: 4.3958]\n",
      "[Epoch 6/500] [Batch 186/312] [D IS TRAINING] [D loss: 0.3919] [G loss: 1.0895]\n",
      "[Epoch 6/500] [Batch 187/312] [D IS TRAINING] [D loss: 0.2076] [G loss: 2.5452]\n",
      "[Epoch 6/500] [Batch 188/312] [D IS TRAINING] [D loss: 0.2155] [G loss: 3.1363]\n",
      "[Epoch 6/500] [Batch 189/312] [D IS TRAINING] [D loss: 0.1791] [G loss: 1.7770]\n",
      "[Epoch 6/500] [Batch 190/312] [D IS TRAINING] [D loss: 0.2332] [G loss: 2.2484]\n",
      "[Epoch 6/500] [Batch 191/312] [D IS TRAINING] [D loss: 0.2024] [G loss: 1.6710]\n",
      "[Epoch 6/500] [Batch 192/312] [D IS TRAINING] [D loss: 0.1795] [G loss: 2.6095]\n",
      "[Epoch 6/500] [Batch 193/312] [D IS TRAINING] [D loss: 0.2170] [G loss: 1.8252]\n",
      "[Epoch 6/500] [Batch 194/312] [D IS TRAINING] [D loss: 0.2274] [G loss: 1.8785]\n",
      "[Epoch 6/500] [Batch 195/312] [D IS TRAINING] [D loss: 0.3409] [G loss: 1.7124]\n",
      "[Epoch 6/500] [Batch 196/312] [D IS TRAINING] [D loss: 0.3312] [G loss: 1.0595]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/500] [Batch 197/312] [D IS TRAINING] [D loss: 0.5040] [G loss: 3.1242]\n",
      "[Epoch 6/500] [Batch 198/312] [D IS TRAINING] [D loss: 0.6151] [G loss: 0.5656]\n",
      "[Epoch 6/500] [Batch 199/312] [D IS TRAINING] [D loss: 0.5461] [G loss: 3.4598]\n",
      "[Epoch 6/500] [Batch 200/312] [D IS TRAINING] [D loss: 0.3589] [G loss: 1.1090]\n",
      "[Epoch 6/500] [Batch 201/312] [D IS TRAINING] [D loss: 0.3186] [G loss: 1.5394]\n",
      "[Epoch 6/500] [Batch 202/312] [D IS TRAINING] [D loss: 0.2665] [G loss: 2.6177]\n",
      "[Epoch 6/500] [Batch 203/312] [D IS TRAINING] [D loss: 0.2709] [G loss: 1.3561]\n",
      "[Epoch 6/500] [Batch 204/312] [D IS TRAINING] [D loss: 0.2517] [G loss: 2.5127]\n",
      "[Epoch 6/500] [Batch 205/312] [D IS TRAINING] [D loss: 0.2259] [G loss: 1.7057]\n",
      "[Epoch 6/500] [Batch 206/312] [D IS TRAINING] [D loss: 0.2609] [G loss: 1.9569]\n",
      "[Epoch 6/500] [Batch 207/312] [D IS TRAINING] [D loss: 0.2801] [G loss: 1.6475]\n",
      "[Epoch 6/500] [Batch 208/312] [D IS TRAINING] [D loss: 0.2288] [G loss: 1.9065]\n",
      "[Epoch 6/500] [Batch 209/312] [D IS TRAINING] [D loss: 0.2386] [G loss: 2.1649]\n",
      "[Epoch 6/500] [Batch 210/312] [D IS TRAINING] [D loss: 0.1752] [G loss: 2.2391]\n",
      "[Epoch 6/500] [Batch 211/312] [D IS TRAINING] [D loss: 0.2117] [G loss: 1.8371]\n",
      "[Epoch 6/500] [Batch 212/312] [D IS TRAINING] [D loss: 0.2207] [G loss: 1.9695]\n",
      "[Epoch 6/500] [Batch 213/312] [D IS TRAINING] [D loss: 0.2721] [G loss: 1.8724]\n",
      "[Epoch 6/500] [Batch 214/312] [D IS TRAINING] [D loss: 0.1772] [G loss: 1.9405]\n",
      "[Epoch 6/500] [Batch 215/312] [D IS TRAINING] [D loss: 0.1669] [G loss: 2.4959]\n",
      "[Epoch 6/500] [Batch 216/312] [D IS TRAINING] [D loss: 0.1736] [G loss: 2.0741]\n",
      "[Epoch 6/500] [Batch 217/312] [D IS TRAINING] [D loss: 0.1908] [G loss: 1.7123]\n",
      "[Epoch 6/500] [Batch 218/312] [D IS TRAINING] [D loss: 0.1795] [G loss: 2.5302]\n",
      "[Epoch 6/500] [Batch 219/312] [D IS TRAINING] [D loss: 0.1744] [G loss: 2.0619]\n",
      "[Epoch 6/500] [Batch 220/312] [D IS TRAINING] [D loss: 0.1486] [G loss: 2.2756]\n",
      "[Epoch 6/500] [Batch 221/312] [D IS TRAINING] [D loss: 0.1534] [G loss: 2.5340]\n",
      "[Epoch 6/500] [Batch 222/312] [D IS TRAINING] [D loss: 0.1373] [G loss: 2.0877]\n",
      "[Epoch 6/500] [Batch 223/312] [D IS TRAINING] [D loss: 0.1988] [G loss: 2.0069]\n",
      "[Epoch 6/500] [Batch 224/312] [D IS TRAINING] [D loss: 0.1924] [G loss: 1.9547]\n",
      "[Epoch 6/500] [Batch 225/312] [D IS TRAINING] [D loss: 0.1657] [G loss: 2.6626]\n",
      "[Epoch 6/500] [Batch 226/312] [D IS TRAINING] [D loss: 0.1533] [G loss: 2.1691]\n",
      "[Epoch 6/500] [Batch 227/312] [D IS TRAINING] [D loss: 0.1353] [G loss: 2.1140]\n",
      "[Epoch 6/500] [Batch 228/312] [D IS TRAINING] [D loss: 0.1426] [G loss: 2.6325]\n",
      "[Epoch 6/500] [Batch 229/312] [D IS TRAINING] [D loss: 0.1367] [G loss: 3.6587]\n",
      "[Epoch 6/500] [Batch 230/312] [D IS TRAINING] [D loss: 0.4423] [G loss: 0.7290]\n",
      "[Epoch 6/500] [Batch 231/312] [D IS TRAINING] [D loss: 1.0898] [G loss: 6.2929]\n",
      "[Epoch 6/500] [Batch 232/312] [D IS TRAINING] [D loss: 0.1399] [G loss: 2.2182]\n",
      "[Epoch 6/500] [Batch 233/312] [D IS TRAINING] [D loss: 1.0278] [G loss: 0.2055]\n",
      "[Epoch 6/500] [Batch 234/312] [D IS TRAINING] [D loss: 1.9444] [G loss: 9.1550]\n",
      "[Epoch 6/500] [Batch 235/312] [D IS TRAINING] [D loss: 0.3058] [G loss: 3.2199]\n",
      "[Epoch 6/500] [Batch 236/312] [D IS TRAINING] [D loss: 1.3943] [G loss: 0.1228]\n",
      "[Epoch 6/500] [Batch 237/312] [D IS TRAINING] [D loss: 0.8487] [G loss: 4.6423]\n",
      "[Epoch 6/500] [Batch 238/312] [D IS TRAINING] [D loss: 0.4105] [G loss: 2.2057]\n",
      "[Epoch 6/500] [Batch 239/312] [D IS TRAINING] [D loss: 0.4869] [G loss: 1.0727]\n",
      "[Epoch 6/500] [Batch 240/312] [D IS TRAINING] [D loss: 0.3256] [G loss: 2.2026]\n",
      "[Epoch 6/500] [Batch 241/312] [D IS TRAINING] [D loss: 0.2548] [G loss: 2.5345]\n",
      "[Epoch 6/500] [Batch 242/312] [D IS TRAINING] [D loss: 0.2842] [G loss: 1.5174]\n",
      "[Epoch 6/500] [Batch 243/312] [D IS TRAINING] [D loss: 0.2840] [G loss: 1.9298]\n",
      "[Epoch 6/500] [Batch 244/312] [D IS TRAINING] [D loss: 0.2734] [G loss: 1.8394]\n",
      "[Epoch 6/500] [Batch 245/312] [D IS TRAINING] [D loss: 0.2274] [G loss: 1.8054]\n",
      "[Epoch 6/500] [Batch 246/312] [D IS TRAINING] [D loss: 0.2623] [G loss: 1.8009]\n",
      "[Epoch 6/500] [Batch 247/312] [D IS TRAINING] [D loss: 0.2081] [G loss: 1.8638]\n",
      "[Epoch 6/500] [Batch 248/312] [D IS TRAINING] [D loss: 0.2239] [G loss: 2.5033]\n",
      "[Epoch 6/500] [Batch 249/312] [D IS TRAINING] [D loss: 0.2358] [G loss: 1.7553]\n",
      "[Epoch 6/500] [Batch 250/312] [D IS TRAINING] [D loss: 0.1834] [G loss: 1.9989]\n",
      "[Epoch 6/500] [Batch 251/312] [D IS TRAINING] [D loss: 0.1886] [G loss: 2.0480]\n",
      "[Epoch 6/500] [Batch 252/312] [D IS TRAINING] [D loss: 0.2164] [G loss: 2.2710]\n",
      "[Epoch 6/500] [Batch 253/312] [D IS TRAINING] [D loss: 0.2974] [G loss: 1.5225]\n",
      "[Epoch 6/500] [Batch 254/312] [D IS TRAINING] [D loss: 0.2072] [G loss: 1.9696]\n",
      "[Epoch 6/500] [Batch 255/312] [D IS TRAINING] [D loss: 0.2756] [G loss: 2.0156]\n",
      "[Epoch 6/500] [Batch 256/312] [D IS TRAINING] [D loss: 0.2094] [G loss: 1.7093]\n",
      "[Epoch 6/500] [Batch 257/312] [D IS TRAINING] [D loss: 0.2387] [G loss: 2.4654]\n",
      "[Epoch 6/500] [Batch 258/312] [D IS TRAINING] [D loss: 0.2777] [G loss: 1.3349]\n",
      "[Epoch 6/500] [Batch 259/312] [D IS TRAINING] [D loss: 0.2709] [G loss: 3.2584]\n",
      "[Epoch 6/500] [Batch 260/312] [D IS TRAINING] [D loss: 0.2386] [G loss: 1.4060]\n",
      "[Epoch 6/500] [Batch 261/312] [D IS TRAINING] [D loss: 0.1520] [G loss: 2.4532]\n",
      "[Epoch 6/500] [Batch 262/312] [D IS TRAINING] [D loss: 0.2182] [G loss: 2.4637]\n",
      "[Epoch 6/500] [Batch 263/312] [D IS TRAINING] [D loss: 0.2977] [G loss: 1.1675]\n",
      "[Epoch 6/500] [Batch 264/312] [D IS TRAINING] [D loss: 0.3225] [G loss: 2.3478]\n",
      "[Epoch 6/500] [Batch 265/312] [D IS TRAINING] [D loss: 0.2598] [G loss: 1.5209]\n",
      "[Epoch 6/500] [Batch 266/312] [D IS TRAINING] [D loss: 0.2636] [G loss: 1.7691]\n",
      "[Epoch 6/500] [Batch 267/312] [D IS TRAINING] [D loss: 0.2685] [G loss: 2.0846]\n",
      "[Epoch 6/500] [Batch 268/312] [D IS TRAINING] [D loss: 0.1857] [G loss: 2.0676]\n",
      "[Epoch 6/500] [Batch 269/312] [D IS TRAINING] [D loss: 0.2624] [G loss: 1.4223]\n",
      "[Epoch 6/500] [Batch 270/312] [D IS TRAINING] [D loss: 0.2533] [G loss: 2.7517]\n",
      "[Epoch 6/500] [Batch 271/312] [D IS TRAINING] [D loss: 0.2700] [G loss: 1.4632]\n",
      "[Epoch 6/500] [Batch 272/312] [D IS TRAINING] [D loss: 0.2525] [G loss: 1.9354]\n",
      "[Epoch 6/500] [Batch 273/312] [D IS TRAINING] [D loss: 0.2007] [G loss: 2.2378]\n",
      "[Epoch 6/500] [Batch 274/312] [D IS TRAINING] [D loss: 0.2231] [G loss: 1.8174]\n",
      "[Epoch 6/500] [Batch 275/312] [D IS TRAINING] [D loss: 0.2635] [G loss: 1.7322]\n",
      "[Epoch 6/500] [Batch 276/312] [D IS TRAINING] [D loss: 0.2195] [G loss: 2.1312]\n",
      "[Epoch 6/500] [Batch 277/312] [D IS TRAINING] [D loss: 0.2458] [G loss: 1.5932]\n",
      "[Epoch 6/500] [Batch 278/312] [D IS TRAINING] [D loss: 0.3164] [G loss: 2.3601]\n",
      "[Epoch 6/500] [Batch 279/312] [D IS TRAINING] [D loss: 0.4228] [G loss: 0.8443]\n",
      "[Epoch 6/500] [Batch 280/312] [D IS TRAINING] [D loss: 0.5971] [G loss: 4.0924]\n",
      "[Epoch 6/500] [Batch 281/312] [D IS TRAINING] [D loss: 0.6257] [G loss: 0.6380]\n",
      "[Epoch 6/500] [Batch 282/312] [D IS TRAINING] [D loss: 0.5296] [G loss: 4.2261]\n",
      "[Epoch 6/500] [Batch 283/312] [D IS TRAINING] [D loss: 0.2533] [G loss: 1.9173]\n",
      "[Epoch 6/500] [Batch 284/312] [D IS TRAINING] [D loss: 0.2933] [G loss: 1.3143]\n",
      "[Epoch 6/500] [Batch 285/312] [D IS TRAINING] [D loss: 0.2734] [G loss: 2.7447]\n",
      "[Epoch 6/500] [Batch 286/312] [D IS TRAINING] [D loss: 0.2667] [G loss: 1.4826]\n",
      "[Epoch 6/500] [Batch 287/312] [D IS TRAINING] [D loss: 0.2651] [G loss: 2.3067]\n",
      "[Epoch 6/500] [Batch 288/312] [D IS TRAINING] [D loss: 0.3027] [G loss: 1.3093]\n",
      "[Epoch 6/500] [Batch 289/312] [D IS TRAINING] [D loss: 0.3927] [G loss: 2.2898]\n",
      "[Epoch 6/500] [Batch 290/312] [D IS TRAINING] [D loss: 0.4698] [G loss: 0.7495]\n",
      "[Epoch 6/500] [Batch 291/312] [D IS TRAINING] [D loss: 0.4611] [G loss: 3.4693]\n",
      "[Epoch 6/500] [Batch 292/312] [D IS TRAINING] [D loss: 0.3558] [G loss: 1.1386]\n",
      "[Epoch 6/500] [Batch 293/312] [D IS TRAINING] [D loss: 0.3382] [G loss: 1.7588]\n",
      "[Epoch 6/500] [Batch 294/312] [D IS TRAINING] [D loss: 0.2926] [G loss: 1.9456]\n",
      "[Epoch 6/500] [Batch 295/312] [D IS TRAINING] [D loss: 0.2581] [G loss: 2.1336]\n",
      "[Epoch 6/500] [Batch 296/312] [D IS TRAINING] [D loss: 0.2144] [G loss: 1.6121]\n",
      "[Epoch 6/500] [Batch 297/312] [D IS TRAINING] [D loss: 0.1845] [G loss: 2.0424]\n",
      "[Epoch 6/500] [Batch 298/312] [D IS TRAINING] [D loss: 0.1974] [G loss: 1.9646]\n",
      "[Epoch 6/500] [Batch 299/312] [D IS TRAINING] [D loss: 0.1801] [G loss: 2.3120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/500] [Batch 300/312] [D IS TRAINING] [D loss: 0.2684] [G loss: 1.8235]\n",
      "[Epoch 6/500] [Batch 301/312] [D IS TRAINING] [D loss: 0.2627] [G loss: 1.3208]\n",
      "[Epoch 6/500] [Batch 302/312] [D IS TRAINING] [D loss: 0.3707] [G loss: 2.6587]\n",
      "[Epoch 6/500] [Batch 303/312] [D IS TRAINING] [D loss: 0.5016] [G loss: 0.7327]\n",
      "[Epoch 6/500] [Batch 304/312] [D IS TRAINING] [D loss: 0.5275] [G loss: 3.9938]\n",
      "[Epoch 6/500] [Batch 305/312] [D IS TRAINING] [D loss: 0.3260] [G loss: 1.2988]\n",
      "[Epoch 6/500] [Batch 306/312] [D IS TRAINING] [D loss: 0.2212] [G loss: 1.8028]\n",
      "[Epoch 6/500] [Batch 307/312] [D IS TRAINING] [D loss: 0.3253] [G loss: 2.3541]\n",
      "[Epoch 6/500] [Batch 308/312] [D IS TRAINING] [D loss: 0.3548] [G loss: 1.2316]\n",
      "[Epoch 6/500] [Batch 309/312] [D IS TRAINING] [D loss: 0.3445] [G loss: 1.8394]\n",
      "[Epoch 6/500] [Batch 310/312] [D IS TRAINING] [D loss: 0.2888] [G loss: 1.5986]\n",
      "[Epoch 6/500] [Batch 311/312] [D IS TRAINING] [D loss: 0.2524] [G loss: 1.6615]\n",
      "[Epoch 7/500] [Batch 0/312] [D IS TRAINING] [D loss: 0.1954] [G loss: 2.1566]\n",
      "[Epoch 7/500] [Batch 1/312] [D IS TRAINING] [D loss: 0.2404] [G loss: 1.9782]\n",
      "[Epoch 7/500] [Batch 2/312] [D IS TRAINING] [D loss: 0.2372] [G loss: 1.4334]\n",
      "[Epoch 7/500] [Batch 3/312] [D IS TRAINING] [D loss: 0.2633] [G loss: 2.6710]\n",
      "[Epoch 7/500] [Batch 4/312] [D IS TRAINING] [D loss: 0.2373] [G loss: 1.3986]\n",
      "[Epoch 7/500] [Batch 5/312] [D IS TRAINING] [D loss: 0.2453] [G loss: 1.9674]\n",
      "[Epoch 7/500] [Batch 6/312] [D IS TRAINING] [D loss: 0.2601] [G loss: 1.8910]\n",
      "[Epoch 7/500] [Batch 7/312] [D IS TRAINING] [D loss: 0.3195] [G loss: 1.1598]\n",
      "[Epoch 7/500] [Batch 8/312] [D IS TRAINING] [D loss: 0.4236] [G loss: 3.0514]\n",
      "[Epoch 7/500] [Batch 9/312] [D IS TRAINING] [D loss: 0.3636] [G loss: 1.0962]\n",
      "[Epoch 7/500] [Batch 10/312] [D IS TRAINING] [D loss: 0.2263] [G loss: 2.7567]\n",
      "[Epoch 7/500] [Batch 11/312] [D IS TRAINING] [D loss: 0.1758] [G loss: 2.3191]\n",
      "[Epoch 7/500] [Batch 12/312] [D IS TRAINING] [D loss: 0.1942] [G loss: 1.6586]\n",
      "[Epoch 7/500] [Batch 13/312] [D IS TRAINING] [D loss: 0.1894] [G loss: 2.3467]\n",
      "[Epoch 7/500] [Batch 14/312] [D IS TRAINING] [D loss: 0.1744] [G loss: 2.3166]\n",
      "[Epoch 7/500] [Batch 15/312] [D IS TRAINING] [D loss: 0.2644] [G loss: 1.3772]\n",
      "[Epoch 7/500] [Batch 16/312] [D IS TRAINING] [D loss: 0.3122] [G loss: 2.8406]\n",
      "[Epoch 7/500] [Batch 17/312] [D IS TRAINING] [D loss: 0.3499] [G loss: 1.0389]\n",
      "[Epoch 7/500] [Batch 18/312] [D IS TRAINING] [D loss: 0.3601] [G loss: 2.4218]\n",
      "[Epoch 7/500] [Batch 19/312] [D IS TRAINING] [D loss: 0.2688] [G loss: 1.5743]\n",
      "[Epoch 7/500] [Batch 20/312] [D IS TRAINING] [D loss: 0.2441] [G loss: 1.7890]\n",
      "[Epoch 7/500] [Batch 21/312] [D IS TRAINING] [D loss: 0.2265] [G loss: 2.4234]\n",
      "[Epoch 7/500] [Batch 22/312] [D IS TRAINING] [D loss: 0.2213] [G loss: 1.6739]\n",
      "[Epoch 7/500] [Batch 23/312] [D IS TRAINING] [D loss: 0.2212] [G loss: 1.9116]\n",
      "[Epoch 7/500] [Batch 24/312] [D IS TRAINING] [D loss: 0.2004] [G loss: 2.3101]\n",
      "[Epoch 7/500] [Batch 25/312] [D IS TRAINING] [D loss: 0.2373] [G loss: 1.5871]\n",
      "[Epoch 7/500] [Batch 26/312] [D IS TRAINING] [D loss: 0.2779] [G loss: 2.5155]\n",
      "[Epoch 7/500] [Batch 27/312] [D IS TRAINING] [D loss: 0.2614] [G loss: 1.3037]\n",
      "[Epoch 7/500] [Batch 28/312] [D IS TRAINING] [D loss: 0.4054] [G loss: 3.6922]\n",
      "[Epoch 7/500] [Batch 29/312] [D IS TRAINING] [D loss: 0.4326] [G loss: 0.7924]\n",
      "[Epoch 7/500] [Batch 30/312] [D IS TRAINING] [D loss: 0.3043] [G loss: 3.5262]\n",
      "[Epoch 7/500] [Batch 31/312] [D IS TRAINING] [D loss: 0.1913] [G loss: 2.2791]\n",
      "[Epoch 7/500] [Batch 32/312] [D IS TRAINING] [D loss: 0.2080] [G loss: 1.6307]\n",
      "[Epoch 7/500] [Batch 33/312] [D IS TRAINING] [D loss: 0.2159] [G loss: 2.5915]\n",
      "[Epoch 7/500] [Batch 34/312] [D IS TRAINING] [D loss: 0.1874] [G loss: 2.2178]\n",
      "[Epoch 7/500] [Batch 35/312] [D IS TRAINING] [D loss: 0.2449] [G loss: 1.4557]\n",
      "[Epoch 7/500] [Batch 36/312] [D IS TRAINING] [D loss: 0.3046] [G loss: 2.8969]\n",
      "[Epoch 7/500] [Batch 37/312] [D IS TRAINING] [D loss: 0.2569] [G loss: 1.4445]\n",
      "[Epoch 7/500] [Batch 38/312] [D IS TRAINING] [D loss: 0.2195] [G loss: 2.8774]\n",
      "[Epoch 7/500] [Batch 39/312] [D IS TRAINING] [D loss: 0.1820] [G loss: 2.1527]\n",
      "[Epoch 7/500] [Batch 40/312] [D IS TRAINING] [D loss: 0.2083] [G loss: 1.7230]\n",
      "[Epoch 7/500] [Batch 41/312] [D IS TRAINING] [D loss: 0.1941] [G loss: 2.5814]\n",
      "[Epoch 7/500] [Batch 42/312] [D IS TRAINING] [D loss: 0.1685] [G loss: 1.9187]\n",
      "[Epoch 7/500] [Batch 43/312] [D IS TRAINING] [D loss: 0.1524] [G loss: 2.5257]\n",
      "[Epoch 7/500] [Batch 44/312] [D IS TRAINING] [D loss: 0.1807] [G loss: 1.8967]\n",
      "[Epoch 7/500] [Batch 45/312] [D IS TRAINING] [D loss: 0.2601] [G loss: 2.5636]\n",
      "[Epoch 7/500] [Batch 46/312] [D IS TRAINING] [D loss: 0.3245] [G loss: 1.0263]\n",
      "[Epoch 7/500] [Batch 47/312] [D IS TRAINING] [D loss: 0.5356] [G loss: 4.4500]\n",
      "[Epoch 7/500] [Batch 48/312] [D IS TRAINING] [D loss: 0.3718] [G loss: 0.8801]\n",
      "[Epoch 7/500] [Batch 49/312] [D IS TRAINING] [D loss: 0.1723] [G loss: 2.6433]\n",
      "[Epoch 7/500] [Batch 50/312] [D IS TRAINING] [D loss: 0.2145] [G loss: 2.8060]\n",
      "[Epoch 7/500] [Batch 51/312] [D IS TRAINING] [D loss: 0.1830] [G loss: 1.7710]\n",
      "[Epoch 7/500] [Batch 52/312] [D IS TRAINING] [D loss: 0.1793] [G loss: 2.1936]\n",
      "[Epoch 7/500] [Batch 53/312] [D IS TRAINING] [D loss: 0.2194] [G loss: 2.4795]\n",
      "[Epoch 7/500] [Batch 54/312] [D IS TRAINING] [D loss: 0.1774] [G loss: 1.7846]\n",
      "[Epoch 7/500] [Batch 55/312] [D IS TRAINING] [D loss: 0.1300] [G loss: 2.4568]\n",
      "[Epoch 7/500] [Batch 56/312] [D IS TRAINING] [D loss: 0.1320] [G loss: 2.6274]\n",
      "[Epoch 7/500] [Batch 57/312] [D IS TRAINING] [D loss: 0.1301] [G loss: 2.7368]\n",
      "[Epoch 7/500] [Batch 58/312] [D IS TRAINING] [D loss: 0.1690] [G loss: 1.7747]\n",
      "[Epoch 7/500] [Batch 59/312] [D IS TRAINING] [D loss: 0.1802] [G loss: 2.1179]\n",
      "[Epoch 7/500] [Batch 60/312] [D IS TRAINING] [D loss: 0.1921] [G loss: 3.2119]\n",
      "[Epoch 7/500] [Batch 61/312] [D IS TRAINING] [D loss: 0.0859] [G loss: 3.2507]\n",
      "[Epoch 7/500] [Batch 62/312] [D IS TRAINING] [D loss: 0.4564] [G loss: 0.7064]\n",
      "[Epoch 7/500] [Batch 63/312] [D IS TRAINING] [D loss: 1.0630] [G loss: 6.4269]\n",
      "[Epoch 7/500] [Batch 64/312] [D IS TRAINING] [D loss: 0.2633] [G loss: 2.5035]\n",
      "[Epoch 7/500] [Batch 65/312] [D IS TRAINING] [D loss: 0.4702] [G loss: 0.9735]\n",
      "[Epoch 7/500] [Batch 66/312] [D IS TRAINING] [D loss: 0.4538] [G loss: 3.9639]\n",
      "[Epoch 7/500] [Batch 67/312] [D IS TRAINING] [D loss: 0.2068] [G loss: 1.9767]\n",
      "[Epoch 7/500] [Batch 68/312] [D IS TRAINING] [D loss: 0.2483] [G loss: 1.3762]\n",
      "[Epoch 7/500] [Batch 69/312] [D IS TRAINING] [D loss: 0.2597] [G loss: 2.4946]\n",
      "[Epoch 7/500] [Batch 70/312] [D IS TRAINING] [D loss: 0.2332] [G loss: 1.7797]\n",
      "[Epoch 7/500] [Batch 71/312] [D IS TRAINING] [D loss: 0.2390] [G loss: 1.8001]\n",
      "[Epoch 7/500] [Batch 72/312] [D IS TRAINING] [D loss: 0.2001] [G loss: 2.2211]\n",
      "[Epoch 7/500] [Batch 73/312] [D IS TRAINING] [D loss: 0.2254] [G loss: 1.7304]\n",
      "[Epoch 7/500] [Batch 74/312] [D IS TRAINING] [D loss: 0.2043] [G loss: 2.3553]\n",
      "[Epoch 7/500] [Batch 75/312] [D IS TRAINING] [D loss: 0.1784] [G loss: 1.8773]\n",
      "[Epoch 7/500] [Batch 76/312] [D IS TRAINING] [D loss: 0.1690] [G loss: 2.4354]\n",
      "[Epoch 7/500] [Batch 77/312] [D IS TRAINING] [D loss: 0.1353] [G loss: 2.3858]\n",
      "[Epoch 7/500] [Batch 78/312] [D IS TRAINING] [D loss: 0.1490] [G loss: 2.2550]\n",
      "[Epoch 7/500] [Batch 79/312] [D IS TRAINING] [D loss: 0.1542] [G loss: 2.4046]\n",
      "[Epoch 7/500] [Batch 80/312] [D IS TRAINING] [D loss: 0.1413] [G loss: 2.0202]\n",
      "[Epoch 7/500] [Batch 81/312] [D IS TRAINING] [D loss: 0.1704] [G loss: 2.1033]\n",
      "[Epoch 7/500] [Batch 82/312] [D IS TRAINING] [D loss: 0.2093] [G loss: 1.9594]\n",
      "[Epoch 7/500] [Batch 83/312] [D IS TRAINING] [D loss: 0.2774] [G loss: 2.3069]\n",
      "[Epoch 7/500] [Batch 84/312] [D IS TRAINING] [D loss: 0.3792] [G loss: 0.9583]\n",
      "[Epoch 7/500] [Batch 85/312] [D IS TRAINING] [D loss: 0.5500] [G loss: 4.4995]\n",
      "[Epoch 7/500] [Batch 86/312] [D IS TRAINING] [D loss: 0.3456] [G loss: 1.4119]\n",
      "[Epoch 7/500] [Batch 87/312] [D IS TRAINING] [D loss: 0.3157] [G loss: 2.5643]\n",
      "[Epoch 7/500] [Batch 88/312] [D IS TRAINING] [D loss: 0.2809] [G loss: 2.9280]\n",
      "[Epoch 7/500] [Batch 89/312] [D IS TRAINING] [D loss: 0.4409] [G loss: 0.9567]\n",
      "[Epoch 7/500] [Batch 90/312] [D IS TRAINING] [D loss: 0.4448] [G loss: 3.7141]\n",
      "[Epoch 7/500] [Batch 91/312] [D IS TRAINING] [D loss: 0.3551] [G loss: 1.3325]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/500] [Batch 92/312] [D IS TRAINING] [D loss: 0.3493] [G loss: 3.2905]\n",
      "[Epoch 7/500] [Batch 93/312] [D IS TRAINING] [D loss: 0.2089] [G loss: 1.7211]\n",
      "[Epoch 7/500] [Batch 94/312] [D IS TRAINING] [D loss: 0.1943] [G loss: 2.1823]\n",
      "[Epoch 7/500] [Batch 95/312] [D IS TRAINING] [D loss: 0.2084] [G loss: 2.3934]\n",
      "[Epoch 7/500] [Batch 96/312] [D IS TRAINING] [D loss: 0.2491] [G loss: 1.9191]\n",
      "[Epoch 7/500] [Batch 97/312] [D IS TRAINING] [D loss: 0.2569] [G loss: 1.4756]\n",
      "[Epoch 7/500] [Batch 98/312] [D IS TRAINING] [D loss: 0.2716] [G loss: 2.7732]\n",
      "[Epoch 7/500] [Batch 99/312] [D IS TRAINING] [D loss: 0.2215] [G loss: 1.5735]\n",
      "[Epoch 7/500] [Batch 100/312] [D IS TRAINING] [D loss: 0.2263] [G loss: 2.0353]\n",
      "[Epoch 7/500] [Batch 101/312] [D IS TRAINING] [D loss: 0.2117] [G loss: 2.0273]\n",
      "[Epoch 7/500] [Batch 102/312] [D IS TRAINING] [D loss: 0.2177] [G loss: 1.8539]\n",
      "[Epoch 7/500] [Batch 103/312] [D IS TRAINING] [D loss: 0.2214] [G loss: 1.8460]\n",
      "[Epoch 7/500] [Batch 104/312] [D IS TRAINING] [D loss: 0.2128] [G loss: 2.2102]\n",
      "[Epoch 7/500] [Batch 105/312] [D IS TRAINING] [D loss: 0.2663] [G loss: 1.6041]\n",
      "[Epoch 7/500] [Batch 106/312] [D IS TRAINING] [D loss: 0.2246] [G loss: 2.1265]\n",
      "[Epoch 7/500] [Batch 107/312] [D IS TRAINING] [D loss: 0.2241] [G loss: 2.0287]\n",
      "[Epoch 7/500] [Batch 108/312] [D IS TRAINING] [D loss: 0.2139] [G loss: 1.6876]\n",
      "[Epoch 7/500] [Batch 109/312] [D IS TRAINING] [D loss: 0.2093] [G loss: 2.5361]\n",
      "[Epoch 7/500] [Batch 110/312] [D IS TRAINING] [D loss: 0.3149] [G loss: 1.2145]\n",
      "[Epoch 7/500] [Batch 111/312] [D IS TRAINING] [D loss: 0.4211] [G loss: 3.7647]\n",
      "[Epoch 7/500] [Batch 112/312] [D IS TRAINING] [D loss: 0.3744] [G loss: 1.0553]\n",
      "[Epoch 7/500] [Batch 113/312] [D IS TRAINING] [D loss: 0.3040] [G loss: 2.6369]\n",
      "[Epoch 7/500] [Batch 114/312] [D IS TRAINING] [D loss: 0.2113] [G loss: 1.8483]\n",
      "[Epoch 7/500] [Batch 115/312] [D IS TRAINING] [D loss: 0.1884] [G loss: 2.2261]\n",
      "[Epoch 7/500] [Batch 116/312] [D IS TRAINING] [D loss: 0.1852] [G loss: 2.1211]\n",
      "[Epoch 7/500] [Batch 117/312] [D IS TRAINING] [D loss: 0.1841] [G loss: 2.2990]\n",
      "[Epoch 7/500] [Batch 118/312] [D IS TRAINING] [D loss: 0.2001] [G loss: 1.9161]\n",
      "[Epoch 7/500] [Batch 119/312] [D IS TRAINING] [D loss: 0.2098] [G loss: 2.5652]\n",
      "[Epoch 7/500] [Batch 120/312] [D IS TRAINING] [D loss: 0.2659] [G loss: 1.3918]\n",
      "[Epoch 7/500] [Batch 121/312] [D IS TRAINING] [D loss: 0.2442] [G loss: 2.3952]\n",
      "[Epoch 7/500] [Batch 122/312] [D IS TRAINING] [D loss: 0.2541] [G loss: 1.5921]\n",
      "[Epoch 7/500] [Batch 123/312] [D IS TRAINING] [D loss: 0.2312] [G loss: 2.7228]\n",
      "[Epoch 7/500] [Batch 124/312] [D IS TRAINING] [D loss: 0.2195] [G loss: 1.3739]\n",
      "[Epoch 7/500] [Batch 125/312] [D IS TRAINING] [D loss: 0.2881] [G loss: 3.3222]\n",
      "[Epoch 7/500] [Batch 126/312] [D IS TRAINING] [D loss: 0.2916] [G loss: 1.0948]\n",
      "[Epoch 7/500] [Batch 127/312] [D IS TRAINING] [D loss: 0.2611] [G loss: 3.3040]\n",
      "[Epoch 7/500] [Batch 128/312] [D IS TRAINING] [D loss: 0.2552] [G loss: 1.4709]\n",
      "[Epoch 7/500] [Batch 129/312] [D IS TRAINING] [D loss: 0.3675] [G loss: 2.6443]\n",
      "[Epoch 7/500] [Batch 130/312] [D IS TRAINING] [D loss: 0.3778] [G loss: 1.1183]\n",
      "[Epoch 7/500] [Batch 131/312] [D IS TRAINING] [D loss: 0.4435] [G loss: 3.4540]\n",
      "[Epoch 7/500] [Batch 132/312] [D IS TRAINING] [D loss: 0.3374] [G loss: 1.1232]\n",
      "[Epoch 7/500] [Batch 133/312] [D IS TRAINING] [D loss: 0.1939] [G loss: 3.4404]\n",
      "[Epoch 7/500] [Batch 134/312] [D IS TRAINING] [D loss: 0.1504] [G loss: 3.1563]\n",
      "[Epoch 7/500] [Batch 135/312] [D IS TRAINING] [D loss: 0.3055] [G loss: 1.2445]\n",
      "[Epoch 7/500] [Batch 136/312] [D IS TRAINING] [D loss: 0.4146] [G loss: 4.1914]\n",
      "[Epoch 7/500] [Batch 137/312] [D IS TRAINING] [D loss: 0.2635] [G loss: 1.5479]\n",
      "[Epoch 7/500] [Batch 138/312] [D IS TRAINING] [D loss: 0.2143] [G loss: 1.8831]\n",
      "[Epoch 7/500] [Batch 139/312] [D IS TRAINING] [D loss: 0.3347] [G loss: 3.4535]\n",
      "[Epoch 7/500] [Batch 140/312] [D IS TRAINING] [D loss: 0.4244] [G loss: 0.8891]\n",
      "[Epoch 7/500] [Batch 141/312] [D IS TRAINING] [D loss: 0.2969] [G loss: 4.3833]\n",
      "[Epoch 7/500] [Batch 142/312] [D IS TRAINING] [D loss: 0.2472] [G loss: 1.5406]\n",
      "[Epoch 7/500] [Batch 143/312] [D IS TRAINING] [D loss: 0.1465] [G loss: 3.4194]\n",
      "[Epoch 7/500] [Batch 144/312] [D IS TRAINING] [D loss: 0.1883] [G loss: 1.8448]\n",
      "[Epoch 7/500] [Batch 145/312] [D IS TRAINING] [D loss: 0.2175] [G loss: 2.4299]\n",
      "[Epoch 7/500] [Batch 146/312] [D IS TRAINING] [D loss: 0.2668] [G loss: 1.6517]\n",
      "[Epoch 7/500] [Batch 147/312] [D IS TRAINING] [D loss: 0.2277] [G loss: 1.8968]\n",
      "[Epoch 7/500] [Batch 148/312] [D IS TRAINING] [D loss: 0.3004] [G loss: 2.9720]\n",
      "[Epoch 7/500] [Batch 149/312] [D IS TRAINING] [D loss: 0.3580] [G loss: 1.2709]\n",
      "[Epoch 7/500] [Batch 150/312] [D IS TRAINING] [D loss: 0.3448] [G loss: 3.1936]\n",
      "[Epoch 7/500] [Batch 151/312] [D IS TRAINING] [D loss: 0.2886] [G loss: 1.2221]\n",
      "[Epoch 7/500] [Batch 152/312] [D IS TRAINING] [D loss: 0.1749] [G loss: 2.7041]\n",
      "[Epoch 7/500] [Batch 153/312] [D IS TRAINING] [D loss: 0.2076] [G loss: 2.4990]\n",
      "[Epoch 7/500] [Batch 154/312] [D IS TRAINING] [D loss: 0.2677] [G loss: 1.2228]\n",
      "[Epoch 7/500] [Batch 155/312] [D IS TRAINING] [D loss: 0.2143] [G loss: 2.8085]\n",
      "[Epoch 7/500] [Batch 156/312] [D IS TRAINING] [D loss: 0.1755] [G loss: 2.0425]\n",
      "[Epoch 7/500] [Batch 157/312] [D IS TRAINING] [D loss: 0.1971] [G loss: 2.0015]\n",
      "[Epoch 7/500] [Batch 158/312] [D IS TRAINING] [D loss: 0.2249] [G loss: 2.3225]\n",
      "[Epoch 7/500] [Batch 159/312] [D IS TRAINING] [D loss: 0.2527] [G loss: 1.4310]\n",
      "[Epoch 7/500] [Batch 160/312] [D IS TRAINING] [D loss: 0.3691] [G loss: 2.8562]\n",
      "[Epoch 7/500] [Batch 161/312] [D IS TRAINING] [D loss: 0.3670] [G loss: 0.9112]\n",
      "[Epoch 7/500] [Batch 162/312] [D IS TRAINING] [D loss: 0.2905] [G loss: 3.5626]\n",
      "[Epoch 7/500] [Batch 163/312] [D IS TRAINING] [D loss: 0.2175] [G loss: 1.9385]\n",
      "[Epoch 7/500] [Batch 164/312] [D IS TRAINING] [D loss: 0.2178] [G loss: 1.5074]\n",
      "[Epoch 7/500] [Batch 165/312] [D IS TRAINING] [D loss: 0.2543] [G loss: 3.4796]\n",
      "[Epoch 7/500] [Batch 166/312] [D IS TRAINING] [D loss: 0.1697] [G loss: 1.9029]\n",
      "[Epoch 7/500] [Batch 167/312] [D IS TRAINING] [D loss: 0.2331] [G loss: 2.0451]\n",
      "[Epoch 7/500] [Batch 168/312] [D IS TRAINING] [D loss: 0.1683] [G loss: 2.0372]\n",
      "[Epoch 7/500] [Batch 169/312] [D IS TRAINING] [D loss: 0.1661] [G loss: 2.2746]\n",
      "[Epoch 7/500] [Batch 170/312] [D IS TRAINING] [D loss: 0.1610] [G loss: 2.3621]\n",
      "[Epoch 7/500] [Batch 171/312] [D IS TRAINING] [D loss: 0.1796] [G loss: 2.4378]\n",
      "[Epoch 7/500] [Batch 172/312] [D IS TRAINING] [D loss: 0.2560] [G loss: 1.3805]\n",
      "[Epoch 7/500] [Batch 173/312] [D IS TRAINING] [D loss: 0.2429] [G loss: 3.6541]\n",
      "[Epoch 7/500] [Batch 174/312] [D IS TRAINING] [D loss: 0.2041] [G loss: 1.7777]\n",
      "[Epoch 7/500] [Batch 175/312] [D IS TRAINING] [D loss: 0.2226] [G loss: 2.0800]\n",
      "[Epoch 7/500] [Batch 176/312] [D IS TRAINING] [D loss: 0.2660] [G loss: 2.4219]\n",
      "[Epoch 7/500] [Batch 177/312] [D IS TRAINING] [D loss: 0.3613] [G loss: 0.9918]\n",
      "[Epoch 7/500] [Batch 178/312] [D IS TRAINING] [D loss: 0.5464] [G loss: 4.0907]\n",
      "[Epoch 7/500] [Batch 179/312] [D IS TRAINING] [D loss: 0.5531] [G loss: 0.7088]\n",
      "[Epoch 7/500] [Batch 180/312] [D IS TRAINING] [D loss: 0.6114] [G loss: 4.7286]\n",
      "[Epoch 7/500] [Batch 181/312] [D IS TRAINING] [D loss: 0.4041] [G loss: 0.9011]\n",
      "[Epoch 7/500] [Batch 182/312] [D IS TRAINING] [D loss: 0.2651] [G loss: 3.1666]\n",
      "[Epoch 7/500] [Batch 183/312] [D IS TRAINING] [D loss: 0.1859] [G loss: 2.8161]\n",
      "[Epoch 7/500] [Batch 184/312] [D IS TRAINING] [D loss: 0.3288] [G loss: 1.0671]\n",
      "[Epoch 7/500] [Batch 185/312] [D IS TRAINING] [D loss: 0.3032] [G loss: 2.7770]\n",
      "[Epoch 7/500] [Batch 186/312] [D IS TRAINING] [D loss: 0.2467] [G loss: 2.1072]\n",
      "[Epoch 7/500] [Batch 187/312] [D IS TRAINING] [D loss: 0.4036] [G loss: 0.9343]\n",
      "[Epoch 7/500] [Batch 188/312] [D IS TRAINING] [D loss: 0.5558] [G loss: 3.8826]\n",
      "[Epoch 7/500] [Batch 189/312] [D IS TRAINING] [D loss: 0.3121] [G loss: 1.5717]\n",
      "[Epoch 7/500] [Batch 190/312] [D IS TRAINING] [D loss: 0.2334] [G loss: 1.5924]\n",
      "[Epoch 7/500] [Batch 191/312] [D IS TRAINING] [D loss: 0.2581] [G loss: 3.1464]\n",
      "[Epoch 7/500] [Batch 192/312] [D IS TRAINING] [D loss: 0.1904] [G loss: 1.8780]\n",
      "[Epoch 7/500] [Batch 193/312] [D IS TRAINING] [D loss: 0.2181] [G loss: 1.6823]\n",
      "[Epoch 7/500] [Batch 194/312] [D IS TRAINING] [D loss: 0.2162] [G loss: 2.5045]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/500] [Batch 195/312] [D IS TRAINING] [D loss: 0.1652] [G loss: 1.9982]\n",
      "[Epoch 7/500] [Batch 196/312] [D IS TRAINING] [D loss: 0.1729] [G loss: 2.5212]\n",
      "[Epoch 7/500] [Batch 197/312] [D IS TRAINING] [D loss: 0.1899] [G loss: 1.9299]\n",
      "[Epoch 7/500] [Batch 198/312] [D IS TRAINING] [D loss: 0.1907] [G loss: 2.2392]\n",
      "[Epoch 7/500] [Batch 199/312] [D IS TRAINING] [D loss: 0.2324] [G loss: 1.7124]\n",
      "[Epoch 7/500] [Batch 200/312] [D IS TRAINING] [D loss: 0.2270] [G loss: 2.6313]\n",
      "[Epoch 7/500] [Batch 201/312] [D IS TRAINING] [D loss: 0.2419] [G loss: 1.2830]\n",
      "[Epoch 7/500] [Batch 202/312] [D IS TRAINING] [D loss: 0.3004] [G loss: 2.9597]\n",
      "[Epoch 7/500] [Batch 203/312] [D IS TRAINING] [D loss: 0.2628] [G loss: 1.3218]\n",
      "[Epoch 7/500] [Batch 204/312] [D IS TRAINING] [D loss: 0.2283] [G loss: 2.6961]\n",
      "[Epoch 7/500] [Batch 205/312] [D IS TRAINING] [D loss: 0.2261] [G loss: 1.4845]\n",
      "[Epoch 7/500] [Batch 206/312] [D IS TRAINING] [D loss: 0.2148] [G loss: 2.7928]\n",
      "[Epoch 7/500] [Batch 207/312] [D IS TRAINING] [D loss: 0.1693] [G loss: 2.0257]\n",
      "[Epoch 7/500] [Batch 208/312] [D IS TRAINING] [D loss: 0.1382] [G loss: 2.4420]\n",
      "[Epoch 7/500] [Batch 209/312] [D IS TRAINING] [D loss: 0.1462] [G loss: 2.1556]\n",
      "[Epoch 7/500] [Batch 210/312] [D IS TRAINING] [D loss: 0.1375] [G loss: 2.3528]\n",
      "[Epoch 7/500] [Batch 211/312] [D IS TRAINING] [D loss: 0.1315] [G loss: 2.6821]\n",
      "[Epoch 7/500] [Batch 212/312] [D IS TRAINING] [D loss: 0.1564] [G loss: 1.9122]\n",
      "[Epoch 7/500] [Batch 213/312] [D IS TRAINING] [D loss: 0.2371] [G loss: 2.1041]\n",
      "[Epoch 7/500] [Batch 214/312] [D IS TRAINING] [D loss: 0.2607] [G loss: 1.6684]\n",
      "[Epoch 7/500] [Batch 215/312] [D IS TRAINING] [D loss: 0.2371] [G loss: 2.6964]\n",
      "[Epoch 7/500] [Batch 216/312] [D IS TRAINING] [D loss: 0.1847] [G loss: 1.8224]\n",
      "[Epoch 7/500] [Batch 217/312] [D IS TRAINING] [D loss: 0.2374] [G loss: 2.6610]\n",
      "[Epoch 7/500] [Batch 218/312] [D IS TRAINING] [D loss: 0.2366] [G loss: 1.6074]\n",
      "[Epoch 7/500] [Batch 219/312] [D IS TRAINING] [D loss: 0.2541] [G loss: 2.8692]\n",
      "[Epoch 7/500] [Batch 220/312] [D IS TRAINING] [D loss: 0.1906] [G loss: 1.5433]\n",
      "[Epoch 7/500] [Batch 221/312] [D IS TRAINING] [D loss: 0.1431] [G loss: 2.7377]\n",
      "[Epoch 7/500] [Batch 222/312] [D IS TRAINING] [D loss: 0.1294] [G loss: 2.6636]\n",
      "[Epoch 7/500] [Batch 223/312] [D IS TRAINING] [D loss: 0.1903] [G loss: 1.6727]\n",
      "[Epoch 7/500] [Batch 224/312] [D IS TRAINING] [D loss: 0.2039] [G loss: 2.4392]\n",
      "[Epoch 7/500] [Batch 225/312] [D IS TRAINING] [D loss: 0.2133] [G loss: 1.7954]\n",
      "[Epoch 7/500] [Batch 226/312] [D IS TRAINING] [D loss: 0.2318] [G loss: 2.3556]\n",
      "[Epoch 7/500] [Batch 227/312] [D IS TRAINING] [D loss: 0.1730] [G loss: 2.0046]\n",
      "[Epoch 7/500] [Batch 228/312] [D IS TRAINING] [D loss: 0.1754] [G loss: 2.0164]\n",
      "[Epoch 7/500] [Batch 229/312] [D IS TRAINING] [D loss: 0.2677] [G loss: 1.7439]\n",
      "[Epoch 7/500] [Batch 230/312] [D IS TRAINING] [D loss: 0.2529] [G loss: 3.0889]\n",
      "[Epoch 7/500] [Batch 231/312] [D IS TRAINING] [D loss: 0.4774] [G loss: 0.6514]\n",
      "[Epoch 7/500] [Batch 232/312] [D IS TRAINING] [D loss: 0.7910] [G loss: 5.2119]\n",
      "[Epoch 7/500] [Batch 233/312] [D IS TRAINING] [D loss: 0.6158] [G loss: 0.8386]\n",
      "[Epoch 7/500] [Batch 234/312] [D IS TRAINING] [D loss: 0.5335] [G loss: 3.5971]\n",
      "[Epoch 7/500] [Batch 235/312] [D IS TRAINING] [D loss: 0.3121] [G loss: 1.1853]\n",
      "[Epoch 7/500] [Batch 236/312] [D IS TRAINING] [D loss: 0.3913] [G loss: 1.2383]\n",
      "[Epoch 7/500] [Batch 237/312] [D IS TRAINING] [D loss: 0.4768] [G loss: 3.3035]\n",
      "[Epoch 7/500] [Batch 238/312] [D IS TRAINING] [D loss: 0.3208] [G loss: 1.2708]\n",
      "[Epoch 7/500] [Batch 239/312] [D IS TRAINING] [D loss: 0.2726] [G loss: 1.9407]\n",
      "[Epoch 7/500] [Batch 240/312] [D IS TRAINING] [D loss: 0.2093] [G loss: 2.6103]\n",
      "[Epoch 7/500] [Batch 241/312] [D IS TRAINING] [D loss: 0.1992] [G loss: 1.9206]\n",
      "[Epoch 7/500] [Batch 242/312] [D IS TRAINING] [D loss: 0.2101] [G loss: 1.9326]\n",
      "[Epoch 7/500] [Batch 243/312] [D IS TRAINING] [D loss: 0.2015] [G loss: 2.2570]\n",
      "[Epoch 7/500] [Batch 244/312] [D IS TRAINING] [D loss: 0.1595] [G loss: 2.3857]\n",
      "[Epoch 7/500] [Batch 245/312] [D IS TRAINING] [D loss: 0.2569] [G loss: 1.5000]\n",
      "[Epoch 7/500] [Batch 246/312] [D IS TRAINING] [D loss: 0.1871] [G loss: 3.3129]\n",
      "[Epoch 7/500] [Batch 247/312] [D IS TRAINING] [D loss: 0.1701] [G loss: 2.3831]\n",
      "[Epoch 7/500] [Batch 248/312] [D IS TRAINING] [D loss: 0.2476] [G loss: 1.3979]\n",
      "[Epoch 7/500] [Batch 249/312] [D IS TRAINING] [D loss: 0.2360] [G loss: 2.6547]\n",
      "[Epoch 7/500] [Batch 250/312] [D IS TRAINING] [D loss: 0.1884] [G loss: 2.5919]\n",
      "[Epoch 7/500] [Batch 251/312] [D IS TRAINING] [D loss: 0.2218] [G loss: 1.5334]\n",
      "[Epoch 7/500] [Batch 252/312] [D IS TRAINING] [D loss: 0.1916] [G loss: 2.4201]\n",
      "[Epoch 7/500] [Batch 253/312] [D IS TRAINING] [D loss: 0.2324] [G loss: 2.5897]\n",
      "[Epoch 7/500] [Batch 254/312] [D IS TRAINING] [D loss: 0.2540] [G loss: 1.3094]\n",
      "[Epoch 7/500] [Batch 255/312] [D IS TRAINING] [D loss: 0.2643] [G loss: 3.1017]\n",
      "[Epoch 7/500] [Batch 256/312] [D IS TRAINING] [D loss: 0.2943] [G loss: 1.2608]\n",
      "[Epoch 7/500] [Batch 257/312] [D IS TRAINING] [D loss: 0.3994] [G loss: 3.4776]\n",
      "[Epoch 7/500] [Batch 258/312] [D IS TRAINING] [D loss: 0.3376] [G loss: 1.0065]\n",
      "[Epoch 7/500] [Batch 259/312] [D IS TRAINING] [D loss: 0.2413] [G loss: 2.7401]\n",
      "[Epoch 7/500] [Batch 260/312] [D IS TRAINING] [D loss: 0.1682] [G loss: 2.1769]\n",
      "[Epoch 7/500] [Batch 261/312] [D IS TRAINING] [D loss: 0.1533] [G loss: 1.8481]\n",
      "[Epoch 7/500] [Batch 262/312] [D IS TRAINING] [D loss: 0.1524] [G loss: 2.5217]\n",
      "[Epoch 7/500] [Batch 263/312] [D IS TRAINING] [D loss: 0.1548] [G loss: 2.9020]\n",
      "[Epoch 7/500] [Batch 264/312] [D IS TRAINING] [D loss: 0.2695] [G loss: 1.3304]\n",
      "[Epoch 7/500] [Batch 265/312] [D IS TRAINING] [D loss: 0.3263] [G loss: 1.5879]\n",
      "[Epoch 7/500] [Batch 266/312] [D IS TRAINING] [D loss: 0.3109] [G loss: 2.6605]\n",
      "[Epoch 7/500] [Batch 267/312] [D IS TRAINING] [D loss: 0.1977] [G loss: 1.7162]\n",
      "[Epoch 7/500] [Batch 268/312] [D IS TRAINING] [D loss: 0.2376] [G loss: 1.7061]\n",
      "[Epoch 7/500] [Batch 269/312] [D IS TRAINING] [D loss: 0.1951] [G loss: 3.3869]\n",
      "[Epoch 7/500] [Batch 270/312] [D IS TRAINING] [D loss: 0.1109] [G loss: 2.9468]\n",
      "[Epoch 7/500] [Batch 271/312] [D IS TRAINING] [D loss: 0.3931] [G loss: 0.8536]\n",
      "[Epoch 7/500] [Batch 272/312] [D IS TRAINING] [D loss: 0.6695] [G loss: 5.0412]\n",
      "[Epoch 7/500] [Batch 273/312] [D IS TRAINING] [D loss: 0.1364] [G loss: 2.4181]\n",
      "[Epoch 7/500] [Batch 274/312] [D IS TRAINING] [D loss: 0.5402] [G loss: 0.6578]\n",
      "[Epoch 7/500] [Batch 275/312] [D IS TRAINING] [D loss: 0.8411] [G loss: 6.5732]\n",
      "[Epoch 7/500] [Batch 276/312] [D IS TRAINING] [D loss: 0.5383] [G loss: 0.7499]\n",
      "[Epoch 7/500] [Batch 277/312] [D IS TRAINING] [D loss: 0.5655] [G loss: 4.6954]\n",
      "[Epoch 7/500] [Batch 278/312] [D IS TRAINING] [D loss: 0.2723] [G loss: 1.9773]\n",
      "[Epoch 7/500] [Batch 279/312] [D IS TRAINING] [D loss: 0.3050] [G loss: 1.5939]\n",
      "[Epoch 7/500] [Batch 280/312] [D IS TRAINING] [D loss: 0.2703] [G loss: 2.2564]\n",
      "[Epoch 7/500] [Batch 281/312] [D IS TRAINING] [D loss: 0.2907] [G loss: 1.6676]\n",
      "[Epoch 7/500] [Batch 282/312] [D IS TRAINING] [D loss: 0.2590] [G loss: 2.3085]\n",
      "[Epoch 7/500] [Batch 283/312] [D IS TRAINING] [D loss: 0.2768] [G loss: 1.4655]\n",
      "[Epoch 7/500] [Batch 284/312] [D IS TRAINING] [D loss: 0.2123] [G loss: 2.5505]\n",
      "[Epoch 7/500] [Batch 285/312] [D IS TRAINING] [D loss: 0.2353] [G loss: 1.9553]\n",
      "[Epoch 7/500] [Batch 286/312] [D IS TRAINING] [D loss: 0.2694] [G loss: 1.4409]\n",
      "[Epoch 7/500] [Batch 287/312] [D IS TRAINING] [D loss: 0.2499] [G loss: 3.0812]\n",
      "[Epoch 7/500] [Batch 288/312] [D IS TRAINING] [D loss: 0.1901] [G loss: 1.7139]\n",
      "[Epoch 7/500] [Batch 289/312] [D IS TRAINING] [D loss: 0.1386] [G loss: 2.7902]\n",
      "[Epoch 7/500] [Batch 290/312] [D IS TRAINING] [D loss: 0.1684] [G loss: 2.3407]\n",
      "[Epoch 7/500] [Batch 291/312] [D IS TRAINING] [D loss: 0.2317] [G loss: 1.5784]\n",
      "[Epoch 7/500] [Batch 292/312] [D IS TRAINING] [D loss: 0.2801] [G loss: 2.5976]\n",
      "[Epoch 7/500] [Batch 293/312] [D IS TRAINING] [D loss: 0.2468] [G loss: 1.4831]\n",
      "[Epoch 7/500] [Batch 294/312] [D IS TRAINING] [D loss: 0.1806] [G loss: 2.5369]\n",
      "[Epoch 7/500] [Batch 295/312] [D IS TRAINING] [D loss: 0.2437] [G loss: 2.2982]\n",
      "[Epoch 7/500] [Batch 296/312] [D IS TRAINING] [D loss: 0.1730] [G loss: 2.2543]\n",
      "[Epoch 7/500] [Batch 297/312] [D IS TRAINING] [D loss: 0.2286] [G loss: 1.7134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/500] [Batch 298/312] [D IS TRAINING] [D loss: 0.2615] [G loss: 2.3726]\n",
      "[Epoch 7/500] [Batch 299/312] [D IS TRAINING] [D loss: 0.2497] [G loss: 1.5614]\n",
      "[Epoch 7/500] [Batch 300/312] [D IS TRAINING] [D loss: 0.2026] [G loss: 2.8630]\n",
      "[Epoch 7/500] [Batch 301/312] [D IS TRAINING] [D loss: 0.1730] [G loss: 1.8661]\n",
      "[Epoch 7/500] [Batch 302/312] [D IS TRAINING] [D loss: 0.1780] [G loss: 2.0457]\n",
      "[Epoch 7/500] [Batch 303/312] [D IS TRAINING] [D loss: 0.2433] [G loss: 2.8439]\n",
      "[Epoch 7/500] [Batch 304/312] [D IS TRAINING] [D loss: 0.2789] [G loss: 1.1564]\n",
      "[Epoch 7/500] [Batch 305/312] [D IS TRAINING] [D loss: 0.3086] [G loss: 3.9764]\n",
      "[Epoch 7/500] [Batch 306/312] [D IS TRAINING] [D loss: 0.1529] [G loss: 1.8955]\n",
      "[Epoch 7/500] [Batch 307/312] [D IS TRAINING] [D loss: 0.1438] [G loss: 2.0373]\n",
      "[Epoch 7/500] [Batch 308/312] [D IS TRAINING] [D loss: 0.1194] [G loss: 3.1531]\n",
      "[Epoch 7/500] [Batch 309/312] [D IS TRAINING] [D loss: 0.1382] [G loss: 3.1202]\n",
      "[Epoch 7/500] [Batch 310/312] [D IS TRAINING] [D loss: 0.2123] [G loss: 1.4587]\n",
      "[Epoch 7/500] [Batch 311/312] [D IS TRAINING] [D loss: 0.2048] [G loss: 2.6499]\n",
      "[Epoch 8/500] [Batch 0/312] [D IS TRAINING] [D loss: 0.1983] [G loss: 2.7753]\n",
      "[Epoch 8/500] [Batch 1/312] [D IS TRAINING] [D loss: 0.1548] [G loss: 1.8479]\n",
      "[Epoch 8/500] [Batch 2/312] [D IS TRAINING] [D loss: 0.1482] [G loss: 2.6050]\n",
      "[Epoch 8/500] [Batch 3/312] [D IS TRAINING] [D loss: 0.1657] [G loss: 3.1104]\n",
      "[Epoch 8/500] [Batch 4/312] [D IS TRAINING] [D loss: 0.2461] [G loss: 1.3510]\n",
      "[Epoch 8/500] [Batch 5/312] [D IS TRAINING] [D loss: 0.3340] [G loss: 2.7112]\n",
      "[Epoch 8/500] [Batch 6/312] [D IS TRAINING] [D loss: 0.3167] [G loss: 1.7292]\n",
      "[Epoch 8/500] [Batch 7/312] [D IS TRAINING] [D loss: 0.2178] [G loss: 2.3378]\n",
      "[Epoch 8/500] [Batch 8/312] [D IS TRAINING] [D loss: 0.1839] [G loss: 2.0011]\n",
      "[Epoch 8/500] [Batch 9/312] [D IS TRAINING] [D loss: 0.2217] [G loss: 2.1948]\n",
      "[Epoch 8/500] [Batch 10/312] [D IS TRAINING] [D loss: 0.2036] [G loss: 1.6646]\n",
      "[Epoch 8/500] [Batch 11/312] [D IS TRAINING] [D loss: 0.2143] [G loss: 2.7895]\n",
      "[Epoch 8/500] [Batch 12/312] [D IS TRAINING] [D loss: 0.2089] [G loss: 1.5672]\n",
      "[Epoch 8/500] [Batch 13/312] [D IS TRAINING] [D loss: 0.1782] [G loss: 3.2786]\n",
      "[Epoch 8/500] [Batch 14/312] [D IS TRAINING] [D loss: 0.1773] [G loss: 1.8295]\n",
      "[Epoch 8/500] [Batch 15/312] [D IS TRAINING] [D loss: 0.1593] [G loss: 2.6205]\n",
      "[Epoch 8/500] [Batch 16/312] [D IS TRAINING] [D loss: 0.1445] [G loss: 2.2832]\n",
      "[Epoch 8/500] [Batch 17/312] [D IS TRAINING] [D loss: 0.1586] [G loss: 2.1580]\n",
      "[Epoch 8/500] [Batch 18/312] [D IS TRAINING] [D loss: 0.1151] [G loss: 2.7769]\n",
      "[Epoch 8/500] [Batch 19/312] [D IS TRAINING] [D loss: 0.1744] [G loss: 1.8686]\n",
      "[Epoch 8/500] [Batch 20/312] [D IS TRAINING] [D loss: 0.1576] [G loss: 3.2332]\n",
      "[Epoch 8/500] [Batch 21/312] [D IS TRAINING] [D loss: 0.2016] [G loss: 1.4467]\n",
      "[Epoch 8/500] [Batch 22/312] [D IS TRAINING] [D loss: 0.3056] [G loss: 3.2846]\n",
      "[Epoch 8/500] [Batch 23/312] [D IS TRAINING] [D loss: 0.2661] [G loss: 1.4721]\n",
      "[Epoch 8/500] [Batch 24/312] [D IS TRAINING] [D loss: 0.2247] [G loss: 3.9336]\n",
      "[Epoch 8/500] [Batch 25/312] [D IS TRAINING] [D loss: 0.0715] [G loss: 3.1589]\n",
      "[Epoch 8/500] [Batch 26/312] [D IS TRAINING] [D loss: 0.2731] [G loss: 1.1870]\n",
      "[Epoch 8/500] [Batch 27/312] [D IS TRAINING] [D loss: 0.3752] [G loss: 3.6340]\n",
      "[Epoch 8/500] [Batch 28/312] [D IS TRAINING] [D loss: 0.1749] [G loss: 1.8348]\n",
      "[Epoch 8/500] [Batch 29/312] [D IS TRAINING] [D loss: 0.1514] [G loss: 3.1544]\n",
      "[Epoch 8/500] [Batch 30/312] [D IS TRAINING] [D loss: 0.1746] [G loss: 2.3136]\n",
      "[Epoch 8/500] [Batch 31/312] [D IS TRAINING] [D loss: 0.1885] [G loss: 1.8889]\n",
      "[Epoch 8/500] [Batch 32/312] [D IS TRAINING] [D loss: 0.3667] [G loss: 2.9386]\n",
      "[Epoch 8/500] [Batch 33/312] [D IS TRAINING] [D loss: 0.7276] [G loss: 0.4089]\n",
      "[Epoch 8/500] [Batch 34/312] [D IS TRAINING] [D loss: 1.5589] [G loss: 7.1387]\n",
      "[Epoch 8/500] [Batch 35/312] [D IS TRAINING] [D loss: 0.3611] [G loss: 1.3032]\n",
      "[Epoch 8/500] [Batch 36/312] [D IS TRAINING] [D loss: 0.3183] [G loss: 1.7106]\n",
      "[Epoch 8/500] [Batch 37/312] [D IS TRAINING] [D loss: 0.3860] [G loss: 3.6845]\n",
      "[Epoch 8/500] [Batch 38/312] [D IS TRAINING] [D loss: 0.4421] [G loss: 0.8953]\n",
      "[Epoch 8/500] [Batch 39/312] [D IS TRAINING] [D loss: 0.4885] [G loss: 3.9027]\n",
      "[Epoch 8/500] [Batch 40/312] [D IS TRAINING] [D loss: 0.3158] [G loss: 1.3726]\n",
      "[Epoch 8/500] [Batch 41/312] [D IS TRAINING] [D loss: 0.1898] [G loss: 2.0595]\n",
      "[Epoch 8/500] [Batch 42/312] [D IS TRAINING] [D loss: 0.2532] [G loss: 2.6328]\n",
      "[Epoch 8/500] [Batch 43/312] [D IS TRAINING] [D loss: 0.2421] [G loss: 1.7505]\n",
      "[Epoch 8/500] [Batch 44/312] [D IS TRAINING] [D loss: 0.2265] [G loss: 1.7296]\n",
      "[Epoch 8/500] [Batch 45/312] [D IS TRAINING] [D loss: 0.2415] [G loss: 2.3507]\n",
      "[Epoch 8/500] [Batch 46/312] [D IS TRAINING] [D loss: 0.2526] [G loss: 1.7572]\n",
      "[Epoch 8/500] [Batch 47/312] [D IS TRAINING] [D loss: 0.2239] [G loss: 1.7797]\n",
      "[Epoch 8/500] [Batch 48/312] [D IS TRAINING] [D loss: 0.2010] [G loss: 2.1521]\n",
      "[Epoch 8/500] [Batch 49/312] [D IS TRAINING] [D loss: 0.1776] [G loss: 2.1783]\n",
      "[Epoch 8/500] [Batch 50/312] [D IS TRAINING] [D loss: 0.2213] [G loss: 1.8337]\n",
      "[Epoch 8/500] [Batch 51/312] [D IS TRAINING] [D loss: 0.2687] [G loss: 1.8791]\n",
      "[Epoch 8/500] [Batch 52/312] [D IS TRAINING] [D loss: 0.2632] [G loss: 1.9542]\n",
      "[Epoch 8/500] [Batch 53/312] [D IS TRAINING] [D loss: 0.2952] [G loss: 1.6549]\n",
      "[Epoch 8/500] [Batch 54/312] [D IS TRAINING] [D loss: 0.2901] [G loss: 2.2760]\n",
      "[Epoch 8/500] [Batch 55/312] [D IS TRAINING] [D loss: 0.2447] [G loss: 1.7670]\n",
      "[Epoch 8/500] [Batch 56/312] [D IS TRAINING] [D loss: 0.1753] [G loss: 2.2567]\n",
      "[Epoch 8/500] [Batch 57/312] [D IS TRAINING] [D loss: 0.2014] [G loss: 2.0892]\n",
      "[Epoch 8/500] [Batch 58/312] [D IS TRAINING] [D loss: 0.3460] [G loss: 1.1454]\n",
      "[Epoch 8/500] [Batch 59/312] [D IS TRAINING] [D loss: 0.3980] [G loss: 3.8402]\n",
      "[Epoch 8/500] [Batch 60/312] [D IS TRAINING] [D loss: 0.3940] [G loss: 0.9315]\n",
      "[Epoch 8/500] [Batch 61/312] [D IS TRAINING] [D loss: 0.3455] [G loss: 3.8733]\n",
      "[Epoch 8/500] [Batch 62/312] [D IS TRAINING] [D loss: 0.1883] [G loss: 1.8577]\n",
      "[Epoch 8/500] [Batch 63/312] [D IS TRAINING] [D loss: 0.2162] [G loss: 2.1739]\n",
      "[Epoch 8/500] [Batch 64/312] [D IS TRAINING] [D loss: 0.2281] [G loss: 1.7830]\n",
      "[Epoch 8/500] [Batch 65/312] [D IS TRAINING] [D loss: 0.2552] [G loss: 2.1273]\n",
      "[Epoch 8/500] [Batch 66/312] [D IS TRAINING] [D loss: 0.2870] [G loss: 1.4056]\n",
      "[Epoch 8/500] [Batch 67/312] [D IS TRAINING] [D loss: 0.2773] [G loss: 3.1306]\n",
      "[Epoch 8/500] [Batch 68/312] [D IS TRAINING] [D loss: 0.1898] [G loss: 2.1225]\n",
      "[Epoch 8/500] [Batch 69/312] [D IS TRAINING] [D loss: 0.2116] [G loss: 1.8130]\n",
      "[Epoch 8/500] [Batch 70/312] [D IS TRAINING] [D loss: 0.2183] [G loss: 2.4051]\n",
      "[Epoch 8/500] [Batch 71/312] [D IS TRAINING] [D loss: 0.2014] [G loss: 1.7735]\n",
      "[Epoch 8/500] [Batch 72/312] [D IS TRAINING] [D loss: 0.2509] [G loss: 2.6564]\n",
      "[Epoch 8/500] [Batch 73/312] [D IS TRAINING] [D loss: 0.3005] [G loss: 1.1627]\n",
      "[Epoch 8/500] [Batch 74/312] [D IS TRAINING] [D loss: 0.2880] [G loss: 2.8356]\n",
      "[Epoch 8/500] [Batch 75/312] [D IS TRAINING] [D loss: 0.2523] [G loss: 1.4902]\n",
      "[Epoch 8/500] [Batch 76/312] [D IS TRAINING] [D loss: 0.2571] [G loss: 1.9706]\n",
      "[Epoch 8/500] [Batch 77/312] [D IS TRAINING] [D loss: 0.2239] [G loss: 2.2332]\n",
      "[Epoch 8/500] [Batch 78/312] [D IS TRAINING] [D loss: 0.2642] [G loss: 1.6313]\n",
      "[Epoch 8/500] [Batch 79/312] [D IS TRAINING] [D loss: 0.2199] [G loss: 2.0839]\n",
      "[Epoch 8/500] [Batch 80/312] [D IS TRAINING] [D loss: 0.2094] [G loss: 2.0053]\n",
      "[Epoch 8/500] [Batch 81/312] [D IS TRAINING] [D loss: 0.1856] [G loss: 2.0329]\n",
      "[Epoch 8/500] [Batch 82/312] [D IS TRAINING] [D loss: 0.2543] [G loss: 1.8783]\n",
      "[Epoch 8/500] [Batch 83/312] [D IS TRAINING] [D loss: 0.2274] [G loss: 1.9430]\n",
      "[Epoch 8/500] [Batch 84/312] [D IS TRAINING] [D loss: 0.2402] [G loss: 1.6137]\n",
      "[Epoch 8/500] [Batch 85/312] [D IS TRAINING] [D loss: 0.2445] [G loss: 2.4653]\n",
      "[Epoch 8/500] [Batch 86/312] [D IS TRAINING] [D loss: 0.2253] [G loss: 1.6030]\n",
      "[Epoch 8/500] [Batch 87/312] [D IS TRAINING] [D loss: 0.1821] [G loss: 2.5657]\n",
      "[Epoch 8/500] [Batch 88/312] [D IS TRAINING] [D loss: 0.1823] [G loss: 1.9550]\n",
      "[Epoch 8/500] [Batch 89/312] [D IS TRAINING] [D loss: 0.1568] [G loss: 2.5603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/500] [Batch 90/312] [D IS TRAINING] [D loss: 0.1756] [G loss: 2.0243]\n",
      "[Epoch 8/500] [Batch 91/312] [D IS TRAINING] [D loss: 0.2066] [G loss: 1.8015]\n",
      "[Epoch 8/500] [Batch 92/312] [D IS TRAINING] [D loss: 0.3092] [G loss: 2.9417]\n",
      "[Epoch 8/500] [Batch 93/312] [D IS TRAINING] [D loss: 0.5459] [G loss: 0.6635]\n",
      "[Epoch 8/500] [Batch 94/312] [D IS TRAINING] [D loss: 0.8828] [G loss: 5.7164]\n",
      "[Epoch 8/500] [Batch 95/312] [D IS TRAINING] [D loss: 0.3125] [G loss: 1.2964]\n",
      "[Epoch 8/500] [Batch 96/312] [D IS TRAINING] [D loss: 0.1898] [G loss: 2.2746]\n",
      "[Epoch 8/500] [Batch 97/312] [D IS TRAINING] [D loss: 0.2838] [G loss: 1.7879]\n",
      "[Epoch 8/500] [Batch 98/312] [D IS TRAINING] [D loss: 0.3362] [G loss: 2.3632]\n",
      "[Epoch 8/500] [Batch 99/312] [D IS TRAINING] [D loss: 0.3573] [G loss: 1.0159]\n",
      "[Epoch 8/500] [Batch 100/312] [D IS TRAINING] [D loss: 0.2972] [G loss: 3.6132]\n",
      "[Epoch 8/500] [Batch 101/312] [D IS TRAINING] [D loss: 0.2116] [G loss: 1.9493]\n",
      "[Epoch 8/500] [Batch 102/312] [D IS TRAINING] [D loss: 0.1463] [G loss: 2.2170]\n",
      "[Epoch 8/500] [Batch 103/312] [D IS TRAINING] [D loss: 0.1904] [G loss: 2.3514]\n",
      "[Epoch 8/500] [Batch 104/312] [D IS TRAINING] [D loss: 0.1677] [G loss: 2.2467]\n",
      "[Epoch 8/500] [Batch 105/312] [D IS TRAINING] [D loss: 0.1932] [G loss: 1.8765]\n",
      "[Epoch 8/500] [Batch 106/312] [D IS TRAINING] [D loss: 0.2055] [G loss: 2.1204]\n",
      "[Epoch 8/500] [Batch 107/312] [D IS TRAINING] [D loss: 0.2247] [G loss: 2.3435]\n",
      "[Epoch 8/500] [Batch 108/312] [D IS TRAINING] [D loss: 0.2376] [G loss: 1.6212]\n",
      "[Epoch 8/500] [Batch 109/312] [D IS TRAINING] [D loss: 0.1739] [G loss: 2.4440]\n",
      "[Epoch 8/500] [Batch 110/312] [D IS TRAINING] [D loss: 0.2003] [G loss: 2.0873]\n",
      "[Epoch 8/500] [Batch 111/312] [D IS TRAINING] [D loss: 0.1654] [G loss: 2.2885]\n",
      "[Epoch 8/500] [Batch 112/312] [D IS TRAINING] [D loss: 0.1640] [G loss: 2.1308]\n",
      "[Epoch 8/500] [Batch 113/312] [D IS TRAINING] [D loss: 0.1670] [G loss: 3.0048]\n",
      "[Epoch 8/500] [Batch 114/312] [D IS TRAINING] [D loss: 0.2368] [G loss: 1.6083]\n",
      "[Epoch 8/500] [Batch 115/312] [D IS TRAINING] [D loss: 0.2147] [G loss: 3.0662]\n",
      "[Epoch 8/500] [Batch 116/312] [D IS TRAINING] [D loss: 0.1830] [G loss: 2.0763]\n",
      "[Epoch 8/500] [Batch 117/312] [D IS TRAINING] [D loss: 0.2137] [G loss: 2.0284]\n",
      "[Epoch 8/500] [Batch 118/312] [D IS TRAINING] [D loss: 0.1722] [G loss: 2.4034]\n",
      "[Epoch 8/500] [Batch 119/312] [D IS TRAINING] [D loss: 0.1521] [G loss: 2.0998]\n",
      "[Epoch 8/500] [Batch 120/312] [D IS TRAINING] [D loss: 0.1629] [G loss: 2.6467]\n",
      "[Epoch 8/500] [Batch 121/312] [D IS TRAINING] [D loss: 0.1570] [G loss: 2.0595]\n",
      "[Epoch 8/500] [Batch 122/312] [D IS TRAINING] [D loss: 0.1797] [G loss: 2.3557]\n",
      "[Epoch 8/500] [Batch 123/312] [D IS TRAINING] [D loss: 0.2088] [G loss: 1.9516]\n",
      "[Epoch 8/500] [Batch 124/312] [D IS TRAINING] [D loss: 0.2347] [G loss: 2.5871]\n",
      "[Epoch 8/500] [Batch 125/312] [D IS TRAINING] [D loss: 0.1844] [G loss: 2.1634]\n",
      "[Epoch 8/500] [Batch 126/312] [D IS TRAINING] [D loss: 0.2292] [G loss: 1.8228]\n",
      "[Epoch 8/500] [Batch 127/312] [D IS TRAINING] [D loss: 0.1627] [G loss: 2.3754]\n",
      "[Epoch 8/500] [Batch 128/312] [D IS TRAINING] [D loss: 0.1631] [G loss: 2.3718]\n",
      "[Epoch 8/500] [Batch 129/312] [D IS TRAINING] [D loss: 0.1888] [G loss: 1.8975]\n",
      "[Epoch 8/500] [Batch 130/312] [D IS TRAINING] [D loss: 0.1885] [G loss: 2.9338]\n",
      "[Epoch 8/500] [Batch 131/312] [D IS TRAINING] [D loss: 0.1647] [G loss: 1.9788]\n",
      "[Epoch 8/500] [Batch 132/312] [D IS TRAINING] [D loss: 0.1727] [G loss: 2.6305]\n",
      "[Epoch 8/500] [Batch 133/312] [D IS TRAINING] [D loss: 0.1952] [G loss: 1.6044]\n",
      "[Epoch 8/500] [Batch 134/312] [D IS TRAINING] [D loss: 0.2373] [G loss: 2.8796]\n",
      "[Epoch 8/500] [Batch 135/312] [D IS TRAINING] [D loss: 0.2220] [G loss: 1.4544]\n",
      "[Epoch 8/500] [Batch 136/312] [D IS TRAINING] [D loss: 0.2251] [G loss: 3.0537]\n",
      "[Epoch 8/500] [Batch 137/312] [D IS TRAINING] [D loss: 0.2154] [G loss: 1.5945]\n",
      "[Epoch 8/500] [Batch 138/312] [D IS TRAINING] [D loss: 0.2105] [G loss: 2.6197]\n",
      "[Epoch 8/500] [Batch 139/312] [D IS TRAINING] [D loss: 0.1238] [G loss: 2.5360]\n",
      "[Epoch 8/500] [Batch 140/312] [D IS TRAINING] [D loss: 0.1980] [G loss: 1.6102]\n",
      "[Epoch 8/500] [Batch 141/312] [D IS TRAINING] [D loss: 0.2879] [G loss: 3.5690]\n",
      "[Epoch 8/500] [Batch 142/312] [D IS TRAINING] [D loss: 0.1648] [G loss: 1.6955]\n",
      "[Epoch 8/500] [Batch 143/312] [D IS TRAINING] [D loss: 0.1746] [G loss: 1.9503]\n",
      "[Epoch 8/500] [Batch 144/312] [D IS TRAINING] [D loss: 0.2095] [G loss: 3.4280]\n",
      "[Epoch 8/500] [Batch 145/312] [D IS TRAINING] [D loss: 0.1846] [G loss: 1.7025]\n",
      "[Epoch 8/500] [Batch 146/312] [D IS TRAINING] [D loss: 0.1528] [G loss: 2.6741]\n",
      "[Epoch 8/500] [Batch 147/312] [D IS TRAINING] [D loss: 0.1795] [G loss: 2.8429]\n",
      "[Epoch 8/500] [Batch 148/312] [D IS TRAINING] [D loss: 0.2104] [G loss: 1.4928]\n",
      "[Epoch 8/500] [Batch 149/312] [D IS TRAINING] [D loss: 0.2996] [G loss: 3.7137]\n",
      "[Epoch 8/500] [Batch 150/312] [D IS TRAINING] [D loss: 0.2876] [G loss: 1.6605]\n",
      "[Epoch 8/500] [Batch 151/312] [D IS TRAINING] [D loss: 0.1845] [G loss: 3.2612]\n",
      "[Epoch 8/500] [Batch 152/312] [D IS TRAINING] [D loss: 0.1222] [G loss: 2.9810]\n",
      "[Epoch 8/500] [Batch 153/312] [D IS TRAINING] [D loss: 0.1850] [G loss: 1.7689]\n",
      "[Epoch 8/500] [Batch 154/312] [D IS TRAINING] [D loss: 0.1352] [G loss: 2.6472]\n",
      "[Epoch 8/500] [Batch 155/312] [D IS TRAINING] [D loss: 0.1432] [G loss: 3.6940]\n",
      "[Epoch 8/500] [Batch 156/312] [D IS TRAINING] [D loss: 0.2621] [G loss: 1.2734]\n",
      "[Epoch 8/500] [Batch 157/312] [D IS TRAINING] [D loss: 0.4059] [G loss: 3.2481]\n",
      "[Epoch 8/500] [Batch 158/312] [D IS TRAINING] [D loss: 0.3645] [G loss: 1.1926]\n",
      "[Epoch 8/500] [Batch 159/312] [D IS TRAINING] [D loss: 0.3981] [G loss: 4.0632]\n",
      "[Epoch 8/500] [Batch 160/312] [D IS TRAINING] [D loss: 0.3419] [G loss: 1.1304]\n",
      "[Epoch 8/500] [Batch 161/312] [D IS TRAINING] [D loss: 0.6128] [G loss: 4.4056]\n",
      "[Epoch 8/500] [Batch 162/312] [D IS TRAINING] [D loss: 0.2333] [G loss: 1.3832]\n",
      "[Epoch 8/500] [Batch 163/312] [D IS TRAINING] [D loss: 0.2534] [G loss: 2.4753]\n",
      "[Epoch 8/500] [Batch 164/312] [D IS TRAINING] [D loss: 0.1426] [G loss: 2.8731]\n",
      "[Epoch 8/500] [Batch 165/312] [D IS TRAINING] [D loss: 0.1553] [G loss: 2.2213]\n",
      "[Epoch 8/500] [Batch 166/312] [D IS TRAINING] [D loss: 0.2461] [G loss: 2.2774]\n",
      "[Epoch 8/500] [Batch 167/312] [D IS TRAINING] [D loss: 0.1394] [G loss: 2.4740]\n",
      "[Epoch 8/500] [Batch 168/312] [D IS TRAINING] [D loss: 0.1138] [G loss: 2.3393]\n",
      "[Epoch 8/500] [Batch 169/312] [D IS TRAINING] [D loss: 0.1788] [G loss: 2.9025]\n",
      "[Epoch 8/500] [Batch 170/312] [D IS TRAINING] [D loss: 0.3456] [G loss: 1.0259]\n",
      "[Epoch 8/500] [Batch 171/312] [D IS TRAINING] [D loss: 0.4928] [G loss: 4.0939]\n",
      "[Epoch 8/500] [Batch 172/312] [D IS TRAINING] [D loss: 0.2770] [G loss: 1.2144]\n",
      "[Epoch 8/500] [Batch 173/312] [D IS TRAINING] [D loss: 0.1985] [G loss: 3.0918]\n",
      "[Epoch 8/500] [Batch 174/312] [D IS TRAINING] [D loss: 0.1618] [G loss: 2.6813]\n",
      "[Epoch 8/500] [Batch 175/312] [D IS TRAINING] [D loss: 0.1287] [G loss: 2.5494]\n",
      "[Epoch 8/500] [Batch 176/312] [D IS TRAINING] [D loss: 0.1770] [G loss: 2.2089]\n",
      "[Epoch 8/500] [Batch 177/312] [D IS TRAINING] [D loss: 0.1359] [G loss: 2.4025]\n",
      "[Epoch 8/500] [Batch 178/312] [D IS TRAINING] [D loss: 0.1198] [G loss: 2.7492]\n",
      "[Epoch 8/500] [Batch 179/312] [D IS TRAINING] [D loss: 0.1233] [G loss: 2.1495]\n",
      "[Epoch 8/500] [Batch 180/312] [D IS TRAINING] [D loss: 0.1123] [G loss: 2.8673]\n",
      "[Epoch 8/500] [Batch 181/312] [D IS TRAINING] [D loss: 0.1233] [G loss: 2.8085]\n",
      "[Epoch 8/500] [Batch 182/312] [D IS TRAINING] [D loss: 0.1907] [G loss: 1.7282]\n",
      "[Epoch 8/500] [Batch 183/312] [D IS TRAINING] [D loss: 0.1572] [G loss: 2.6767]\n",
      "[Epoch 8/500] [Batch 184/312] [D IS TRAINING] [D loss: 0.1497] [G loss: 2.7890]\n",
      "[Epoch 8/500] [Batch 185/312] [D IS TRAINING] [D loss: 0.2056] [G loss: 1.6253]\n",
      "[Epoch 8/500] [Batch 186/312] [D IS TRAINING] [D loss: 0.1866] [G loss: 2.7721]\n",
      "[Epoch 8/500] [Batch 187/312] [D IS TRAINING] [D loss: 0.1638] [G loss: 2.0598]\n",
      "[Epoch 8/500] [Batch 188/312] [D IS TRAINING] [D loss: 0.1301] [G loss: 2.6727]\n",
      "[Epoch 8/500] [Batch 189/312] [D IS TRAINING] [D loss: 0.0816] [G loss: 2.9446]\n",
      "[Epoch 8/500] [Batch 190/312] [D IS TRAINING] [D loss: 0.1654] [G loss: 1.8061]\n",
      "[Epoch 8/500] [Batch 191/312] [D IS TRAINING] [D loss: 0.2341] [G loss: 2.9615]\n",
      "[Epoch 8/500] [Batch 192/312] [D IS TRAINING] [D loss: 0.1862] [G loss: 1.6423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/500] [Batch 193/312] [D IS TRAINING] [D loss: 0.2367] [G loss: 3.7040]\n",
      "[Epoch 8/500] [Batch 194/312] [D IS TRAINING] [D loss: 0.1657] [G loss: 1.6753]\n",
      "[Epoch 8/500] [Batch 195/312] [D IS TRAINING] [D loss: 0.1417] [G loss: 3.0616]\n",
      "[Epoch 8/500] [Batch 196/312] [D IS TRAINING] [D loss: 0.1198] [G loss: 2.4707]\n",
      "[Epoch 8/500] [Batch 197/312] [D IS TRAINING] [D loss: 0.0931] [G loss: 2.8504]\n",
      "[Epoch 8/500] [Batch 198/312] [D IS TRAINING] [D loss: 0.1993] [G loss: 1.5623]\n",
      "[Epoch 8/500] [Batch 199/312] [D IS TRAINING] [D loss: 0.3251] [G loss: 3.9972]\n",
      "[Epoch 8/500] [Batch 200/312] [D IS TRAINING] [D loss: 0.3394] [G loss: 1.0353]\n",
      "[Epoch 8/500] [Batch 201/312] [D IS TRAINING] [D loss: 0.3677] [G loss: 3.7481]\n",
      "[Epoch 8/500] [Batch 202/312] [D IS TRAINING] [D loss: 0.5049] [G loss: 0.7185]\n",
      "[Epoch 8/500] [Batch 203/312] [D IS TRAINING] [D loss: 0.6304] [G loss: 5.5225]\n",
      "[Epoch 8/500] [Batch 204/312] [D IS TRAINING] [D loss: 1.2357] [G loss: 0.2200]\n",
      "[Epoch 8/500] [Batch 205/312] [D IS TRAINING] [D loss: 1.2837] [G loss: 6.9922]\n",
      "[Epoch 8/500] [Batch 206/312] [D IS TRAINING] [D loss: 0.5358] [G loss: 1.3407]\n",
      "[Epoch 8/500] [Batch 207/312] [D IS TRAINING] [D loss: 0.4249] [G loss: 1.1706]\n",
      "[Epoch 8/500] [Batch 208/312] [D IS TRAINING] [D loss: 0.4447] [G loss: 3.3636]\n",
      "[Epoch 8/500] [Batch 209/312] [D IS TRAINING] [D loss: 0.2520] [G loss: 1.5607]\n",
      "[Epoch 8/500] [Batch 210/312] [D IS TRAINING] [D loss: 0.2734] [G loss: 1.8600]\n",
      "[Epoch 8/500] [Batch 211/312] [D IS TRAINING] [D loss: 0.1956] [G loss: 3.0076]\n",
      "[Epoch 8/500] [Batch 212/312] [D IS TRAINING] [D loss: 0.2161] [G loss: 1.9693]\n",
      "[Epoch 8/500] [Batch 213/312] [D IS TRAINING] [D loss: 0.2286] [G loss: 2.2796]\n",
      "[Epoch 8/500] [Batch 214/312] [D IS TRAINING] [D loss: 0.2076] [G loss: 2.8767]\n",
      "[Epoch 8/500] [Batch 215/312] [D IS TRAINING] [D loss: 0.2516] [G loss: 1.5097]\n",
      "[Epoch 8/500] [Batch 216/312] [D IS TRAINING] [D loss: 0.1786] [G loss: 2.3527]\n",
      "[Epoch 8/500] [Batch 217/312] [D IS TRAINING] [D loss: 0.3470] [G loss: 2.6312]\n",
      "[Epoch 8/500] [Batch 218/312] [D IS TRAINING] [D loss: 0.4475] [G loss: 0.8457]\n",
      "[Epoch 8/500] [Batch 219/312] [D IS TRAINING] [D loss: 0.5199] [G loss: 4.3195]\n",
      "[Epoch 8/500] [Batch 220/312] [D IS TRAINING] [D loss: 0.4566] [G loss: 0.8721]\n",
      "[Epoch 8/500] [Batch 221/312] [D IS TRAINING] [D loss: 0.4836] [G loss: 3.7791]\n",
      "[Epoch 8/500] [Batch 222/312] [D IS TRAINING] [D loss: 0.2301] [G loss: 2.2803]\n",
      "[Epoch 8/500] [Batch 223/312] [D IS TRAINING] [D loss: 0.2857] [G loss: 1.4298]\n",
      "[Epoch 8/500] [Batch 224/312] [D IS TRAINING] [D loss: 0.2462] [G loss: 2.9173]\n",
      "[Epoch 8/500] [Batch 225/312] [D IS TRAINING] [D loss: 0.2499] [G loss: 2.0170]\n",
      "[Epoch 8/500] [Batch 226/312] [D IS TRAINING] [D loss: 0.2814] [G loss: 1.6331]\n",
      "[Epoch 8/500] [Batch 227/312] [D IS TRAINING] [D loss: 0.2429] [G loss: 2.2910]\n",
      "[Epoch 8/500] [Batch 228/312] [D IS TRAINING] [D loss: 0.2230] [G loss: 1.6980]\n",
      "[Epoch 8/500] [Batch 229/312] [D IS TRAINING] [D loss: 0.2753] [G loss: 2.7237]\n",
      "[Epoch 8/500] [Batch 230/312] [D IS TRAINING] [D loss: 0.4411] [G loss: 0.8323]\n",
      "[Epoch 8/500] [Batch 231/312] [D IS TRAINING] [D loss: 0.5276] [G loss: 3.9261]\n",
      "[Epoch 8/500] [Batch 232/312] [D IS TRAINING] [D loss: 0.2508] [G loss: 1.6284]\n",
      "[Epoch 8/500] [Batch 233/312] [D IS TRAINING] [D loss: 0.1203] [G loss: 3.1510]\n",
      "[Epoch 8/500] [Batch 234/312] [D IS TRAINING] [D loss: 0.1647] [G loss: 2.0949]\n",
      "[Epoch 8/500] [Batch 235/312] [D IS TRAINING] [D loss: 0.1646] [G loss: 2.7633]\n",
      "[Epoch 8/500] [Batch 236/312] [D IS TRAINING] [D loss: 0.1549] [G loss: 2.5675]\n",
      "[Epoch 8/500] [Batch 237/312] [D IS TRAINING] [D loss: 0.2271] [G loss: 1.5471]\n",
      "[Epoch 8/500] [Batch 238/312] [D IS TRAINING] [D loss: 0.2174] [G loss: 3.1830]\n",
      "[Epoch 8/500] [Batch 239/312] [D IS TRAINING] [D loss: 0.1788] [G loss: 1.8225]\n",
      "[Epoch 8/500] [Batch 240/312] [D IS TRAINING] [D loss: 0.1889] [G loss: 2.0411]\n",
      "[Epoch 8/500] [Batch 241/312] [D IS TRAINING] [D loss: 0.1669] [G loss: 3.2608]\n",
      "[Epoch 8/500] [Batch 242/312] [D IS TRAINING] [D loss: 0.2222] [G loss: 2.0354]\n",
      "[Epoch 8/500] [Batch 243/312] [D IS TRAINING] [D loss: 0.3133] [G loss: 1.2543]\n",
      "[Epoch 8/500] [Batch 244/312] [D IS TRAINING] [D loss: 0.5615] [G loss: 6.0460]\n",
      "[Epoch 8/500] [Batch 245/312] [D IS TRAINING] [D loss: 0.4559] [G loss: 0.8070]\n",
      "[Epoch 8/500] [Batch 246/312] [D IS TRAINING] [D loss: 0.2631] [G loss: 4.8422]\n",
      "[Epoch 8/500] [Batch 247/312] [D IS TRAINING] [D loss: 0.1375] [G loss: 2.7071]\n",
      "[Epoch 8/500] [Batch 248/312] [D IS TRAINING] [D loss: 0.2092] [G loss: 1.7094]\n",
      "[Epoch 8/500] [Batch 249/312] [D IS TRAINING] [D loss: 0.1911] [G loss: 3.2730]\n",
      "[Epoch 8/500] [Batch 250/312] [D IS TRAINING] [D loss: 0.1677] [G loss: 2.2198]\n",
      "[Epoch 8/500] [Batch 251/312] [D IS TRAINING] [D loss: 0.2613] [G loss: 1.5773]\n",
      "[Epoch 8/500] [Batch 252/312] [D IS TRAINING] [D loss: 0.3109] [G loss: 3.2591]\n",
      "[Epoch 8/500] [Batch 253/312] [D IS TRAINING] [D loss: 0.4891] [G loss: 0.8117]\n",
      "[Epoch 8/500] [Batch 254/312] [D IS TRAINING] [D loss: 0.3840] [G loss: 4.1115]\n",
      "[Epoch 8/500] [Batch 255/312] [D IS TRAINING] [D loss: 0.1834] [G loss: 2.0625]\n",
      "[Epoch 8/500] [Batch 256/312] [D IS TRAINING] [D loss: 0.2155] [G loss: 1.9372]\n",
      "[Epoch 8/500] [Batch 257/312] [D IS TRAINING] [D loss: 0.1521] [G loss: 2.4805]\n",
      "[Epoch 8/500] [Batch 258/312] [D IS TRAINING] [D loss: 0.1708] [G loss: 2.4699]\n",
      "[Epoch 8/500] [Batch 259/312] [D IS TRAINING] [D loss: 0.1514] [G loss: 2.0419]\n",
      "[Epoch 8/500] [Batch 260/312] [D IS TRAINING] [D loss: 0.2132] [G loss: 2.5162]\n",
      "[Epoch 8/500] [Batch 261/312] [D IS TRAINING] [D loss: 0.2245] [G loss: 1.7324]\n",
      "[Epoch 8/500] [Batch 262/312] [D IS TRAINING] [D loss: 0.2121] [G loss: 2.3320]\n",
      "[Epoch 8/500] [Batch 263/312] [D IS TRAINING] [D loss: 0.1695] [G loss: 2.3246]\n",
      "[Epoch 8/500] [Batch 264/312] [D IS TRAINING] [D loss: 0.2273] [G loss: 2.0177]\n",
      "[Epoch 8/500] [Batch 265/312] [D IS TRAINING] [D loss: 0.2067] [G loss: 2.0143]\n",
      "[Epoch 8/500] [Batch 266/312] [D IS TRAINING] [D loss: 0.1570] [G loss: 2.2508]\n",
      "[Epoch 8/500] [Batch 267/312] [D IS TRAINING] [D loss: 0.1502] [G loss: 2.5202]\n",
      "[Epoch 8/500] [Batch 268/312] [D IS TRAINING] [D loss: 0.2169] [G loss: 2.1485]\n",
      "[Epoch 8/500] [Batch 269/312] [D IS TRAINING] [D loss: 0.2076] [G loss: 1.6369]\n",
      "[Epoch 8/500] [Batch 270/312] [D IS TRAINING] [D loss: 0.2545] [G loss: 3.7602]\n",
      "[Epoch 8/500] [Batch 271/312] [D IS TRAINING] [D loss: 0.1424] [G loss: 1.9436]\n",
      "[Epoch 8/500] [Batch 272/312] [D IS TRAINING] [D loss: 0.1903] [G loss: 1.8141]\n",
      "[Epoch 8/500] [Batch 273/312] [D IS TRAINING] [D loss: 0.2229] [G loss: 3.5875]\n",
      "[Epoch 8/500] [Batch 274/312] [D IS TRAINING] [D loss: 0.1767] [G loss: 2.1604]\n",
      "[Epoch 8/500] [Batch 275/312] [D IS TRAINING] [D loss: 0.1408] [G loss: 2.0998]\n",
      "[Epoch 8/500] [Batch 276/312] [D IS TRAINING] [D loss: 0.1641] [G loss: 2.0962]\n",
      "[Epoch 8/500] [Batch 277/312] [D IS TRAINING] [D loss: 0.2004] [G loss: 2.8887]\n",
      "[Epoch 8/500] [Batch 278/312] [D IS TRAINING] [D loss: 0.1837] [G loss: 1.9154]\n",
      "[Epoch 8/500] [Batch 279/312] [D IS TRAINING] [D loss: 0.1642] [G loss: 2.1075]\n",
      "[Epoch 8/500] [Batch 280/312] [D IS TRAINING] [D loss: 0.3014] [G loss: 2.3730]\n",
      "[Epoch 8/500] [Batch 281/312] [D IS TRAINING] [D loss: 0.3010] [G loss: 1.3062]\n",
      "[Epoch 8/500] [Batch 282/312] [D IS TRAINING] [D loss: 0.3365] [G loss: 3.5473]\n",
      "[Epoch 8/500] [Batch 283/312] [D IS TRAINING] [D loss: 0.3188] [G loss: 1.0344]\n",
      "[Epoch 8/500] [Batch 284/312] [D IS TRAINING] [D loss: 0.3204] [G loss: 4.1400]\n",
      "[Epoch 8/500] [Batch 285/312] [D IS TRAINING] [D loss: 0.2468] [G loss: 1.8056]\n",
      "[Epoch 8/500] [Batch 286/312] [D IS TRAINING] [D loss: 0.2153] [G loss: 2.8884]\n",
      "[Epoch 8/500] [Batch 287/312] [D IS TRAINING] [D loss: 0.2399] [G loss: 1.6702]\n",
      "[Epoch 8/500] [Batch 288/312] [D IS TRAINING] [D loss: 0.2450] [G loss: 2.7129]\n",
      "[Epoch 8/500] [Batch 289/312] [D IS TRAINING] [D loss: 0.3033] [G loss: 1.2905]\n",
      "[Epoch 8/500] [Batch 290/312] [D IS TRAINING] [D loss: 0.2521] [G loss: 3.7941]\n",
      "[Epoch 8/500] [Batch 291/312] [D IS TRAINING] [D loss: 0.2073] [G loss: 1.6932]\n",
      "[Epoch 8/500] [Batch 292/312] [D IS TRAINING] [D loss: 0.2107] [G loss: 2.5196]\n",
      "[Epoch 8/500] [Batch 293/312] [D IS TRAINING] [D loss: 0.1389] [G loss: 2.4411]\n",
      "[Epoch 8/500] [Batch 294/312] [D IS TRAINING] [D loss: 0.1271] [G loss: 2.3526]\n",
      "[Epoch 8/500] [Batch 295/312] [D IS TRAINING] [D loss: 0.1857] [G loss: 1.8737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/500] [Batch 296/312] [D IS TRAINING] [D loss: 0.2620] [G loss: 3.8631]\n",
      "[Epoch 8/500] [Batch 297/312] [D IS TRAINING] [D loss: 0.2952] [G loss: 1.2368]\n",
      "[Epoch 8/500] [Batch 298/312] [D IS TRAINING] [D loss: 0.2898] [G loss: 3.2120]\n",
      "[Epoch 8/500] [Batch 299/312] [D IS TRAINING] [D loss: 0.1386] [G loss: 2.3592]\n",
      "[Epoch 8/500] [Batch 300/312] [D IS TRAINING] [D loss: 0.3797] [G loss: 0.9777]\n",
      "[Epoch 8/500] [Batch 301/312] [D IS TRAINING] [D loss: 0.7253] [G loss: 5.2947]\n",
      "[Epoch 8/500] [Batch 302/312] [D IS TRAINING] [D loss: 0.5311] [G loss: 0.9752]\n",
      "[Epoch 8/500] [Batch 303/312] [D IS TRAINING] [D loss: 0.4397] [G loss: 4.2374]\n",
      "[Epoch 8/500] [Batch 304/312] [D IS TRAINING] [D loss: 0.2000] [G loss: 2.2592]\n",
      "[Epoch 8/500] [Batch 305/312] [D IS TRAINING] [D loss: 0.2532] [G loss: 1.4098]\n",
      "[Epoch 8/500] [Batch 306/312] [D IS TRAINING] [D loss: 0.2770] [G loss: 2.9947]\n",
      "[Epoch 8/500] [Batch 307/312] [D IS TRAINING] [D loss: 0.2287] [G loss: 1.8186]\n",
      "[Epoch 8/500] [Batch 308/312] [D IS TRAINING] [D loss: 0.2193] [G loss: 2.2379]\n",
      "[Epoch 8/500] [Batch 309/312] [D IS TRAINING] [D loss: 0.2665] [G loss: 2.6293]\n",
      "[Epoch 8/500] [Batch 310/312] [D IS TRAINING] [D loss: 0.2305] [G loss: 1.4362]\n",
      "[Epoch 8/500] [Batch 311/312] [D IS TRAINING] [D loss: 0.1869] [G loss: 2.8585]\n",
      "[Epoch 9/500] [Batch 0/312] [D IS TRAINING] [D loss: 0.1423] [G loss: 2.4758]\n",
      "[Epoch 9/500] [Batch 1/312] [D IS TRAINING] [D loss: 0.1740] [G loss: 1.8310]\n",
      "[Epoch 9/500] [Batch 2/312] [D IS TRAINING] [D loss: 0.1422] [G loss: 2.6436]\n",
      "[Epoch 9/500] [Batch 3/312] [D IS TRAINING] [D loss: 0.1852] [G loss: 2.7502]\n",
      "[Epoch 9/500] [Batch 4/312] [D IS TRAINING] [D loss: 0.1604] [G loss: 1.7784]\n",
      "[Epoch 9/500] [Batch 5/312] [D IS TRAINING] [D loss: 0.1663] [G loss: 2.3188]\n",
      "[Epoch 9/500] [Batch 6/312] [D IS TRAINING] [D loss: 0.1643] [G loss: 2.3184]\n",
      "[Epoch 9/500] [Batch 7/312] [D IS TRAINING] [D loss: 0.1576] [G loss: 2.0460]\n",
      "[Epoch 9/500] [Batch 8/312] [D IS TRAINING] [D loss: 0.1469] [G loss: 2.2902]\n",
      "[Epoch 9/500] [Batch 9/312] [D IS TRAINING] [D loss: 0.1629] [G loss: 2.9367]\n",
      "[Epoch 9/500] [Batch 10/312] [D IS TRAINING] [D loss: 0.1821] [G loss: 1.8923]\n",
      "[Epoch 9/500] [Batch 11/312] [D IS TRAINING] [D loss: 0.2345] [G loss: 1.6283]\n",
      "[Epoch 9/500] [Batch 12/312] [D IS TRAINING] [D loss: 0.3346] [G loss: 3.2320]\n",
      "[Epoch 9/500] [Batch 13/312] [D IS TRAINING] [D loss: 0.3685] [G loss: 0.9454]\n",
      "[Epoch 9/500] [Batch 14/312] [D IS TRAINING] [D loss: 0.3286] [G loss: 3.9685]\n",
      "[Epoch 9/500] [Batch 15/312] [D IS TRAINING] [D loss: 0.2179] [G loss: 1.6968]\n",
      "[Epoch 9/500] [Batch 16/312] [D IS TRAINING] [D loss: 0.1740] [G loss: 2.0937]\n",
      "[Epoch 9/500] [Batch 17/312] [D IS TRAINING] [D loss: 0.1375] [G loss: 3.1257]\n",
      "[Epoch 9/500] [Batch 18/312] [D IS TRAINING] [D loss: 0.1536] [G loss: 2.0210]\n",
      "[Epoch 9/500] [Batch 19/312] [D IS TRAINING] [D loss: 0.2053] [G loss: 2.0636]\n",
      "[Epoch 9/500] [Batch 20/312] [D IS TRAINING] [D loss: 0.1667] [G loss: 2.1564]\n",
      "[Epoch 9/500] [Batch 21/312] [D IS TRAINING] [D loss: 0.1868] [G loss: 2.1742]\n",
      "[Epoch 9/500] [Batch 22/312] [D IS TRAINING] [D loss: 0.1699] [G loss: 2.3134]\n",
      "[Epoch 9/500] [Batch 23/312] [D IS TRAINING] [D loss: 0.2102] [G loss: 1.9032]\n",
      "[Epoch 9/500] [Batch 24/312] [D IS TRAINING] [D loss: 0.2079] [G loss: 2.4242]\n",
      "[Epoch 9/500] [Batch 25/312] [D IS TRAINING] [D loss: 0.1826] [G loss: 1.9738]\n",
      "[Epoch 9/500] [Batch 26/312] [D IS TRAINING] [D loss: 0.1318] [G loss: 2.7300]\n",
      "[Epoch 9/500] [Batch 27/312] [D IS TRAINING] [D loss: 0.1364] [G loss: 2.2543]\n",
      "[Epoch 9/500] [Batch 28/312] [D IS TRAINING] [D loss: 0.1839] [G loss: 2.2413]\n",
      "[Epoch 9/500] [Batch 29/312] [D IS TRAINING] [D loss: 0.1490] [G loss: 2.2549]\n",
      "[Epoch 9/500] [Batch 30/312] [D IS TRAINING] [D loss: 0.1875] [G loss: 2.4293]\n",
      "[Epoch 9/500] [Batch 31/312] [D IS TRAINING] [D loss: 0.1709] [G loss: 2.0819]\n",
      "[Epoch 9/500] [Batch 32/312] [D IS TRAINING] [D loss: 0.1476] [G loss: 2.9123]\n",
      "[Epoch 9/500] [Batch 33/312] [D IS TRAINING] [D loss: 0.1356] [G loss: 2.7015]\n",
      "[Epoch 9/500] [Batch 34/312] [D IS TRAINING] [D loss: 0.2335] [G loss: 1.4046]\n",
      "[Epoch 9/500] [Batch 35/312] [D IS TRAINING] [D loss: 0.2360] [G loss: 3.8049]\n",
      "[Epoch 9/500] [Batch 36/312] [D IS TRAINING] [D loss: 0.3148] [G loss: 1.6447]\n",
      "[Epoch 9/500] [Batch 37/312] [D IS TRAINING] [D loss: 0.3058] [G loss: 2.7920]\n",
      "[Epoch 9/500] [Batch 38/312] [D IS TRAINING] [D loss: 0.2113] [G loss: 1.8612]\n",
      "[Epoch 9/500] [Batch 39/312] [D IS TRAINING] [D loss: 0.2057] [G loss: 2.3696]\n",
      "[Epoch 9/500] [Batch 40/312] [D IS TRAINING] [D loss: 0.1661] [G loss: 2.3013]\n",
      "[Epoch 9/500] [Batch 41/312] [D IS TRAINING] [D loss: 0.2014] [G loss: 2.2139]\n",
      "[Epoch 9/500] [Batch 42/312] [D IS TRAINING] [D loss: 0.1989] [G loss: 2.2250]\n",
      "[Epoch 9/500] [Batch 43/312] [D IS TRAINING] [D loss: 0.1552] [G loss: 2.0614]\n",
      "[Epoch 9/500] [Batch 44/312] [D IS TRAINING] [D loss: 0.2093] [G loss: 2.5366]\n",
      "[Epoch 9/500] [Batch 45/312] [D IS TRAINING] [D loss: 0.2937] [G loss: 1.3293]\n",
      "[Epoch 9/500] [Batch 46/312] [D IS TRAINING] [D loss: 0.2835] [G loss: 3.2030]\n",
      "[Epoch 9/500] [Batch 47/312] [D IS TRAINING] [D loss: 0.2738] [G loss: 1.2463]\n",
      "[Epoch 9/500] [Batch 48/312] [D IS TRAINING] [D loss: 0.2743] [G loss: 3.7433]\n",
      "[Epoch 9/500] [Batch 49/312] [D IS TRAINING] [D loss: 0.2029] [G loss: 1.7338]\n",
      "[Epoch 9/500] [Batch 50/312] [D IS TRAINING] [D loss: 0.1802] [G loss: 2.3646]\n",
      "[Epoch 9/500] [Batch 51/312] [D IS TRAINING] [D loss: 0.1900] [G loss: 2.0510]\n",
      "[Epoch 9/500] [Batch 52/312] [D IS TRAINING] [D loss: 0.1738] [G loss: 2.5727]\n",
      "[Epoch 9/500] [Batch 53/312] [D IS TRAINING] [D loss: 0.1939] [G loss: 2.1871]\n",
      "[Epoch 9/500] [Batch 54/312] [D IS TRAINING] [D loss: 0.2171] [G loss: 1.5639]\n",
      "[Epoch 9/500] [Batch 55/312] [D IS TRAINING] [D loss: 0.2351] [G loss: 3.6962]\n",
      "[Epoch 9/500] [Batch 56/312] [D IS TRAINING] [D loss: 0.2107] [G loss: 1.6362]\n",
      "[Epoch 9/500] [Batch 57/312] [D IS TRAINING] [D loss: 0.1395] [G loss: 2.5972]\n",
      "[Epoch 9/500] [Batch 58/312] [D IS TRAINING] [D loss: 0.1631] [G loss: 2.8378]\n",
      "[Epoch 9/500] [Batch 59/312] [D IS TRAINING] [D loss: 0.2067] [G loss: 1.5346]\n",
      "[Epoch 9/500] [Batch 60/312] [D IS TRAINING] [D loss: 0.3085] [G loss: 3.0507]\n",
      "[Epoch 9/500] [Batch 61/312] [D IS TRAINING] [D loss: 0.3734] [G loss: 0.9368]\n",
      "[Epoch 9/500] [Batch 62/312] [D IS TRAINING] [D loss: 0.3793] [G loss: 4.2724]\n",
      "[Epoch 9/500] [Batch 63/312] [D IS TRAINING] [D loss: 0.3624] [G loss: 0.9909]\n",
      "[Epoch 9/500] [Batch 64/312] [D IS TRAINING] [D loss: 0.4120] [G loss: 4.3824]\n",
      "[Epoch 9/500] [Batch 65/312] [D IS TRAINING] [D loss: 0.3207] [G loss: 1.0962]\n",
      "[Epoch 9/500] [Batch 66/312] [D IS TRAINING] [D loss: 0.2838] [G loss: 3.6725]\n",
      "[Epoch 9/500] [Batch 67/312] [D IS TRAINING] [D loss: 0.1913] [G loss: 1.9040]\n",
      "[Epoch 9/500] [Batch 68/312] [D IS TRAINING] [D loss: 0.1557] [G loss: 2.3217]\n",
      "[Epoch 9/500] [Batch 69/312] [D IS TRAINING] [D loss: 0.1888] [G loss: 2.0323]\n",
      "[Epoch 9/500] [Batch 70/312] [D IS TRAINING] [D loss: 0.1512] [G loss: 2.4182]\n",
      "[Epoch 9/500] [Batch 71/312] [D IS TRAINING] [D loss: 0.1700] [G loss: 2.2459]\n",
      "[Epoch 9/500] [Batch 72/312] [D IS TRAINING] [D loss: 0.2579] [G loss: 2.2457]\n",
      "[Epoch 9/500] [Batch 73/312] [D IS TRAINING] [D loss: 0.4409] [G loss: 0.9081]\n",
      "[Epoch 9/500] [Batch 74/312] [D IS TRAINING] [D loss: 0.7760] [G loss: 5.4890]\n",
      "[Epoch 9/500] [Batch 75/312] [D IS TRAINING] [D loss: 0.6056] [G loss: 0.5337]\n",
      "[Epoch 9/500] [Batch 76/312] [D IS TRAINING] [D loss: 0.6538] [G loss: 4.9695]\n",
      "[Epoch 9/500] [Batch 77/312] [D IS TRAINING] [D loss: 0.6206] [G loss: 0.5923]\n",
      "[Epoch 9/500] [Batch 78/312] [D IS TRAINING] [D loss: 0.3109] [G loss: 3.9044]\n",
      "[Epoch 9/500] [Batch 79/312] [D IS TRAINING] [D loss: 0.2930] [G loss: 1.7631]\n",
      "[Epoch 9/500] [Batch 80/312] [D IS TRAINING] [D loss: 0.2532] [G loss: 1.4687]\n",
      "[Epoch 9/500] [Batch 81/312] [D IS TRAINING] [D loss: 0.2315] [G loss: 2.6030]\n",
      "[Epoch 9/500] [Batch 82/312] [D IS TRAINING] [D loss: 0.2606] [G loss: 1.7220]\n",
      "[Epoch 9/500] [Batch 83/312] [D IS TRAINING] [D loss: 0.2428] [G loss: 2.6180]\n",
      "[Epoch 9/500] [Batch 84/312] [D IS TRAINING] [D loss: 0.3092] [G loss: 1.2372]\n",
      "[Epoch 9/500] [Batch 85/312] [D IS TRAINING] [D loss: 0.2794] [G loss: 3.3288]\n",
      "[Epoch 9/500] [Batch 86/312] [D IS TRAINING] [D loss: 0.1966] [G loss: 1.7641]\n",
      "[Epoch 9/500] [Batch 87/312] [D IS TRAINING] [D loss: 0.1552] [G loss: 2.5426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/500] [Batch 88/312] [D IS TRAINING] [D loss: 0.1615] [G loss: 2.3056]\n",
      "[Epoch 9/500] [Batch 89/312] [D IS TRAINING] [D loss: 0.2106] [G loss: 2.0528]\n",
      "[Epoch 9/500] [Batch 90/312] [D IS TRAINING] [D loss: 0.2411] [G loss: 1.8967]\n",
      "[Epoch 9/500] [Batch 91/312] [D IS TRAINING] [D loss: 0.2134] [G loss: 1.7347]\n",
      "[Epoch 9/500] [Batch 92/312] [D IS TRAINING] [D loss: 0.2120] [G loss: 3.0371]\n",
      "[Epoch 9/500] [Batch 93/312] [D IS TRAINING] [D loss: 0.2726] [G loss: 1.4170]\n",
      "[Epoch 9/500] [Batch 94/312] [D IS TRAINING] [D loss: 0.2154] [G loss: 2.5508]\n",
      "[Epoch 9/500] [Batch 95/312] [D IS TRAINING] [D loss: 0.2441] [G loss: 1.7346]\n",
      "[Epoch 9/500] [Batch 96/312] [D IS TRAINING] [D loss: 0.1766] [G loss: 2.0894]\n",
      "[Epoch 9/500] [Batch 97/312] [D IS TRAINING] [D loss: 0.1433] [G loss: 2.6731]\n",
      "[Epoch 9/500] [Batch 98/312] [D IS TRAINING] [D loss: 0.1626] [G loss: 2.1985]\n",
      "[Epoch 9/500] [Batch 99/312] [D IS TRAINING] [D loss: 0.1713] [G loss: 2.4086]\n",
      "[Epoch 9/500] [Batch 100/312] [D IS TRAINING] [D loss: 0.1945] [G loss: 2.1616]\n",
      "[Epoch 9/500] [Batch 101/312] [D IS TRAINING] [D loss: 0.1466] [G loss: 2.2262]\n",
      "[Epoch 9/500] [Batch 102/312] [D IS TRAINING] [D loss: 0.1975] [G loss: 2.2890]\n",
      "[Epoch 9/500] [Batch 103/312] [D IS TRAINING] [D loss: 0.1802] [G loss: 2.5577]\n",
      "[Epoch 9/500] [Batch 104/312] [D IS TRAINING] [D loss: 0.2032] [G loss: 1.8179]\n",
      "[Epoch 9/500] [Batch 105/312] [D IS TRAINING] [D loss: 0.1788] [G loss: 2.0952]\n",
      "[Epoch 9/500] [Batch 106/312] [D IS TRAINING] [D loss: 0.1733] [G loss: 2.7994]\n",
      "[Epoch 9/500] [Batch 107/312] [D IS TRAINING] [D loss: 0.1993] [G loss: 1.9847]\n",
      "[Epoch 9/500] [Batch 108/312] [D IS TRAINING] [D loss: 0.2142] [G loss: 1.6363]\n",
      "[Epoch 9/500] [Batch 109/312] [D IS TRAINING] [D loss: 0.3460] [G loss: 3.5908]\n",
      "[Epoch 9/500] [Batch 110/312] [D IS TRAINING] [D loss: 0.3178] [G loss: 1.0370]\n",
      "[Epoch 9/500] [Batch 111/312] [D IS TRAINING] [D loss: 0.2655] [G loss: 3.2563]\n",
      "[Epoch 9/500] [Batch 112/312] [D IS TRAINING] [D loss: 0.1693] [G loss: 2.0448]\n",
      "[Epoch 9/500] [Batch 113/312] [D IS TRAINING] [D loss: 0.2030] [G loss: 1.6888]\n",
      "[Epoch 9/500] [Batch 114/312] [D IS TRAINING] [D loss: 0.3172] [G loss: 3.2103]\n",
      "[Epoch 9/500] [Batch 115/312] [D IS TRAINING] [D loss: 0.3430] [G loss: 1.1648]\n",
      "[Epoch 9/500] [Batch 116/312] [D IS TRAINING] [D loss: 0.2877] [G loss: 3.3548]\n",
      "[Epoch 9/500] [Batch 117/312] [D IS TRAINING] [D loss: 0.2437] [G loss: 1.7930]\n",
      "[Epoch 9/500] [Batch 118/312] [D IS TRAINING] [D loss: 0.1319] [G loss: 3.0415]\n",
      "[Epoch 9/500] [Batch 119/312] [D IS TRAINING] [D loss: 0.1985] [G loss: 1.9216]\n",
      "[Epoch 9/500] [Batch 120/312] [D IS TRAINING] [D loss: 0.1426] [G loss: 3.1386]\n",
      "[Epoch 9/500] [Batch 121/312] [D IS TRAINING] [D loss: 0.2199] [G loss: 1.7836]\n",
      "[Epoch 9/500] [Batch 122/312] [D IS TRAINING] [D loss: 0.3013] [G loss: 2.4592]\n",
      "[Epoch 9/500] [Batch 123/312] [D IS TRAINING] [D loss: 0.2975] [G loss: 1.5197]\n",
      "[Epoch 9/500] [Batch 124/312] [D IS TRAINING] [D loss: 0.3413] [G loss: 3.1249]\n",
      "[Epoch 9/500] [Batch 125/312] [D IS TRAINING] [D loss: 0.2339] [G loss: 1.4889]\n",
      "[Epoch 9/500] [Batch 126/312] [D IS TRAINING] [D loss: 0.2196] [G loss: 2.9652]\n",
      "[Epoch 9/500] [Batch 127/312] [D IS TRAINING] [D loss: 0.1763] [G loss: 2.1646]\n",
      "[Epoch 9/500] [Batch 128/312] [D IS TRAINING] [D loss: 0.1713] [G loss: 2.0620]\n",
      "[Epoch 9/500] [Batch 129/312] [D IS TRAINING] [D loss: 0.1603] [G loss: 2.9581]\n",
      "[Epoch 9/500] [Batch 130/312] [D IS TRAINING] [D loss: 0.2312] [G loss: 1.6231]\n",
      "[Epoch 9/500] [Batch 131/312] [D IS TRAINING] [D loss: 0.2464] [G loss: 2.3610]\n",
      "[Epoch 9/500] [Batch 132/312] [D IS TRAINING] [D loss: 0.3083] [G loss: 1.6413]\n",
      "[Epoch 9/500] [Batch 133/312] [D IS TRAINING] [D loss: 0.2137] [G loss: 2.8090]\n",
      "[Epoch 9/500] [Batch 134/312] [D IS TRAINING] [D loss: 0.2144] [G loss: 2.0797]\n",
      "[Epoch 9/500] [Batch 135/312] [D IS TRAINING] [D loss: 0.1789] [G loss: 2.6561]\n",
      "[Epoch 9/500] [Batch 136/312] [D IS TRAINING] [D loss: 0.1646] [G loss: 1.8051]\n",
      "[Epoch 9/500] [Batch 137/312] [D IS TRAINING] [D loss: 0.2190] [G loss: 3.1315]\n",
      "[Epoch 9/500] [Batch 138/312] [D IS TRAINING] [D loss: 0.2909] [G loss: 1.1319]\n",
      "[Epoch 9/500] [Batch 139/312] [D IS TRAINING] [D loss: 0.4786] [G loss: 4.3134]\n",
      "[Epoch 9/500] [Batch 140/312] [D IS TRAINING] [D loss: 0.6525] [G loss: 0.7950]\n",
      "[Epoch 9/500] [Batch 141/312] [D IS TRAINING] [D loss: 0.5871] [G loss: 4.3487]\n",
      "[Epoch 9/500] [Batch 142/312] [D IS TRAINING] [D loss: 0.3203] [G loss: 1.1162]\n",
      "[Epoch 9/500] [Batch 143/312] [D IS TRAINING] [D loss: 0.2393] [G loss: 3.5547]\n",
      "[Epoch 9/500] [Batch 144/312] [D IS TRAINING] [D loss: 0.1575] [G loss: 2.5568]\n",
      "[Epoch 9/500] [Batch 145/312] [D IS TRAINING] [D loss: 0.2277] [G loss: 1.7018]\n",
      "[Epoch 9/500] [Batch 146/312] [D IS TRAINING] [D loss: 0.3356] [G loss: 2.7465]\n",
      "[Epoch 9/500] [Batch 147/312] [D IS TRAINING] [D loss: 0.2712] [G loss: 1.6012]\n",
      "[Epoch 9/500] [Batch 148/312] [D IS TRAINING] [D loss: 0.2485] [G loss: 3.4314]\n",
      "[Epoch 9/500] [Batch 149/312] [D IS TRAINING] [D loss: 0.2211] [G loss: 1.7948]\n",
      "[Epoch 9/500] [Batch 150/312] [D IS TRAINING] [D loss: 0.1869] [G loss: 1.9388]\n",
      "[Epoch 9/500] [Batch 151/312] [D IS TRAINING] [D loss: 0.1751] [G loss: 2.9224]\n",
      "[Epoch 9/500] [Batch 152/312] [D IS TRAINING] [D loss: 0.1799] [G loss: 2.0393]\n",
      "[Epoch 9/500] [Batch 153/312] [D IS TRAINING] [D loss: 0.1761] [G loss: 1.9110]\n",
      "[Epoch 9/500] [Batch 154/312] [D IS TRAINING] [D loss: 0.2041] [G loss: 2.4137]\n",
      "[Epoch 9/500] [Batch 155/312] [D IS TRAINING] [D loss: 0.1809] [G loss: 1.8607]\n",
      "[Epoch 9/500] [Batch 156/312] [D IS TRAINING] [D loss: 0.1883] [G loss: 2.2191]\n",
      "[Epoch 9/500] [Batch 157/312] [D IS TRAINING] [D loss: 0.1629] [G loss: 2.2866]\n",
      "[Epoch 9/500] [Batch 158/312] [D IS TRAINING] [D loss: 0.1930] [G loss: 2.3137]\n",
      "[Epoch 9/500] [Batch 159/312] [D IS TRAINING] [D loss: 0.2157] [G loss: 1.5621]\n",
      "[Epoch 9/500] [Batch 160/312] [D IS TRAINING] [D loss: 0.2085] [G loss: 2.7223]\n",
      "[Epoch 9/500] [Batch 161/312] [D IS TRAINING] [D loss: 0.1763] [G loss: 1.9634]\n",
      "[Epoch 9/500] [Batch 162/312] [D IS TRAINING] [D loss: 0.1920] [G loss: 1.9000]\n",
      "[Epoch 9/500] [Batch 163/312] [D IS TRAINING] [D loss: 0.1775] [G loss: 2.9440]\n",
      "[Epoch 9/500] [Batch 164/312] [D IS TRAINING] [D loss: 0.1454] [G loss: 2.4816]\n",
      "[Epoch 9/500] [Batch 165/312] [D IS TRAINING] [D loss: 0.1835] [G loss: 1.6983]\n",
      "[Epoch 9/500] [Batch 166/312] [D IS TRAINING] [D loss: 0.2454] [G loss: 2.8064]\n",
      "[Epoch 9/500] [Batch 167/312] [D IS TRAINING] [D loss: 0.1895] [G loss: 1.7191]\n",
      "[Epoch 9/500] [Batch 168/312] [D IS TRAINING] [D loss: 0.1848] [G loss: 2.3830]\n",
      "[Epoch 9/500] [Batch 169/312] [D IS TRAINING] [D loss: 0.1980] [G loss: 1.9028]\n",
      "[Epoch 9/500] [Batch 170/312] [D IS TRAINING] [D loss: 0.1500] [G loss: 2.5070]\n",
      "[Epoch 9/500] [Batch 171/312] [D IS TRAINING] [D loss: 0.1383] [G loss: 2.4712]\n",
      "[Epoch 9/500] [Batch 172/312] [D IS TRAINING] [D loss: 0.1775] [G loss: 1.9149]\n",
      "[Epoch 9/500] [Batch 173/312] [D IS TRAINING] [D loss: 0.1918] [G loss: 2.2920]\n",
      "[Epoch 9/500] [Batch 174/312] [D IS TRAINING] [D loss: 0.1960] [G loss: 2.0168]\n",
      "[Epoch 9/500] [Batch 175/312] [D IS TRAINING] [D loss: 0.1952] [G loss: 1.8709]\n",
      "[Epoch 9/500] [Batch 176/312] [D IS TRAINING] [D loss: 0.1612] [G loss: 2.5757]\n",
      "[Epoch 9/500] [Batch 177/312] [D IS TRAINING] [D loss: 0.1995] [G loss: 2.4738]\n",
      "[Epoch 9/500] [Batch 178/312] [D IS TRAINING] [D loss: 0.2723] [G loss: 1.3211]\n",
      "[Epoch 9/500] [Batch 179/312] [D IS TRAINING] [D loss: 0.2590] [G loss: 2.7767]\n",
      "[Epoch 9/500] [Batch 180/312] [D IS TRAINING] [D loss: 0.1965] [G loss: 1.6733]\n",
      "[Epoch 9/500] [Batch 181/312] [D IS TRAINING] [D loss: 0.1885] [G loss: 2.1814]\n",
      "[Epoch 9/500] [Batch 182/312] [D IS TRAINING] [D loss: 0.1935] [G loss: 2.5975]\n",
      "[Epoch 9/500] [Batch 183/312] [D IS TRAINING] [D loss: 0.1902] [G loss: 1.6601]\n",
      "[Epoch 9/500] [Batch 184/312] [D IS TRAINING] [D loss: 0.1939] [G loss: 2.8278]\n",
      "[Epoch 9/500] [Batch 185/312] [D IS TRAINING] [D loss: 0.2049] [G loss: 2.3250]\n",
      "[Epoch 9/500] [Batch 186/312] [D IS TRAINING] [D loss: 0.2772] [G loss: 1.9040]\n",
      "[Epoch 9/500] [Batch 187/312] [D IS TRAINING] [D loss: 0.2841] [G loss: 3.1835]\n",
      "[Epoch 9/500] [Batch 188/312] [D IS TRAINING] [D loss: 0.2215] [G loss: 1.5736]\n",
      "[Epoch 9/500] [Batch 189/312] [D IS TRAINING] [D loss: 0.2538] [G loss: 3.0391]\n",
      "[Epoch 9/500] [Batch 190/312] [D IS TRAINING] [D loss: 0.2666] [G loss: 1.3898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/500] [Batch 191/312] [D IS TRAINING] [D loss: 0.2110] [G loss: 3.4886]\n",
      "[Epoch 9/500] [Batch 192/312] [D IS TRAINING] [D loss: 0.2172] [G loss: 1.9202]\n",
      "[Epoch 9/500] [Batch 193/312] [D IS TRAINING] [D loss: 0.1781] [G loss: 3.0282]\n",
      "[Epoch 9/500] [Batch 194/312] [D IS TRAINING] [D loss: 0.1489] [G loss: 2.5173]\n",
      "[Epoch 9/500] [Batch 195/312] [D IS TRAINING] [D loss: 0.2186] [G loss: 1.8862]\n",
      "[Epoch 9/500] [Batch 196/312] [D IS TRAINING] [D loss: 0.1712] [G loss: 2.8587]\n",
      "[Epoch 9/500] [Batch 197/312] [D IS TRAINING] [D loss: 0.1596] [G loss: 2.1004]\n",
      "[Epoch 9/500] [Batch 198/312] [D IS TRAINING] [D loss: 0.2314] [G loss: 1.8785]\n",
      "[Epoch 9/500] [Batch 199/312] [D IS TRAINING] [D loss: 0.1991] [G loss: 2.4875]\n",
      "[Epoch 9/500] [Batch 200/312] [D IS TRAINING] [D loss: 0.1916] [G loss: 1.6084]\n",
      "[Epoch 9/500] [Batch 201/312] [D IS TRAINING] [D loss: 0.2678] [G loss: 3.3821]\n",
      "[Epoch 9/500] [Batch 202/312] [D IS TRAINING] [D loss: 0.3390] [G loss: 1.0200]\n",
      "[Epoch 9/500] [Batch 203/312] [D IS TRAINING] [D loss: 0.4194] [G loss: 4.4914]\n",
      "[Epoch 9/500] [Batch 204/312] [D IS TRAINING] [D loss: 0.2506] [G loss: 1.2553]\n",
      "[Epoch 9/500] [Batch 205/312] [D IS TRAINING] [D loss: 0.2434] [G loss: 3.4774]\n",
      "[Epoch 9/500] [Batch 206/312] [D IS TRAINING] [D loss: 0.2052] [G loss: 1.8680]\n",
      "[Epoch 9/500] [Batch 207/312] [D IS TRAINING] [D loss: 0.2632] [G loss: 1.3667]\n",
      "[Epoch 9/500] [Batch 208/312] [D IS TRAINING] [D loss: 0.4794] [G loss: 4.1907]\n",
      "[Epoch 9/500] [Batch 209/312] [D IS TRAINING] [D loss: 0.7015] [G loss: 0.4364]\n",
      "[Epoch 9/500] [Batch 210/312] [D IS TRAINING] [D loss: 0.6282] [G loss: 3.8985]\n",
      "[Epoch 9/500] [Batch 211/312] [D IS TRAINING] [D loss: 1.2674] [G loss: 0.1558]\n",
      "[Epoch 9/500] [Batch 212/312] [D IS TRAINING] [D loss: 1.5118] [G loss: 6.7317]\n",
      "[Epoch 9/500] [Batch 213/312] [D IS TRAINING] [D loss: 0.3124] [G loss: 1.8340]\n",
      "[Epoch 9/500] [Batch 214/312] [D IS TRAINING] [D loss: 0.3616] [G loss: 1.3355]\n",
      "[Epoch 9/500] [Batch 215/312] [D IS TRAINING] [D loss: 0.4122] [G loss: 3.4992]\n",
      "[Epoch 9/500] [Batch 216/312] [D IS TRAINING] [D loss: 0.3897] [G loss: 1.4481]\n",
      "[Epoch 9/500] [Batch 217/312] [D IS TRAINING] [D loss: 0.4534] [G loss: 3.3938]\n",
      "[Epoch 9/500] [Batch 218/312] [D IS TRAINING] [D loss: 0.3636] [G loss: 1.1569]\n",
      "[Epoch 9/500] [Batch 219/312] [D IS TRAINING] [D loss: 0.2411] [G loss: 2.0343]\n",
      "[Epoch 9/500] [Batch 220/312] [D IS TRAINING] [D loss: 0.2353] [G loss: 3.0051]\n",
      "[Epoch 9/500] [Batch 221/312] [D IS TRAINING] [D loss: 0.2400] [G loss: 1.7135]\n",
      "[Epoch 9/500] [Batch 222/312] [D IS TRAINING] [D loss: 0.2459] [G loss: 1.7871]\n",
      "[Epoch 9/500] [Batch 223/312] [D IS TRAINING] [D loss: 0.2038] [G loss: 1.9783]\n",
      "[Epoch 9/500] [Batch 224/312] [D IS TRAINING] [D loss: 0.2353] [G loss: 2.8534]\n",
      "[Epoch 9/500] [Batch 225/312] [D IS TRAINING] [D loss: 0.2571] [G loss: 1.3388]\n",
      "[Epoch 9/500] [Batch 226/312] [D IS TRAINING] [D loss: 0.1607] [G loss: 2.7162]\n",
      "[Epoch 9/500] [Batch 227/312] [D IS TRAINING] [D loss: 0.1978] [G loss: 2.5224]\n",
      "[Epoch 9/500] [Batch 228/312] [D IS TRAINING] [D loss: 0.1929] [G loss: 1.6276]\n",
      "[Epoch 9/500] [Batch 229/312] [D IS TRAINING] [D loss: 0.2092] [G loss: 2.1398]\n",
      "[Epoch 9/500] [Batch 230/312] [D IS TRAINING] [D loss: 0.1451] [G loss: 2.1401]\n",
      "[Epoch 9/500] [Batch 231/312] [D IS TRAINING] [D loss: 0.1819] [G loss: 2.8616]\n",
      "[Epoch 9/500] [Batch 232/312] [D IS TRAINING] [D loss: 0.1637] [G loss: 1.8480]\n",
      "[Epoch 9/500] [Batch 233/312] [D IS TRAINING] [D loss: 0.1785] [G loss: 2.1892]\n",
      "[Epoch 9/500] [Batch 234/312] [D IS TRAINING] [D loss: 0.1683] [G loss: 2.1103]\n",
      "[Epoch 9/500] [Batch 235/312] [D IS TRAINING] [D loss: 0.2036] [G loss: 2.3370]\n",
      "[Epoch 9/500] [Batch 236/312] [D IS TRAINING] [D loss: 0.1927] [G loss: 1.6049]\n",
      "[Epoch 9/500] [Batch 237/312] [D IS TRAINING] [D loss: 0.2266] [G loss: 2.6729]\n",
      "[Epoch 9/500] [Batch 238/312] [D IS TRAINING] [D loss: 0.2200] [G loss: 1.5684]\n",
      "[Epoch 9/500] [Batch 239/312] [D IS TRAINING] [D loss: 0.1714] [G loss: 2.3674]\n",
      "[Epoch 9/500] [Batch 240/312] [D IS TRAINING] [D loss: 0.2123] [G loss: 2.6423]\n",
      "[Epoch 9/500] [Batch 241/312] [D IS TRAINING] [D loss: 0.3187] [G loss: 1.0401]\n",
      "[Epoch 9/500] [Batch 242/312] [D IS TRAINING] [D loss: 0.3632] [G loss: 3.6617]\n",
      "[Epoch 9/500] [Batch 243/312] [D IS TRAINING] [D loss: 0.2682] [G loss: 1.4655]\n",
      "[Epoch 9/500] [Batch 244/312] [D IS TRAINING] [D loss: 0.2186] [G loss: 3.1213]\n",
      "[Epoch 9/500] [Batch 245/312] [D IS TRAINING] [D loss: 0.1703] [G loss: 1.8576]\n",
      "[Epoch 9/500] [Batch 246/312] [D IS TRAINING] [D loss: 0.2048] [G loss: 2.0915]\n",
      "[Epoch 9/500] [Batch 247/312] [D IS TRAINING] [D loss: 0.2613] [G loss: 2.3007]\n",
      "[Epoch 9/500] [Batch 248/312] [D IS TRAINING] [D loss: 0.1767] [G loss: 2.0995]\n",
      "[Epoch 9/500] [Batch 249/312] [D IS TRAINING] [D loss: 0.1752] [G loss: 2.1659]\n",
      "[Epoch 9/500] [Batch 250/312] [D IS TRAINING] [D loss: 0.1833] [G loss: 2.8971]\n",
      "[Epoch 9/500] [Batch 251/312] [D IS TRAINING] [D loss: 0.2150] [G loss: 1.4953]\n",
      "[Epoch 9/500] [Batch 252/312] [D IS TRAINING] [D loss: 0.2224] [G loss: 2.7409]\n",
      "[Epoch 9/500] [Batch 253/312] [D IS TRAINING] [D loss: 0.2028] [G loss: 2.0125]\n",
      "[Epoch 9/500] [Batch 254/312] [D IS TRAINING] [D loss: 0.1832] [G loss: 1.9831]\n",
      "[Epoch 9/500] [Batch 255/312] [D IS TRAINING] [D loss: 0.1793] [G loss: 3.0271]\n",
      "[Epoch 9/500] [Batch 256/312] [D IS TRAINING] [D loss: 0.1887] [G loss: 1.7400]\n",
      "[Epoch 9/500] [Batch 257/312] [D IS TRAINING] [D loss: 0.1630] [G loss: 2.3361]\n",
      "[Epoch 9/500] [Batch 258/312] [D IS TRAINING] [D loss: 0.1652] [G loss: 2.5239]\n",
      "[Epoch 9/500] [Batch 259/312] [D IS TRAINING] [D loss: 0.1957] [G loss: 1.7021]\n",
      "[Epoch 9/500] [Batch 260/312] [D IS TRAINING] [D loss: 0.1610] [G loss: 2.4217]\n",
      "[Epoch 9/500] [Batch 261/312] [D IS TRAINING] [D loss: 0.2175] [G loss: 2.5704]\n",
      "[Epoch 9/500] [Batch 262/312] [D IS TRAINING] [D loss: 0.1975] [G loss: 1.5456]\n",
      "[Epoch 9/500] [Batch 263/312] [D IS TRAINING] [D loss: 0.1872] [G loss: 2.6442]\n",
      "[Epoch 9/500] [Batch 264/312] [D IS TRAINING] [D loss: 0.1850] [G loss: 2.0089]\n",
      "[Epoch 9/500] [Batch 265/312] [D IS TRAINING] [D loss: 0.1442] [G loss: 2.4888]\n",
      "[Epoch 9/500] [Batch 266/312] [D IS TRAINING] [D loss: 0.1985] [G loss: 1.9763]\n",
      "[Epoch 9/500] [Batch 267/312] [D IS TRAINING] [D loss: 0.1942] [G loss: 2.0453]\n",
      "[Epoch 9/500] [Batch 268/312] [D IS TRAINING] [D loss: 0.1592] [G loss: 1.9576]\n",
      "[Epoch 9/500] [Batch 269/312] [D IS TRAINING] [D loss: 0.1634] [G loss: 2.7861]\n",
      "[Epoch 9/500] [Batch 270/312] [D IS TRAINING] [D loss: 0.1749] [G loss: 1.8170]\n",
      "[Epoch 9/500] [Batch 271/312] [D IS TRAINING] [D loss: 0.2157] [G loss: 2.7010]\n",
      "[Epoch 9/500] [Batch 272/312] [D IS TRAINING] [D loss: 0.2054] [G loss: 1.5889]\n",
      "[Epoch 9/500] [Batch 273/312] [D IS TRAINING] [D loss: 0.1788] [G loss: 3.4913]\n",
      "[Epoch 9/500] [Batch 274/312] [D IS TRAINING] [D loss: 0.1264] [G loss: 2.2013]\n",
      "[Epoch 9/500] [Batch 275/312] [D IS TRAINING] [D loss: 0.1464] [G loss: 2.0455]\n",
      "[Epoch 9/500] [Batch 276/312] [D IS TRAINING] [D loss: 0.1745] [G loss: 2.5388]\n",
      "[Epoch 9/500] [Batch 277/312] [D IS TRAINING] [D loss: 0.1671] [G loss: 2.2828]\n",
      "[Epoch 9/500] [Batch 278/312] [D IS TRAINING] [D loss: 0.1446] [G loss: 2.2000]\n",
      "[Epoch 9/500] [Batch 279/312] [D IS TRAINING] [D loss: 0.1586] [G loss: 1.9621]\n",
      "[Epoch 9/500] [Batch 280/312] [D IS TRAINING] [D loss: 0.2708] [G loss: 3.3777]\n",
      "[Epoch 9/500] [Batch 281/312] [D IS TRAINING] [D loss: 0.3285] [G loss: 1.1406]\n",
      "[Epoch 9/500] [Batch 282/312] [D IS TRAINING] [D loss: 0.2568] [G loss: 3.7556]\n",
      "[Epoch 9/500] [Batch 283/312] [D IS TRAINING] [D loss: 0.0925] [G loss: 2.7664]\n",
      "[Epoch 9/500] [Batch 284/312] [D IS TRAINING] [D loss: 0.1738] [G loss: 2.0752]\n",
      "[Epoch 9/500] [Batch 285/312] [D IS TRAINING] [D loss: 0.1867] [G loss: 2.2815]\n",
      "[Epoch 9/500] [Batch 286/312] [D IS TRAINING] [D loss: 0.1826] [G loss: 2.7604]\n",
      "[Epoch 9/500] [Batch 287/312] [D IS TRAINING] [D loss: 0.2107] [G loss: 1.6028]\n",
      "[Epoch 9/500] [Batch 288/312] [D IS TRAINING] [D loss: 0.2357] [G loss: 2.1583]\n",
      "[Epoch 9/500] [Batch 289/312] [D IS TRAINING] [D loss: 0.2267] [G loss: 3.0545]\n",
      "[Epoch 9/500] [Batch 290/312] [D IS TRAINING] [D loss: 0.2808] [G loss: 1.1388]\n",
      "[Epoch 9/500] [Batch 291/312] [D IS TRAINING] [D loss: 0.2793] [G loss: 4.1947]\n",
      "[Epoch 9/500] [Batch 292/312] [D IS TRAINING] [D loss: 0.3069] [G loss: 1.1497]\n",
      "[Epoch 9/500] [Batch 293/312] [D IS TRAINING] [D loss: 0.4615] [G loss: 4.7542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/500] [Batch 294/312] [D IS TRAINING] [D loss: 0.4319] [G loss: 0.9032]\n",
      "[Epoch 9/500] [Batch 295/312] [D IS TRAINING] [D loss: 0.2626] [G loss: 4.6963]\n",
      "[Epoch 9/500] [Batch 296/312] [D IS TRAINING] [D loss: 0.1550] [G loss: 2.3211]\n",
      "[Epoch 9/500] [Batch 297/312] [D IS TRAINING] [D loss: 0.2544] [G loss: 1.5590]\n",
      "[Epoch 9/500] [Batch 298/312] [D IS TRAINING] [D loss: 0.2858] [G loss: 3.4986]\n",
      "[Epoch 9/500] [Batch 299/312] [D IS TRAINING] [D loss: 0.2760] [G loss: 1.2897]\n",
      "[Epoch 9/500] [Batch 300/312] [D IS TRAINING] [D loss: 0.2512] [G loss: 2.8552]\n",
      "[Epoch 9/500] [Batch 301/312] [D IS TRAINING] [D loss: 0.1353] [G loss: 2.4272]\n",
      "[Epoch 9/500] [Batch 302/312] [D IS TRAINING] [D loss: 0.1753] [G loss: 1.7860]\n",
      "[Epoch 9/500] [Batch 303/312] [D IS TRAINING] [D loss: 0.1655] [G loss: 3.1256]\n",
      "[Epoch 9/500] [Batch 304/312] [D IS TRAINING] [D loss: 0.1774] [G loss: 2.2010]\n",
      "[Epoch 9/500] [Batch 305/312] [D IS TRAINING] [D loss: 0.1933] [G loss: 1.8217]\n",
      "[Epoch 9/500] [Batch 306/312] [D IS TRAINING] [D loss: 0.1657] [G loss: 3.1427]\n",
      "[Epoch 9/500] [Batch 307/312] [D IS TRAINING] [D loss: 0.2031] [G loss: 1.8423]\n",
      "[Epoch 9/500] [Batch 308/312] [D IS TRAINING] [D loss: 0.2405] [G loss: 2.6529]\n",
      "[Epoch 9/500] [Batch 309/312] [D IS TRAINING] [D loss: 0.2296] [G loss: 2.4156]\n",
      "[Epoch 9/500] [Batch 310/312] [D IS TRAINING] [D loss: 0.2045] [G loss: 2.4764]\n",
      "[Epoch 9/500] [Batch 311/312] [D IS TRAINING] [D loss: 0.1615] [G loss: 1.9454]\n",
      "[Epoch 10/500] [Batch 0/312] [D IS TRAINING] [D loss: 0.1226] [G loss: 2.9846]\n",
      "[Epoch 10/500] [Batch 1/312] [D IS TRAINING] [D loss: 0.1122] [G loss: 2.8067]\n",
      "[Epoch 10/500] [Batch 2/312] [D IS TRAINING] [D loss: 0.1644] [G loss: 1.7302]\n",
      "[Epoch 10/500] [Batch 3/312] [D IS TRAINING] [D loss: 0.2163] [G loss: 2.8696]\n",
      "[Epoch 10/500] [Batch 4/312] [D IS TRAINING] [D loss: 0.1675] [G loss: 1.7216]\n",
      "[Epoch 10/500] [Batch 5/312] [D IS TRAINING] [D loss: 0.1523] [G loss: 3.0336]\n",
      "[Epoch 10/500] [Batch 6/312] [D IS TRAINING] [D loss: 0.1335] [G loss: 2.3530]\n",
      "[Epoch 10/500] [Batch 7/312] [D IS TRAINING] [D loss: 0.1470] [G loss: 1.7684]\n",
      "[Epoch 10/500] [Batch 8/312] [D IS TRAINING] [D loss: 0.2270] [G loss: 3.1437]\n",
      "[Epoch 10/500] [Batch 9/312] [D IS TRAINING] [D loss: 0.2035] [G loss: 1.5137]\n",
      "[Epoch 10/500] [Batch 10/312] [D IS TRAINING] [D loss: 0.1748] [G loss: 3.0394]\n",
      "[Epoch 10/500] [Batch 11/312] [D IS TRAINING] [D loss: 0.1388] [G loss: 2.3247]\n",
      "[Epoch 10/500] [Batch 12/312] [D IS TRAINING] [D loss: 0.1725] [G loss: 1.7069]\n",
      "[Epoch 10/500] [Batch 13/312] [D IS TRAINING] [D loss: 0.2407] [G loss: 3.5697]\n",
      "[Epoch 10/500] [Batch 14/312] [D IS TRAINING] [D loss: 0.2087] [G loss: 1.6695]\n",
      "[Epoch 10/500] [Batch 15/312] [D IS TRAINING] [D loss: 0.2179] [G loss: 3.1437]\n",
      "[Epoch 10/500] [Batch 16/312] [D IS TRAINING] [D loss: 0.1561] [G loss: 2.9953]\n",
      "[Epoch 10/500] [Batch 17/312] [D IS TRAINING] [D loss: 0.1423] [G loss: 2.1917]\n",
      "[Epoch 10/500] [Batch 18/312] [D IS TRAINING] [D loss: 0.1648] [G loss: 2.4523]\n",
      "[Epoch 10/500] [Batch 19/312] [D IS TRAINING] [D loss: 0.1223] [G loss: 3.1374]\n",
      "[Epoch 10/500] [Batch 20/312] [D IS TRAINING] [D loss: 0.1130] [G loss: 2.7341]\n",
      "[Epoch 10/500] [Batch 21/312] [D IS TRAINING] [D loss: 0.2028] [G loss: 1.6554]\n",
      "[Epoch 10/500] [Batch 22/312] [D IS TRAINING] [D loss: 0.1662] [G loss: 3.1654]\n",
      "[Epoch 10/500] [Batch 23/312] [D IS TRAINING] [D loss: 0.1220] [G loss: 2.7421]\n",
      "[Epoch 10/500] [Batch 24/312] [D IS TRAINING] [D loss: 0.1885] [G loss: 1.5519]\n",
      "[Epoch 10/500] [Batch 25/312] [D IS TRAINING] [D loss: 0.3138] [G loss: 3.9071]\n",
      "[Epoch 10/500] [Batch 26/312] [D IS TRAINING] [D loss: 0.4055] [G loss: 0.8424]\n",
      "[Epoch 10/500] [Batch 27/312] [D IS TRAINING] [D loss: 0.7932] [G loss: 5.4302]\n",
      "[Epoch 10/500] [Batch 28/312] [D IS TRAINING] [D loss: 0.7302] [G loss: 0.4139]\n",
      "[Epoch 10/500] [Batch 29/312] [D IS TRAINING] [D loss: 0.5453] [G loss: 5.1539]\n",
      "[Epoch 10/500] [Batch 30/312] [D IS TRAINING] [D loss: 1.2233] [G loss: 0.1935]\n",
      "[Epoch 10/500] [Batch 31/312] [D IS TRAINING] [D loss: 1.4821] [G loss: 7.8202]\n",
      "[Epoch 10/500] [Batch 32/312] [D IS TRAINING] [D loss: 0.3647] [G loss: 2.1964]\n",
      "[Epoch 10/500] [Batch 33/312] [D IS TRAINING] [D loss: 0.7559] [G loss: 0.6786]\n",
      "[Epoch 10/500] [Batch 34/312] [D IS TRAINING] [D loss: 0.5459] [G loss: 4.0979]\n",
      "[Epoch 10/500] [Batch 35/312] [D IS TRAINING] [D loss: 0.3457] [G loss: 1.4820]\n",
      "[Epoch 10/500] [Batch 36/312] [D IS TRAINING] [D loss: 0.5972] [G loss: 2.6772]\n",
      "[Epoch 10/500] [Batch 37/312] [D IS TRAINING] [D loss: 0.5714] [G loss: 1.7391]\n",
      "[Epoch 10/500] [Batch 38/312] [D IS TRAINING] [D loss: 0.4190] [G loss: 4.1055]\n",
      "[Epoch 10/500] [Batch 39/312] [D IS TRAINING] [D loss: 0.2712] [G loss: 2.0000]\n",
      "[Epoch 10/500] [Batch 40/312] [D IS TRAINING] [D loss: 0.2447] [G loss: 1.7032]\n",
      "[Epoch 10/500] [Batch 41/312] [D IS TRAINING] [D loss: 0.2178] [G loss: 1.7170]\n",
      "[Epoch 10/500] [Batch 42/312] [D IS TRAINING] [D loss: 0.2294] [G loss: 2.8500]\n",
      "[Epoch 10/500] [Batch 43/312] [D IS TRAINING] [D loss: 0.2579] [G loss: 1.8457]\n",
      "[Epoch 10/500] [Batch 44/312] [D IS TRAINING] [D loss: 0.3031] [G loss: 1.2773]\n",
      "[Epoch 10/500] [Batch 45/312] [D IS TRAINING] [D loss: 0.4195] [G loss: 3.0649]\n",
      "[Epoch 10/500] [Batch 46/312] [D IS TRAINING] [D loss: 0.3862] [G loss: 0.9842]\n",
      "[Epoch 10/500] [Batch 47/312] [D IS TRAINING] [D loss: 0.2401] [G loss: 2.9530]\n",
      "[Epoch 10/500] [Batch 48/312] [D IS TRAINING] [D loss: 0.2122] [G loss: 2.6124]\n",
      "[Epoch 10/500] [Batch 49/312] [D IS TRAINING] [D loss: 0.2376] [G loss: 1.4663]\n",
      "[Epoch 10/500] [Batch 50/312] [D IS TRAINING] [D loss: 0.1745] [G loss: 2.3147]\n",
      "[Epoch 10/500] [Batch 51/312] [D IS TRAINING] [D loss: 0.2035] [G loss: 2.5050]\n",
      "[Epoch 10/500] [Batch 52/312] [D IS TRAINING] [D loss: 0.1829] [G loss: 2.1122]\n",
      "[Epoch 10/500] [Batch 53/312] [D IS TRAINING] [D loss: 0.2200] [G loss: 1.5780]\n",
      "[Epoch 10/500] [Batch 54/312] [D IS TRAINING] [D loss: 0.2142] [G loss: 2.7217]\n",
      "[Epoch 10/500] [Batch 55/312] [D IS TRAINING] [D loss: 0.2291] [G loss: 1.8958]\n",
      "[Epoch 10/500] [Batch 56/312] [D IS TRAINING] [D loss: 0.2285] [G loss: 1.6152]\n",
      "[Epoch 10/500] [Batch 57/312] [D IS TRAINING] [D loss: 0.1966] [G loss: 2.7546]\n",
      "[Epoch 10/500] [Batch 58/312] [D IS TRAINING] [D loss: 0.1906] [G loss: 1.7649]\n",
      "[Epoch 10/500] [Batch 59/312] [D IS TRAINING] [D loss: 0.1290] [G loss: 2.8044]\n",
      "[Epoch 10/500] [Batch 60/312] [D IS TRAINING] [D loss: 0.1850] [G loss: 2.2827]\n",
      "[Epoch 10/500] [Batch 61/312] [D IS TRAINING] [D loss: 0.1904] [G loss: 1.8127]\n",
      "[Epoch 10/500] [Batch 62/312] [D IS TRAINING] [D loss: 0.2008] [G loss: 2.3918]\n",
      "[Epoch 10/500] [Batch 63/312] [D IS TRAINING] [D loss: 0.2113] [G loss: 1.9924]\n",
      "[Epoch 10/500] [Batch 64/312] [D IS TRAINING] [D loss: 0.2009] [G loss: 2.2663]\n",
      "[Epoch 10/500] [Batch 65/312] [D IS TRAINING] [D loss: 0.1923] [G loss: 2.1219]\n",
      "[Epoch 10/500] [Batch 66/312] [D IS TRAINING] [D loss: 0.2305] [G loss: 1.9990]\n",
      "[Epoch 10/500] [Batch 67/312] [D IS TRAINING] [D loss: 0.1791] [G loss: 1.9112]\n",
      "[Epoch 10/500] [Batch 68/312] [D IS TRAINING] [D loss: 0.1566] [G loss: 2.6572]\n",
      "[Epoch 10/500] [Batch 69/312] [D IS TRAINING] [D loss: 0.1393] [G loss: 2.5307]\n",
      "[Epoch 10/500] [Batch 70/312] [D IS TRAINING] [D loss: 0.1832] [G loss: 2.2563]\n",
      "[Epoch 10/500] [Batch 71/312] [D IS TRAINING] [D loss: 0.2209] [G loss: 1.6879]\n",
      "[Epoch 10/500] [Batch 72/312] [D IS TRAINING] [D loss: 0.2129] [G loss: 2.7365]\n",
      "[Epoch 10/500] [Batch 73/312] [D IS TRAINING] [D loss: 0.2841] [G loss: 1.3976]\n",
      "[Epoch 10/500] [Batch 74/312] [D IS TRAINING] [D loss: 0.3015] [G loss: 3.5222]\n",
      "[Epoch 10/500] [Batch 75/312] [D IS TRAINING] [D loss: 0.1912] [G loss: 1.6993]\n",
      "[Epoch 10/500] [Batch 76/312] [D IS TRAINING] [D loss: 0.1663] [G loss: 2.7263]\n",
      "[Epoch 10/500] [Batch 77/312] [D IS TRAINING] [D loss: 0.1643] [G loss: 2.1424]\n",
      "[Epoch 10/500] [Batch 78/312] [D IS TRAINING] [D loss: 0.1468] [G loss: 2.4418]\n",
      "[Epoch 10/500] [Batch 79/312] [D IS TRAINING] [D loss: 0.1783] [G loss: 2.3609]\n",
      "[Epoch 10/500] [Batch 80/312] [D IS TRAINING] [D loss: 0.1937] [G loss: 1.6348]\n",
      "[Epoch 10/500] [Batch 81/312] [D IS TRAINING] [D loss: 0.2496] [G loss: 2.7825]\n",
      "[Epoch 10/500] [Batch 82/312] [D IS TRAINING] [D loss: 0.2800] [G loss: 1.2739]\n",
      "[Epoch 10/500] [Batch 83/312] [D IS TRAINING] [D loss: 0.2308] [G loss: 2.9427]\n",
      "[Epoch 10/500] [Batch 84/312] [D IS TRAINING] [D loss: 0.1747] [G loss: 1.7371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/500] [Batch 85/312] [D IS TRAINING] [D loss: 0.1898] [G loss: 2.0579]\n",
      "[Epoch 10/500] [Batch 86/312] [D IS TRAINING] [D loss: 0.1718] [G loss: 2.4060]\n",
      "[Epoch 10/500] [Batch 87/312] [D IS TRAINING] [D loss: 0.1530] [G loss: 2.4774]\n",
      "[Epoch 10/500] [Batch 88/312] [D IS TRAINING] [D loss: 0.1885] [G loss: 1.7685]\n",
      "[Epoch 10/500] [Batch 89/312] [D IS TRAINING] [D loss: 0.1682] [G loss: 2.5472]\n",
      "[Epoch 10/500] [Batch 90/312] [D IS TRAINING] [D loss: 0.1704] [G loss: 2.1357]\n",
      "[Epoch 10/500] [Batch 91/312] [D IS TRAINING] [D loss: 0.1057] [G loss: 2.3998]\n",
      "[Epoch 10/500] [Batch 92/312] [D IS TRAINING] [D loss: 0.1667] [G loss: 2.2864]\n",
      "[Epoch 10/500] [Batch 93/312] [D IS TRAINING] [D loss: 0.1869] [G loss: 2.3837]\n",
      "[Epoch 10/500] [Batch 94/312] [D IS TRAINING] [D loss: 0.2309] [G loss: 1.4931]\n",
      "[Epoch 10/500] [Batch 95/312] [D IS TRAINING] [D loss: 0.2486] [G loss: 2.6250]\n",
      "[Epoch 10/500] [Batch 96/312] [D IS TRAINING] [D loss: 0.2170] [G loss: 1.7180]\n",
      "[Epoch 10/500] [Batch 97/312] [D IS TRAINING] [D loss: 0.2054] [G loss: 2.0539]\n",
      "[Epoch 10/500] [Batch 98/312] [D IS TRAINING] [D loss: 0.1548] [G loss: 2.4931]\n",
      "[Epoch 10/500] [Batch 99/312] [D IS TRAINING] [D loss: 0.1486] [G loss: 2.0099]\n",
      "[Epoch 10/500] [Batch 100/312] [D IS TRAINING] [D loss: 0.1700] [G loss: 2.5870]\n",
      "[Epoch 10/500] [Batch 101/312] [D IS TRAINING] [D loss: 0.1524] [G loss: 2.1776]\n",
      "[Epoch 10/500] [Batch 102/312] [D IS TRAINING] [D loss: 0.1765] [G loss: 2.1057]\n",
      "[Epoch 10/500] [Batch 103/312] [D IS TRAINING] [D loss: 0.2241] [G loss: 1.9950]\n",
      "[Epoch 10/500] [Batch 104/312] [D IS TRAINING] [D loss: 0.1651] [G loss: 2.1538]\n",
      "[Epoch 10/500] [Batch 105/312] [D IS TRAINING] [D loss: 0.1478] [G loss: 2.7946]\n",
      "[Epoch 10/500] [Batch 106/312] [D IS TRAINING] [D loss: 0.1481] [G loss: 2.2291]\n",
      "[Epoch 10/500] [Batch 107/312] [D IS TRAINING] [D loss: 0.1568] [G loss: 2.3134]\n",
      "[Epoch 10/500] [Batch 108/312] [D IS TRAINING] [D loss: 0.1684] [G loss: 2.2742]\n",
      "[Epoch 10/500] [Batch 109/312] [D IS TRAINING] [D loss: 0.1506] [G loss: 2.4162]\n",
      "[Epoch 10/500] [Batch 110/312] [D IS TRAINING] [D loss: 0.1803] [G loss: 1.8953]\n",
      "[Epoch 10/500] [Batch 111/312] [D IS TRAINING] [D loss: 0.1937] [G loss: 2.3306]\n",
      "[Epoch 10/500] [Batch 112/312] [D IS TRAINING] [D loss: 0.2330] [G loss: 1.6318]\n",
      "[Epoch 10/500] [Batch 113/312] [D IS TRAINING] [D loss: 0.2688] [G loss: 3.5028]\n",
      "[Epoch 10/500] [Batch 114/312] [D IS TRAINING] [D loss: 0.4404] [G loss: 0.7974]\n",
      "[Epoch 10/500] [Batch 115/312] [D IS TRAINING] [D loss: 0.5334] [G loss: 4.4753]\n",
      "[Epoch 10/500] [Batch 116/312] [D IS TRAINING] [D loss: 1.1812] [G loss: 0.2035]\n",
      "[Epoch 10/500] [Batch 117/312] [D IS TRAINING] [D loss: 2.0870] [G loss: 8.5733]\n",
      "[Epoch 10/500] [Batch 118/312] [D IS TRAINING] [D loss: 0.2351] [G loss: 2.8141]\n",
      "[Epoch 10/500] [Batch 119/312] [D IS TRAINING] [D loss: 0.9302] [G loss: 0.4822]\n",
      "[Epoch 10/500] [Batch 120/312] [D IS TRAINING] [D loss: 0.9566] [G loss: 5.4332]\n",
      "[Epoch 10/500] [Batch 121/312] [D IS TRAINING] [D loss: 0.4344] [G loss: 1.1000]\n",
      "[Epoch 10/500] [Batch 122/312] [D IS TRAINING] [D loss: 0.3952] [G loss: 1.4225]\n",
      "[Epoch 10/500] [Batch 123/312] [D IS TRAINING] [D loss: 0.3907] [G loss: 3.2737]\n",
      "[Epoch 10/500] [Batch 124/312] [D IS TRAINING] [D loss: 0.2749] [G loss: 1.6340]\n",
      "[Epoch 10/500] [Batch 125/312] [D IS TRAINING] [D loss: 0.2729] [G loss: 1.8947]\n",
      "[Epoch 10/500] [Batch 126/312] [D IS TRAINING] [D loss: 0.2176] [G loss: 2.5345]\n",
      "[Epoch 10/500] [Batch 127/312] [D IS TRAINING] [D loss: 0.2204] [G loss: 1.5362]\n",
      "[Epoch 10/500] [Batch 128/312] [D IS TRAINING] [D loss: 0.2043] [G loss: 3.0452]\n",
      "[Epoch 10/500] [Batch 129/312] [D IS TRAINING] [D loss: 0.1692] [G loss: 2.2730]\n",
      "[Epoch 10/500] [Batch 130/312] [D IS TRAINING] [D loss: 0.2245] [G loss: 1.6101]\n",
      "[Epoch 10/500] [Batch 131/312] [D IS TRAINING] [D loss: 0.1808] [G loss: 2.7672]\n",
      "[Epoch 10/500] [Batch 132/312] [D IS TRAINING] [D loss: 0.1614] [G loss: 2.4512]\n",
      "[Epoch 10/500] [Batch 133/312] [D IS TRAINING] [D loss: 0.2284] [G loss: 1.5076]\n",
      "[Epoch 10/500] [Batch 134/312] [D IS TRAINING] [D loss: 0.2001] [G loss: 2.5226]\n",
      "[Epoch 10/500] [Batch 135/312] [D IS TRAINING] [D loss: 0.1745] [G loss: 1.9500]\n",
      "[Epoch 10/500] [Batch 136/312] [D IS TRAINING] [D loss: 0.2453] [G loss: 1.8246]\n",
      "[Epoch 10/500] [Batch 137/312] [D IS TRAINING] [D loss: 0.2234] [G loss: 2.4565]\n",
      "[Epoch 10/500] [Batch 138/312] [D IS TRAINING] [D loss: 0.2334] [G loss: 1.5336]\n",
      "[Epoch 10/500] [Batch 139/312] [D IS TRAINING] [D loss: 0.1356] [G loss: 2.8487]\n",
      "[Epoch 10/500] [Batch 140/312] [D IS TRAINING] [D loss: 0.1554] [G loss: 2.3422]\n",
      "[Epoch 10/500] [Batch 141/312] [D IS TRAINING] [D loss: 0.1887] [G loss: 1.7006]\n",
      "[Epoch 10/500] [Batch 142/312] [D IS TRAINING] [D loss: 0.2361] [G loss: 2.7941]\n",
      "[Epoch 10/500] [Batch 143/312] [D IS TRAINING] [D loss: 0.3074] [G loss: 1.1526]\n",
      "[Epoch 10/500] [Batch 144/312] [D IS TRAINING] [D loss: 0.2125] [G loss: 3.2215]\n",
      "[Epoch 10/500] [Batch 145/312] [D IS TRAINING] [D loss: 0.1088] [G loss: 2.7242]\n",
      "[Epoch 10/500] [Batch 146/312] [D IS TRAINING] [D loss: 0.1170] [G loss: 2.0694]\n",
      "[Epoch 10/500] [Batch 147/312] [D IS TRAINING] [D loss: 0.1429] [G loss: 2.1814]\n",
      "[Epoch 10/500] [Batch 148/312] [D IS TRAINING] [D loss: 0.1532] [G loss: 2.2975]\n",
      "[Epoch 10/500] [Batch 149/312] [D IS TRAINING] [D loss: 0.1969] [G loss: 2.8408]\n",
      "[Epoch 10/500] [Batch 150/312] [D IS TRAINING] [D loss: 0.2415] [G loss: 1.5414]\n",
      "[Epoch 10/500] [Batch 151/312] [D IS TRAINING] [D loss: 0.1502] [G loss: 2.8709]\n",
      "[Epoch 10/500] [Batch 152/312] [D IS TRAINING] [D loss: 0.1744] [G loss: 2.1791]\n",
      "[Epoch 10/500] [Batch 153/312] [D IS TRAINING] [D loss: 0.1775] [G loss: 2.3916]\n",
      "[Epoch 10/500] [Batch 154/312] [D IS TRAINING] [D loss: 0.2047] [G loss: 1.7513]\n",
      "[Epoch 10/500] [Batch 155/312] [D IS TRAINING] [D loss: 0.2138] [G loss: 2.2871]\n",
      "[Epoch 10/500] [Batch 156/312] [D IS TRAINING] [D loss: 0.2033] [G loss: 1.9263]\n",
      "[Epoch 10/500] [Batch 157/312] [D IS TRAINING] [D loss: 0.1634] [G loss: 2.5865]\n",
      "[Epoch 10/500] [Batch 158/312] [D IS TRAINING] [D loss: 0.1743] [G loss: 2.4278]\n",
      "[Epoch 10/500] [Batch 159/312] [D IS TRAINING] [D loss: 0.1854] [G loss: 1.7801]\n",
      "[Epoch 10/500] [Batch 160/312] [D IS TRAINING] [D loss: 0.2212] [G loss: 2.7328]\n",
      "[Epoch 10/500] [Batch 161/312] [D IS TRAINING] [D loss: 0.1643] [G loss: 1.7779]\n",
      "[Epoch 10/500] [Batch 162/312] [D IS TRAINING] [D loss: 0.2105] [G loss: 2.7046]\n",
      "[Epoch 10/500] [Batch 163/312] [D IS TRAINING] [D loss: 0.2097] [G loss: 1.5532]\n",
      "[Epoch 10/500] [Batch 164/312] [D IS TRAINING] [D loss: 0.2376] [G loss: 3.0329]\n",
      "[Epoch 10/500] [Batch 165/312] [D IS TRAINING] [D loss: 0.2098] [G loss: 1.5959]\n",
      "[Epoch 10/500] [Batch 166/312] [D IS TRAINING] [D loss: 0.1594] [G loss: 2.5064]\n",
      "[Epoch 10/500] [Batch 167/312] [D IS TRAINING] [D loss: 0.2358] [G loss: 2.2695]\n",
      "[Epoch 10/500] [Batch 168/312] [D IS TRAINING] [D loss: 0.2782] [G loss: 1.2300]\n",
      "[Epoch 10/500] [Batch 169/312] [D IS TRAINING] [D loss: 0.3014] [G loss: 4.0843]\n",
      "[Epoch 10/500] [Batch 170/312] [D IS TRAINING] [D loss: 0.3113] [G loss: 1.0547]\n",
      "[Epoch 10/500] [Batch 171/312] [D IS TRAINING] [D loss: 0.3917] [G loss: 3.8239]\n",
      "[Epoch 10/500] [Batch 172/312] [D IS TRAINING] [D loss: 0.5328] [G loss: 0.5962]\n",
      "[Epoch 10/500] [Batch 173/312] [D IS TRAINING] [D loss: 0.5332] [G loss: 4.6154]\n",
      "[Epoch 10/500] [Batch 174/312] [D IS TRAINING] [D loss: 0.3206] [G loss: 1.0864]\n",
      "[Epoch 10/500] [Batch 175/312] [D IS TRAINING] [D loss: 0.1707] [G loss: 3.4829]\n",
      "[Epoch 10/500] [Batch 176/312] [D IS TRAINING] [D loss: 0.1564] [G loss: 2.7512]\n",
      "[Epoch 10/500] [Batch 177/312] [D IS TRAINING] [D loss: 0.2708] [G loss: 1.2850]\n",
      "[Epoch 10/500] [Batch 178/312] [D IS TRAINING] [D loss: 0.2478] [G loss: 3.2472]\n",
      "[Epoch 10/500] [Batch 179/312] [D IS TRAINING] [D loss: 0.1779] [G loss: 1.9179]\n",
      "[Epoch 10/500] [Batch 180/312] [D IS TRAINING] [D loss: 0.1754] [G loss: 2.0927]\n",
      "[Epoch 10/500] [Batch 181/312] [D IS TRAINING] [D loss: 0.2414] [G loss: 2.3085]\n",
      "[Epoch 10/500] [Batch 182/312] [D IS TRAINING] [D loss: 0.1893] [G loss: 2.0213]\n",
      "[Epoch 10/500] [Batch 183/312] [D IS TRAINING] [D loss: 0.1770] [G loss: 2.4429]\n",
      "[Epoch 10/500] [Batch 184/312] [D IS TRAINING] [D loss: 0.2358] [G loss: 1.7098]\n",
      "[Epoch 10/500] [Batch 185/312] [D IS TRAINING] [D loss: 0.2096] [G loss: 2.1439]\n",
      "[Epoch 10/500] [Batch 186/312] [D IS TRAINING] [D loss: 0.1885] [G loss: 2.6920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/500] [Batch 187/312] [D IS TRAINING] [D loss: 0.1773] [G loss: 1.8616]\n",
      "[Epoch 10/500] [Batch 188/312] [D IS TRAINING] [D loss: 0.2346] [G loss: 1.9061]\n",
      "[Epoch 10/500] [Batch 189/312] [D IS TRAINING] [D loss: 0.2716] [G loss: 1.9353]\n",
      "[Epoch 10/500] [Batch 190/312] [D IS TRAINING] [D loss: 0.2461] [G loss: 2.6350]\n",
      "[Epoch 10/500] [Batch 191/312] [D IS TRAINING] [D loss: 0.1878] [G loss: 1.7254]\n",
      "[Epoch 10/500] [Batch 192/312] [D IS TRAINING] [D loss: 0.1844] [G loss: 2.1622]\n",
      "[Epoch 10/500] [Batch 193/312] [D IS TRAINING] [D loss: 0.2155] [G loss: 2.8918]\n",
      "[Epoch 10/500] [Batch 194/312] [D IS TRAINING] [D loss: 0.1680] [G loss: 1.8203]\n",
      "[Epoch 10/500] [Batch 195/312] [D IS TRAINING] [D loss: 0.2137] [G loss: 1.8415]\n",
      "[Epoch 10/500] [Batch 196/312] [D IS TRAINING] [D loss: 0.2417] [G loss: 3.0426]\n",
      "[Epoch 10/500] [Batch 197/312] [D IS TRAINING] [D loss: 0.2859] [G loss: 1.2390]\n",
      "[Epoch 10/500] [Batch 198/312] [D IS TRAINING] [D loss: 0.2299] [G loss: 3.4379]\n",
      "[Epoch 10/500] [Batch 199/312] [D IS TRAINING] [D loss: 0.1789] [G loss: 1.9936]\n",
      "[Epoch 10/500] [Batch 200/312] [D IS TRAINING] [D loss: 0.1890] [G loss: 1.6993]\n",
      "[Epoch 10/500] [Batch 201/312] [D IS TRAINING] [D loss: 0.2722] [G loss: 2.5324]\n",
      "[Epoch 10/500] [Batch 202/312] [D IS TRAINING] [D loss: 0.4467] [G loss: 0.8503]\n",
      "[Epoch 10/500] [Batch 203/312] [D IS TRAINING] [D loss: 0.4948] [G loss: 4.6373]\n",
      "[Epoch 10/500] [Batch 204/312] [D IS TRAINING] [D loss: 0.3016] [G loss: 1.2530]\n",
      "[Epoch 10/500] [Batch 205/312] [D IS TRAINING] [D loss: 0.2111] [G loss: 2.2229]\n",
      "[Epoch 10/500] [Batch 206/312] [D IS TRAINING] [D loss: 0.2443] [G loss: 2.5515]\n",
      "[Epoch 10/500] [Batch 207/312] [D IS TRAINING] [D loss: 0.2082] [G loss: 1.5786]\n",
      "[Epoch 10/500] [Batch 208/312] [D IS TRAINING] [D loss: 0.1493] [G loss: 2.3395]\n",
      "[Epoch 10/500] [Batch 209/312] [D IS TRAINING] [D loss: 0.1955] [G loss: 3.2316]\n",
      "[Epoch 10/500] [Batch 210/312] [D IS TRAINING] [D loss: 0.1775] [G loss: 1.8391]\n",
      "[Epoch 10/500] [Batch 211/312] [D IS TRAINING] [D loss: 0.2076] [G loss: 1.7523]\n",
      "[Epoch 10/500] [Batch 212/312] [D IS TRAINING] [D loss: 0.2976] [G loss: 2.9374]\n",
      "[Epoch 10/500] [Batch 213/312] [D IS TRAINING] [D loss: 0.2186] [G loss: 1.6859]\n",
      "[Epoch 10/500] [Batch 214/312] [D IS TRAINING] [D loss: 0.1663] [G loss: 1.9941]\n",
      "[Epoch 10/500] [Batch 215/312] [D IS TRAINING] [D loss: 0.1729] [G loss: 3.0342]\n",
      "[Epoch 10/500] [Batch 216/312] [D IS TRAINING] [D loss: 0.1902] [G loss: 2.0920]\n",
      "[Epoch 10/500] [Batch 217/312] [D IS TRAINING] [D loss: 0.2223] [G loss: 1.5199]\n",
      "[Epoch 10/500] [Batch 218/312] [D IS TRAINING] [D loss: 0.2324] [G loss: 3.3049]\n",
      "[Epoch 10/500] [Batch 219/312] [D IS TRAINING] [D loss: 0.1712] [G loss: 2.3005]\n",
      "[Epoch 10/500] [Batch 220/312] [D IS TRAINING] [D loss: 0.4599] [G loss: 0.8780]\n",
      "[Epoch 10/500] [Batch 221/312] [D IS TRAINING] [D loss: 0.6991] [G loss: 4.8001]\n",
      "[Epoch 10/500] [Batch 222/312] [D IS TRAINING] [D loss: 0.3195] [G loss: 1.1661]\n",
      "[Epoch 10/500] [Batch 223/312] [D IS TRAINING] [D loss: 0.2078] [G loss: 2.7330]\n",
      "[Epoch 10/500] [Batch 224/312] [D IS TRAINING] [D loss: 0.1525] [G loss: 2.5990]\n",
      "[Epoch 10/500] [Batch 225/312] [D IS TRAINING] [D loss: 0.1701] [G loss: 2.5635]\n",
      "[Epoch 10/500] [Batch 226/312] [D IS TRAINING] [D loss: 0.2485] [G loss: 1.5999]\n",
      "[Epoch 10/500] [Batch 227/312] [D IS TRAINING] [D loss: 0.2288] [G loss: 2.6306]\n",
      "[Epoch 10/500] [Batch 228/312] [D IS TRAINING] [D loss: 0.1983] [G loss: 1.8955]\n",
      "[Epoch 10/500] [Batch 229/312] [D IS TRAINING] [D loss: 0.1423] [G loss: 2.8776]\n",
      "[Epoch 10/500] [Batch 230/312] [D IS TRAINING] [D loss: 0.1319] [G loss: 2.3833]\n",
      "[Epoch 10/500] [Batch 231/312] [D IS TRAINING] [D loss: 0.1497] [G loss: 2.0619]\n",
      "[Epoch 10/500] [Batch 232/312] [D IS TRAINING] [D loss: 0.1839] [G loss: 2.7592]\n",
      "[Epoch 10/500] [Batch 233/312] [D IS TRAINING] [D loss: 0.2142] [G loss: 1.6059]\n",
      "[Epoch 10/500] [Batch 234/312] [D IS TRAINING] [D loss: 0.1787] [G loss: 2.8790]\n",
      "[Epoch 10/500] [Batch 235/312] [D IS TRAINING] [D loss: 0.1534] [G loss: 2.2402]\n",
      "[Epoch 10/500] [Batch 236/312] [D IS TRAINING] [D loss: 0.1495] [G loss: 2.0941]\n",
      "[Epoch 10/500] [Batch 237/312] [D IS TRAINING] [D loss: 0.1758] [G loss: 2.4058]\n",
      "[Epoch 10/500] [Batch 238/312] [D IS TRAINING] [D loss: 0.2420] [G loss: 1.9736]\n",
      "[Epoch 10/500] [Batch 239/312] [D IS TRAINING] [D loss: 0.2014] [G loss: 1.7043]\n",
      "[Epoch 10/500] [Batch 240/312] [D IS TRAINING] [D loss: 0.2555] [G loss: 3.3023]\n",
      "[Epoch 10/500] [Batch 241/312] [D IS TRAINING] [D loss: 0.2144] [G loss: 1.4747]\n",
      "[Epoch 10/500] [Batch 242/312] [D IS TRAINING] [D loss: 0.2366] [G loss: 2.5728]\n",
      "[Epoch 10/500] [Batch 243/312] [D IS TRAINING] [D loss: 0.1070] [G loss: 2.8391]\n",
      "[Epoch 10/500] [Batch 244/312] [D IS TRAINING] [D loss: 0.1972] [G loss: 1.6177]\n",
      "[Epoch 10/500] [Batch 245/312] [D IS TRAINING] [D loss: 0.2469] [G loss: 2.7787]\n",
      "[Epoch 10/500] [Batch 246/312] [D IS TRAINING] [D loss: 0.3177] [G loss: 1.4006]\n",
      "[Epoch 10/500] [Batch 247/312] [D IS TRAINING] [D loss: 0.3643] [G loss: 2.7226]\n",
      "[Epoch 10/500] [Batch 248/312] [D IS TRAINING] [D loss: 0.3422] [G loss: 2.6403]\n",
      "[Epoch 10/500] [Batch 249/312] [D IS TRAINING] [D loss: 0.2078] [G loss: 2.7541]\n",
      "[Epoch 10/500] [Batch 250/312] [D IS TRAINING] [D loss: 0.3291] [G loss: 1.2257]\n",
      "[Epoch 10/500] [Batch 251/312] [D IS TRAINING] [D loss: 0.4011] [G loss: 4.0888]\n",
      "[Epoch 10/500] [Batch 252/312] [D IS TRAINING] [D loss: 0.3408] [G loss: 1.1329]\n",
      "[Epoch 10/500] [Batch 253/312] [D IS TRAINING] [D loss: 0.3713] [G loss: 4.0410]\n",
      "[Epoch 10/500] [Batch 254/312] [D IS TRAINING] [D loss: 0.2974] [G loss: 1.1875]\n",
      "[Epoch 10/500] [Batch 255/312] [D IS TRAINING] [D loss: 0.1769] [G loss: 3.2088]\n",
      "[Epoch 10/500] [Batch 256/312] [D IS TRAINING] [D loss: 0.1893] [G loss: 2.8894]\n",
      "[Epoch 10/500] [Batch 257/312] [D IS TRAINING] [D loss: 0.2448] [G loss: 1.4507]\n",
      "[Epoch 10/500] [Batch 258/312] [D IS TRAINING] [D loss: 0.2208] [G loss: 3.0066]\n",
      "[Epoch 10/500] [Batch 259/312] [D IS TRAINING] [D loss: 0.1760] [G loss: 1.9912]\n",
      "[Epoch 10/500] [Batch 260/312] [D IS TRAINING] [D loss: 0.1441] [G loss: 2.2091]\n",
      "[Epoch 10/500] [Batch 261/312] [D IS TRAINING] [D loss: 0.1659] [G loss: 2.1356]\n",
      "[Epoch 10/500] [Batch 262/312] [D IS TRAINING] [D loss: 0.1613] [G loss: 2.5264]\n",
      "[Epoch 10/500] [Batch 263/312] [D IS TRAINING] [D loss: 0.1299] [G loss: 2.1348]\n",
      "[Epoch 10/500] [Batch 264/312] [D IS TRAINING] [D loss: 0.2366] [G loss: 2.6983]\n",
      "[Epoch 10/500] [Batch 265/312] [D IS TRAINING] [D loss: 0.2368] [G loss: 1.3196]\n",
      "[Epoch 10/500] [Batch 266/312] [D IS TRAINING] [D loss: 0.1793] [G loss: 3.0257]\n",
      "[Epoch 10/500] [Batch 267/312] [D IS TRAINING] [D loss: 0.1369] [G loss: 2.5807]\n",
      "[Epoch 10/500] [Batch 268/312] [D IS TRAINING] [D loss: 0.1753] [G loss: 1.8130]\n",
      "[Epoch 10/500] [Batch 269/312] [D IS TRAINING] [D loss: 0.2053] [G loss: 2.5829]\n",
      "[Epoch 10/500] [Batch 270/312] [D IS TRAINING] [D loss: 0.1779] [G loss: 1.6456]\n",
      "[Epoch 10/500] [Batch 271/312] [D IS TRAINING] [D loss: 0.1423] [G loss: 2.6625]\n",
      "[Epoch 10/500] [Batch 272/312] [D IS TRAINING] [D loss: 0.1367] [G loss: 2.6441]\n",
      "[Epoch 10/500] [Batch 273/312] [D IS TRAINING] [D loss: 0.1790] [G loss: 1.8880]\n",
      "[Epoch 10/500] [Batch 274/312] [D IS TRAINING] [D loss: 0.1627] [G loss: 2.7279]\n",
      "[Epoch 10/500] [Batch 275/312] [D IS TRAINING] [D loss: 0.1692] [G loss: 1.8048]\n",
      "[Epoch 10/500] [Batch 276/312] [D IS TRAINING] [D loss: 0.1560] [G loss: 2.4352]\n",
      "[Epoch 10/500] [Batch 277/312] [D IS TRAINING] [D loss: 0.1554] [G loss: 2.1381]\n",
      "[Epoch 10/500] [Batch 278/312] [D IS TRAINING] [D loss: 0.1707] [G loss: 2.4016]\n",
      "[Epoch 10/500] [Batch 279/312] [D IS TRAINING] [D loss: 0.1508] [G loss: 1.9913]\n",
      "[Epoch 10/500] [Batch 280/312] [D IS TRAINING] [D loss: 0.1928] [G loss: 2.9066]\n",
      "[Epoch 10/500] [Batch 281/312] [D IS TRAINING] [D loss: 0.1545] [G loss: 1.7139]\n",
      "[Epoch 10/500] [Batch 282/312] [D IS TRAINING] [D loss: 0.1853] [G loss: 2.5992]\n",
      "[Epoch 10/500] [Batch 283/312] [D IS TRAINING] [D loss: 0.1808] [G loss: 1.9602]\n",
      "[Epoch 10/500] [Batch 284/312] [D IS TRAINING] [D loss: 0.1962] [G loss: 2.8128]\n",
      "[Epoch 10/500] [Batch 285/312] [D IS TRAINING] [D loss: 0.1702] [G loss: 1.7949]\n",
      "[Epoch 10/500] [Batch 286/312] [D IS TRAINING] [D loss: 0.1927] [G loss: 2.1336]\n",
      "[Epoch 10/500] [Batch 287/312] [D IS TRAINING] [D loss: 0.2276] [G loss: 2.7815]\n",
      "[Epoch 10/500] [Batch 288/312] [D IS TRAINING] [D loss: 0.2267] [G loss: 1.3284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/500] [Batch 289/312] [D IS TRAINING] [D loss: 0.3555] [G loss: 3.4979]\n",
      "[Epoch 10/500] [Batch 290/312] [D IS TRAINING] [D loss: 0.3324] [G loss: 0.9153]\n",
      "[Epoch 10/500] [Batch 291/312] [D IS TRAINING] [D loss: 0.2122] [G loss: 3.8745]\n",
      "[Epoch 10/500] [Batch 292/312] [D IS TRAINING] [D loss: 0.1945] [G loss: 3.0470]\n",
      "[Epoch 10/500] [Batch 293/312] [D IS TRAINING] [D loss: 0.5438] [G loss: 0.5689]\n",
      "[Epoch 10/500] [Batch 294/312] [D IS TRAINING] [D loss: 0.6065] [G loss: 5.4159]\n",
      "[Epoch 10/500] [Batch 295/312] [D IS TRAINING] [D loss: 1.2235] [G loss: 0.2281]\n",
      "[Epoch 10/500] [Batch 296/312] [D IS TRAINING] [D loss: 1.5665] [G loss: 8.3964]\n",
      "[Epoch 10/500] [Batch 297/312] [D IS TRAINING] [D loss: 0.5010] [G loss: 1.3510]\n",
      "[Epoch 10/500] [Batch 298/312] [D IS TRAINING] [D loss: 0.3585] [G loss: 2.0115]\n",
      "[Epoch 10/500] [Batch 299/312] [D IS TRAINING] [D loss: 0.3951] [G loss: 1.3602]\n",
      "[Epoch 10/500] [Batch 300/312] [D IS TRAINING] [D loss: 0.3502] [G loss: 3.1564]\n",
      "[Epoch 10/500] [Batch 301/312] [D IS TRAINING] [D loss: 0.3899] [G loss: 1.4057]\n",
      "[Epoch 10/500] [Batch 302/312] [D IS TRAINING] [D loss: 0.4408] [G loss: 3.9183]\n",
      "[Epoch 10/500] [Batch 303/312] [D IS TRAINING] [D loss: 0.1762] [G loss: 2.3974]\n",
      "[Epoch 10/500] [Batch 304/312] [D IS TRAINING] [D loss: 0.1534] [G loss: 2.2280]\n",
      "[Epoch 10/500] [Batch 305/312] [D IS TRAINING] [D loss: 0.1516] [G loss: 2.8011]\n",
      "[Epoch 10/500] [Batch 306/312] [D IS TRAINING] [D loss: 0.2096] [G loss: 1.9777]\n",
      "[Epoch 10/500] [Batch 307/312] [D IS TRAINING] [D loss: 0.2638] [G loss: 2.0128]\n",
      "[Epoch 10/500] [Batch 308/312] [D IS TRAINING] [D loss: 0.2163] [G loss: 1.7632]\n",
      "[Epoch 10/500] [Batch 309/312] [D IS TRAINING] [D loss: 0.1966] [G loss: 2.7046]\n",
      "[Epoch 10/500] [Batch 310/312] [D IS TRAINING] [D loss: 0.1555] [G loss: 2.0973]\n",
      "[Epoch 10/500] [Batch 311/312] [D IS TRAINING] [D loss: 0.1616] [G loss: 2.6225]\n",
      "[Epoch 11/500] [Batch 0/312] [D IS TRAINING] [D loss: 0.1122] [G loss: 2.6028]\n",
      "[Epoch 11/500] [Batch 1/312] [D IS TRAINING] [D loss: 0.1635] [G loss: 1.8550]\n",
      "[Epoch 11/500] [Batch 2/312] [D IS TRAINING] [D loss: 0.1568] [G loss: 2.6872]\n",
      "[Epoch 11/500] [Batch 3/312] [D IS TRAINING] [D loss: 0.2267] [G loss: 2.3710]\n",
      "[Epoch 11/500] [Batch 4/312] [D IS TRAINING] [D loss: 0.2187] [G loss: 1.5063]\n",
      "[Epoch 11/500] [Batch 5/312] [D IS TRAINING] [D loss: 0.1431] [G loss: 2.8855]\n",
      "[Epoch 11/500] [Batch 6/312] [D IS TRAINING] [D loss: 0.1770] [G loss: 2.3165]\n",
      "[Epoch 11/500] [Batch 7/312] [D IS TRAINING] [D loss: 0.2138] [G loss: 1.5589]\n",
      "[Epoch 11/500] [Batch 8/312] [D IS TRAINING] [D loss: 0.2910] [G loss: 3.0099]\n",
      "[Epoch 11/500] [Batch 9/312] [D IS TRAINING] [D loss: 0.2228] [G loss: 1.4423]\n",
      "[Epoch 11/500] [Batch 10/312] [D IS TRAINING] [D loss: 0.2154] [G loss: 2.9923]\n",
      "[Epoch 11/500] [Batch 11/312] [D IS TRAINING] [D loss: 0.1703] [G loss: 1.9057]\n",
      "[Epoch 11/500] [Batch 12/312] [D IS TRAINING] [D loss: 0.2055] [G loss: 1.7667]\n",
      "[Epoch 11/500] [Batch 13/312] [D IS TRAINING] [D loss: 0.1416] [G loss: 3.1319]\n",
      "[Epoch 11/500] [Batch 14/312] [D IS TRAINING] [D loss: 0.1888] [G loss: 2.4274]\n",
      "[Epoch 11/500] [Batch 15/312] [D IS TRAINING] [D loss: 0.1534] [G loss: 1.8934]\n",
      "[Epoch 11/500] [Batch 16/312] [D IS TRAINING] [D loss: 0.2379] [G loss: 2.7351]\n",
      "[Epoch 11/500] [Batch 17/312] [D IS TRAINING] [D loss: 0.2556] [G loss: 1.4315]\n",
      "[Epoch 11/500] [Batch 18/312] [D IS TRAINING] [D loss: 0.3864] [G loss: 3.7089]\n",
      "[Epoch 11/500] [Batch 19/312] [D IS TRAINING] [D loss: 0.3073] [G loss: 1.4445]\n",
      "[Epoch 11/500] [Batch 20/312] [D IS TRAINING] [D loss: 0.3302] [G loss: 3.2410]\n",
      "[Epoch 11/500] [Batch 21/312] [D IS TRAINING] [D loss: 0.1817] [G loss: 1.8355]\n",
      "[Epoch 11/500] [Batch 22/312] [D IS TRAINING] [D loss: 0.1800] [G loss: 2.3281]\n",
      "[Epoch 11/500] [Batch 23/312] [D IS TRAINING] [D loss: 0.2499] [G loss: 2.8336]\n",
      "[Epoch 11/500] [Batch 24/312] [D IS TRAINING] [D loss: 0.3002] [G loss: 1.3001]\n",
      "[Epoch 11/500] [Batch 25/312] [D IS TRAINING] [D loss: 0.2171] [G loss: 3.3581]\n",
      "[Epoch 11/500] [Batch 26/312] [D IS TRAINING] [D loss: 0.1561] [G loss: 2.7312]\n",
      "[Epoch 11/500] [Batch 27/312] [D IS TRAINING] [D loss: 0.1943] [G loss: 1.6938]\n",
      "[Epoch 11/500] [Batch 28/312] [D IS TRAINING] [D loss: 0.1449] [G loss: 3.2266]\n",
      "[Epoch 11/500] [Batch 29/312] [D IS TRAINING] [D loss: 0.1352] [G loss: 2.8114]\n",
      "[Epoch 11/500] [Batch 30/312] [D IS TRAINING] [D loss: 0.2132] [G loss: 1.5726]\n",
      "[Epoch 11/500] [Batch 31/312] [D IS TRAINING] [D loss: 0.1581] [G loss: 2.7816]\n",
      "[Epoch 11/500] [Batch 32/312] [D IS TRAINING] [D loss: 0.1571] [G loss: 2.4659]\n",
      "[Epoch 11/500] [Batch 33/312] [D IS TRAINING] [D loss: 0.1578] [G loss: 2.0016]\n",
      "[Epoch 11/500] [Batch 34/312] [D IS TRAINING] [D loss: 0.1426] [G loss: 2.6795]\n",
      "[Epoch 11/500] [Batch 35/312] [D IS TRAINING] [D loss: 0.1456] [G loss: 2.3076]\n",
      "[Epoch 11/500] [Batch 36/312] [D IS TRAINING] [D loss: 0.1776] [G loss: 2.0706]\n",
      "[Epoch 11/500] [Batch 37/312] [D IS TRAINING] [D loss: 0.0945] [G loss: 3.0206]\n",
      "[Epoch 11/500] [Batch 38/312] [D IS TRAINING] [D loss: 0.0969] [G loss: 2.5991]\n",
      "[Epoch 11/500] [Batch 39/312] [D IS TRAINING] [D loss: 0.0696] [G loss: 3.0045]\n",
      "[Epoch 11/500] [Batch 40/312] [D IS TRAINING] [D loss: 0.1644] [G loss: 2.0696]\n",
      "[Epoch 11/500] [Batch 41/312] [D IS TRAINING] [D loss: 0.2053] [G loss: 2.2526]\n",
      "[Epoch 11/500] [Batch 42/312] [D IS TRAINING] [D loss: 0.1671] [G loss: 2.3444]\n",
      "[Epoch 11/500] [Batch 43/312] [D IS TRAINING] [D loss: 0.2085] [G loss: 3.1783]\n",
      "[Epoch 11/500] [Batch 44/312] [D IS TRAINING] [D loss: 0.1920] [G loss: 1.6764]\n",
      "[Epoch 11/500] [Batch 45/312] [D IS TRAINING] [D loss: 0.1356] [G loss: 2.9268]\n",
      "[Epoch 11/500] [Batch 46/312] [D IS TRAINING] [D loss: 0.1332] [G loss: 2.8368]\n",
      "[Epoch 11/500] [Batch 47/312] [D IS TRAINING] [D loss: 0.2125] [G loss: 1.6144]\n",
      "[Epoch 11/500] [Batch 48/312] [D IS TRAINING] [D loss: 0.3076] [G loss: 3.2697]\n",
      "[Epoch 11/500] [Batch 49/312] [D IS TRAINING] [D loss: 0.3248] [G loss: 1.0390]\n",
      "[Epoch 11/500] [Batch 50/312] [D IS TRAINING] [D loss: 0.3055] [G loss: 3.9050]\n",
      "[Epoch 11/500] [Batch 51/312] [D IS TRAINING] [D loss: 0.1884] [G loss: 1.7374]\n",
      "[Epoch 11/500] [Batch 52/312] [D IS TRAINING] [D loss: 0.1397] [G loss: 2.9548]\n",
      "[Epoch 11/500] [Batch 53/312] [D IS TRAINING] [D loss: 0.1136] [G loss: 2.8852]\n",
      "[Epoch 11/500] [Batch 54/312] [D IS TRAINING] [D loss: 0.2359] [G loss: 1.7063]\n",
      "[Epoch 11/500] [Batch 55/312] [D IS TRAINING] [D loss: 0.3313] [G loss: 3.7433]\n",
      "[Epoch 11/500] [Batch 56/312] [D IS TRAINING] [D loss: 0.4003] [G loss: 0.8947]\n",
      "[Epoch 11/500] [Batch 57/312] [D IS TRAINING] [D loss: 0.4246] [G loss: 5.0170]\n",
      "[Epoch 11/500] [Batch 58/312] [D IS TRAINING] [D loss: 0.3788] [G loss: 1.2707]\n",
      "[Epoch 11/500] [Batch 59/312] [D IS TRAINING] [D loss: 0.3071] [G loss: 3.6838]\n",
      "[Epoch 11/500] [Batch 60/312] [D IS TRAINING] [D loss: 0.2395] [G loss: 1.5454]\n",
      "[Epoch 11/500] [Batch 61/312] [D IS TRAINING] [D loss: 0.1495] [G loss: 3.0075]\n",
      "[Epoch 11/500] [Batch 62/312] [D IS TRAINING] [D loss: 0.1511] [G loss: 2.5750]\n",
      "[Epoch 11/500] [Batch 63/312] [D IS TRAINING] [D loss: 0.2050] [G loss: 1.7036]\n",
      "[Epoch 11/500] [Batch 64/312] [D IS TRAINING] [D loss: 0.2628] [G loss: 3.1697]\n",
      "[Epoch 11/500] [Batch 65/312] [D IS TRAINING] [D loss: 0.1492] [G loss: 2.0146]\n",
      "[Epoch 11/500] [Batch 66/312] [D IS TRAINING] [D loss: 0.1876] [G loss: 1.8360]\n",
      "[Epoch 11/500] [Batch 67/312] [D IS TRAINING] [D loss: 0.4141] [G loss: 3.4280]\n",
      "[Epoch 11/500] [Batch 68/312] [D IS TRAINING] [D loss: 0.3813] [G loss: 0.8579]\n",
      "[Epoch 11/500] [Batch 69/312] [D IS TRAINING] [D loss: 0.2624] [G loss: 2.7020]\n",
      "[Epoch 11/500] [Batch 70/312] [D IS TRAINING] [D loss: 0.2037] [G loss: 3.5013]\n",
      "[Epoch 11/500] [Batch 71/312] [D IS TRAINING] [D loss: 0.2560] [G loss: 1.3274]\n",
      "[Epoch 11/500] [Batch 72/312] [D IS TRAINING] [D loss: 0.2329] [G loss: 2.6847]\n",
      "[Epoch 11/500] [Batch 73/312] [D IS TRAINING] [D loss: 0.2366] [G loss: 2.3993]\n",
      "[Epoch 11/500] [Batch 74/312] [D IS TRAINING] [D loss: 0.1959] [G loss: 2.2457]\n",
      "[Epoch 11/500] [Batch 75/312] [D IS TRAINING] [D loss: 0.1454] [G loss: 3.5217]\n",
      "[Epoch 11/500] [Batch 76/312] [D IS TRAINING] [D loss: 0.2526] [G loss: 1.3533]\n",
      "[Epoch 11/500] [Batch 77/312] [D IS TRAINING] [D loss: 0.2048] [G loss: 2.6387]\n",
      "[Epoch 11/500] [Batch 78/312] [D IS TRAINING] [D loss: 0.1347] [G loss: 3.3945]\n",
      "[Epoch 11/500] [Batch 79/312] [D IS TRAINING] [D loss: 0.2368] [G loss: 1.3667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/500] [Batch 80/312] [D IS TRAINING] [D loss: 0.2722] [G loss: 4.3512]\n",
      "[Epoch 11/500] [Batch 81/312] [D IS TRAINING] [D loss: 0.4121] [G loss: 0.9398]\n",
      "[Epoch 11/500] [Batch 82/312] [D IS TRAINING] [D loss: 0.5271] [G loss: 4.7231]\n",
      "[Epoch 11/500] [Batch 83/312] [D IS TRAINING] [D loss: 0.4776] [G loss: 0.7726]\n",
      "[Epoch 11/500] [Batch 84/312] [D IS TRAINING] [D loss: 0.3038] [G loss: 4.7713]\n",
      "[Epoch 11/500] [Batch 85/312] [D IS TRAINING] [D loss: 0.5511] [G loss: 0.7034]\n",
      "[Epoch 11/500] [Batch 86/312] [D IS TRAINING] [D loss: 0.5796] [G loss: 5.0185]\n",
      "[Epoch 11/500] [Batch 87/312] [D IS TRAINING] [D loss: 0.4533] [G loss: 0.9420]\n",
      "[Epoch 11/500] [Batch 88/312] [D IS TRAINING] [D loss: 0.3941] [G loss: 3.3745]\n",
      "[Epoch 11/500] [Batch 89/312] [D IS TRAINING] [D loss: 0.2357] [G loss: 1.9216]\n",
      "[Epoch 11/500] [Batch 90/312] [D IS TRAINING] [D loss: 0.2264] [G loss: 3.8059]\n",
      "[Epoch 11/500] [Batch 91/312] [D IS TRAINING] [D loss: 0.2714] [G loss: 1.2845]\n",
      "[Epoch 11/500] [Batch 92/312] [D IS TRAINING] [D loss: 0.1751] [G loss: 3.1259]\n",
      "[Epoch 11/500] [Batch 93/312] [D IS TRAINING] [D loss: 0.1526] [G loss: 2.5053]\n",
      "[Epoch 11/500] [Batch 94/312] [D IS TRAINING] [D loss: 0.1546] [G loss: 1.9574]\n",
      "[Epoch 11/500] [Batch 95/312] [D IS TRAINING] [D loss: 0.2307] [G loss: 2.6354]\n",
      "[Epoch 11/500] [Batch 96/312] [D IS TRAINING] [D loss: 0.2183] [G loss: 1.8236]\n",
      "[Epoch 11/500] [Batch 97/312] [D IS TRAINING] [D loss: 0.1661] [G loss: 2.3449]\n",
      "[Epoch 11/500] [Batch 98/312] [D IS TRAINING] [D loss: 0.1858] [G loss: 2.7625]\n",
      "[Epoch 11/500] [Batch 99/312] [D IS TRAINING] [D loss: 0.1682] [G loss: 1.9136]\n",
      "[Epoch 11/500] [Batch 100/312] [D IS TRAINING] [D loss: 0.1653] [G loss: 2.5503]\n",
      "[Epoch 11/500] [Batch 101/312] [D IS TRAINING] [D loss: 0.1841] [G loss: 2.0340]\n",
      "[Epoch 11/500] [Batch 102/312] [D IS TRAINING] [D loss: 0.2051] [G loss: 2.0923]\n",
      "[Epoch 11/500] [Batch 103/312] [D IS TRAINING] [D loss: 0.1956] [G loss: 1.9079]\n",
      "[Epoch 11/500] [Batch 104/312] [D IS TRAINING] [D loss: 0.1524] [G loss: 2.5193]\n",
      "[Epoch 11/500] [Batch 105/312] [D IS TRAINING] [D loss: 0.1572] [G loss: 2.4188]\n",
      "[Epoch 11/500] [Batch 106/312] [D IS TRAINING] [D loss: 0.1694] [G loss: 1.7450]\n",
      "[Epoch 11/500] [Batch 107/312] [D IS TRAINING] [D loss: 0.2084] [G loss: 2.6600]\n",
      "[Epoch 11/500] [Batch 108/312] [D IS TRAINING] [D loss: 0.1714] [G loss: 1.9081]\n",
      "[Epoch 11/500] [Batch 109/312] [D IS TRAINING] [D loss: 0.1317] [G loss: 2.5659]\n",
      "[Epoch 11/500] [Batch 110/312] [D IS TRAINING] [D loss: 0.1565] [G loss: 2.8428]\n",
      "[Epoch 11/500] [Batch 111/312] [D IS TRAINING] [D loss: 0.1832] [G loss: 1.5992]\n",
      "[Epoch 11/500] [Batch 112/312] [D IS TRAINING] [D loss: 0.1760] [G loss: 2.6132]\n",
      "[Epoch 11/500] [Batch 113/312] [D IS TRAINING] [D loss: 0.2084] [G loss: 1.9484]\n",
      "[Epoch 11/500] [Batch 114/312] [D IS TRAINING] [D loss: 0.1965] [G loss: 1.8377]\n",
      "[Epoch 11/500] [Batch 115/312] [D IS TRAINING] [D loss: 0.1694] [G loss: 2.6629]\n",
      "[Epoch 11/500] [Batch 116/312] [D IS TRAINING] [D loss: 0.1654] [G loss: 1.9232]\n",
      "[Epoch 11/500] [Batch 117/312] [D IS TRAINING] [D loss: 0.2277] [G loss: 1.9991]\n",
      "[Epoch 11/500] [Batch 118/312] [D IS TRAINING] [D loss: 0.1960] [G loss: 2.1814]\n",
      "[Epoch 11/500] [Batch 119/312] [D IS TRAINING] [D loss: 0.1558] [G loss: 2.1907]\n",
      "[Epoch 11/500] [Batch 120/312] [D IS TRAINING] [D loss: 0.1672] [G loss: 2.3864]\n",
      "[Epoch 11/500] [Batch 121/312] [D IS TRAINING] [D loss: 0.1528] [G loss: 1.9498]\n",
      "[Epoch 11/500] [Batch 122/312] [D IS TRAINING] [D loss: 0.1619] [G loss: 2.6659]\n",
      "[Epoch 11/500] [Batch 123/312] [D IS TRAINING] [D loss: 0.1338] [G loss: 2.4422]\n",
      "[Epoch 11/500] [Batch 124/312] [D IS TRAINING] [D loss: 0.1645] [G loss: 2.0470]\n",
      "[Epoch 11/500] [Batch 125/312] [D IS TRAINING] [D loss: 0.1722] [G loss: 2.1227]\n",
      "[Epoch 11/500] [Batch 126/312] [D IS TRAINING] [D loss: 0.2413] [G loss: 2.5016]\n",
      "[Epoch 11/500] [Batch 127/312] [D IS TRAINING] [D loss: 0.2778] [G loss: 1.2748]\n",
      "[Epoch 11/500] [Batch 128/312] [D IS TRAINING] [D loss: 0.3391] [G loss: 3.8265]\n",
      "[Epoch 11/500] [Batch 129/312] [D IS TRAINING] [D loss: 0.2796] [G loss: 1.2937]\n",
      "[Epoch 11/500] [Batch 130/312] [D IS TRAINING] [D loss: 0.1747] [G loss: 3.2144]\n",
      "[Epoch 11/500] [Batch 131/312] [D IS TRAINING] [D loss: 0.1688] [G loss: 2.4174]\n",
      "[Epoch 11/500] [Batch 132/312] [D IS TRAINING] [D loss: 0.2266] [G loss: 1.4328]\n",
      "[Epoch 11/500] [Batch 133/312] [D IS TRAINING] [D loss: 0.2440] [G loss: 3.7511]\n",
      "[Epoch 11/500] [Batch 134/312] [D IS TRAINING] [D loss: 0.2187] [G loss: 1.7287]\n",
      "[Epoch 11/500] [Batch 135/312] [D IS TRAINING] [D loss: 0.1479] [G loss: 2.6413]\n",
      "[Epoch 11/500] [Batch 136/312] [D IS TRAINING] [D loss: 0.1552] [G loss: 2.5438]\n",
      "[Epoch 11/500] [Batch 137/312] [D IS TRAINING] [D loss: 0.1650] [G loss: 1.8160]\n",
      "[Epoch 11/500] [Batch 138/312] [D IS TRAINING] [D loss: 0.2068] [G loss: 2.2978]\n",
      "[Epoch 11/500] [Batch 139/312] [D IS TRAINING] [D loss: 0.1624] [G loss: 2.3201]\n",
      "[Epoch 11/500] [Batch 140/312] [D IS TRAINING] [D loss: 0.1473] [G loss: 2.0844]\n",
      "[Epoch 11/500] [Batch 141/312] [D IS TRAINING] [D loss: 0.1885] [G loss: 2.4873]\n",
      "[Epoch 11/500] [Batch 142/312] [D IS TRAINING] [D loss: 0.1861] [G loss: 2.2356]\n",
      "[Epoch 11/500] [Batch 143/312] [D IS TRAINING] [D loss: 0.1479] [G loss: 1.7698]\n",
      "[Epoch 11/500] [Batch 144/312] [D IS TRAINING] [D loss: 0.1311] [G loss: 3.4616]\n",
      "[Epoch 11/500] [Batch 145/312] [D IS TRAINING] [D loss: 0.1247] [G loss: 2.6613]\n",
      "[Epoch 11/500] [Batch 146/312] [D IS TRAINING] [D loss: 0.1816] [G loss: 1.6921]\n",
      "[Epoch 11/500] [Batch 147/312] [D IS TRAINING] [D loss: 0.2102] [G loss: 2.4894]\n",
      "[Epoch 11/500] [Batch 148/312] [D IS TRAINING] [D loss: 0.1892] [G loss: 2.3526]\n",
      "[Epoch 11/500] [Batch 149/312] [D IS TRAINING] [D loss: 0.2047] [G loss: 1.6126]\n",
      "[Epoch 11/500] [Batch 150/312] [D IS TRAINING] [D loss: 0.2239] [G loss: 2.3656]\n",
      "[Epoch 11/500] [Batch 151/312] [D IS TRAINING] [D loss: 0.1586] [G loss: 2.0876]\n",
      "[Epoch 11/500] [Batch 152/312] [D IS TRAINING] [D loss: 0.1645] [G loss: 2.2704]\n",
      "[Epoch 11/500] [Batch 153/312] [D IS TRAINING] [D loss: 0.1416] [G loss: 2.6088]\n",
      "[Epoch 11/500] [Batch 154/312] [D IS TRAINING] [D loss: 0.1474] [G loss: 2.0389]\n",
      "[Epoch 11/500] [Batch 155/312] [D IS TRAINING] [D loss: 0.1734] [G loss: 2.5155]\n",
      "[Epoch 11/500] [Batch 156/312] [D IS TRAINING] [D loss: 0.1841] [G loss: 1.8544]\n",
      "[Epoch 11/500] [Batch 157/312] [D IS TRAINING] [D loss: 0.1650] [G loss: 2.4884]\n",
      "[Epoch 11/500] [Batch 158/312] [D IS TRAINING] [D loss: 0.1688] [G loss: 2.7887]\n",
      "[Epoch 11/500] [Batch 159/312] [D IS TRAINING] [D loss: 0.1474] [G loss: 2.0035]\n",
      "[Epoch 11/500] [Batch 160/312] [D IS TRAINING] [D loss: 0.1748] [G loss: 2.0289]\n",
      "[Epoch 11/500] [Batch 161/312] [D IS TRAINING] [D loss: 0.1710] [G loss: 2.7642]\n",
      "[Epoch 11/500] [Batch 162/312] [D IS TRAINING] [D loss: 0.1636] [G loss: 1.8931]\n",
      "[Epoch 11/500] [Batch 163/312] [D IS TRAINING] [D loss: 0.1909] [G loss: 2.6412]\n",
      "[Epoch 11/500] [Batch 164/312] [D IS TRAINING] [D loss: 0.1588] [G loss: 1.8385]\n",
      "[Epoch 11/500] [Batch 165/312] [D IS TRAINING] [D loss: 0.2013] [G loss: 2.2797]\n",
      "[Epoch 11/500] [Batch 166/312] [D IS TRAINING] [D loss: 0.2835] [G loss: 2.4764]\n",
      "[Epoch 11/500] [Batch 167/312] [D IS TRAINING] [D loss: 0.2630] [G loss: 1.4211]\n",
      "[Epoch 11/500] [Batch 168/312] [D IS TRAINING] [D loss: 0.3235] [G loss: 4.1486]\n",
      "[Epoch 11/500] [Batch 169/312] [D IS TRAINING] [D loss: 0.2842] [G loss: 1.4850]\n",
      "[Epoch 11/500] [Batch 170/312] [D IS TRAINING] [D loss: 0.1738] [G loss: 2.9882]\n",
      "[Epoch 11/500] [Batch 171/312] [D IS TRAINING] [D loss: 0.1974] [G loss: 2.6827]\n",
      "[Epoch 11/500] [Batch 172/312] [D IS TRAINING] [D loss: 0.2013] [G loss: 1.8642]\n",
      "[Epoch 11/500] [Batch 173/312] [D IS TRAINING] [D loss: 0.2315] [G loss: 1.9656]\n",
      "[Epoch 11/500] [Batch 174/312] [D IS TRAINING] [D loss: 0.2419] [G loss: 3.2623]\n",
      "[Epoch 11/500] [Batch 175/312] [D IS TRAINING] [D loss: 0.3970] [G loss: 0.8468]\n",
      "[Epoch 11/500] [Batch 176/312] [D IS TRAINING] [D loss: 0.5753] [G loss: 4.9612]\n",
      "[Epoch 11/500] [Batch 177/312] [D IS TRAINING] [D loss: 0.5872] [G loss: 0.5890]\n",
      "[Epoch 11/500] [Batch 178/312] [D IS TRAINING] [D loss: 0.5183] [G loss: 5.6490]\n",
      "[Epoch 11/500] [Batch 179/312] [D IS TRAINING] [D loss: 0.3431] [G loss: 1.3804]\n",
      "[Epoch 11/500] [Batch 180/312] [D IS TRAINING] [D loss: 0.2415] [G loss: 3.5712]\n",
      "[Epoch 11/500] [Batch 181/312] [D IS TRAINING] [D loss: 0.2455] [G loss: 1.5877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/500] [Batch 182/312] [D IS TRAINING] [D loss: 0.3206] [G loss: 3.2210]\n",
      "[Epoch 11/500] [Batch 183/312] [D IS TRAINING] [D loss: 0.4711] [G loss: 0.8786]\n",
      "[Epoch 11/500] [Batch 184/312] [D IS TRAINING] [D loss: 0.4578] [G loss: 5.2373]\n",
      "[Epoch 11/500] [Batch 185/312] [D IS TRAINING] [D loss: 0.5464] [G loss: 0.7701]\n",
      "[Epoch 11/500] [Batch 186/312] [D IS TRAINING] [D loss: 0.6238] [G loss: 5.9662]\n",
      "[Epoch 11/500] [Batch 187/312] [D IS TRAINING] [D loss: 0.8306] [G loss: 0.4251]\n",
      "[Epoch 11/500] [Batch 188/312] [D IS TRAINING] [D loss: 0.4458] [G loss: 4.0304]\n",
      "[Epoch 11/500] [Batch 189/312] [D IS TRAINING] [D loss: 0.3638] [G loss: 1.5742]\n",
      "[Epoch 11/500] [Batch 190/312] [D IS TRAINING] [D loss: 0.5306] [G loss: 0.9586]\n",
      "[Epoch 11/500] [Batch 191/312] [D IS TRAINING] [D loss: 0.6139] [G loss: 3.7721]\n",
      "[Epoch 11/500] [Batch 192/312] [D IS TRAINING] [D loss: 0.5522] [G loss: 0.9209]\n",
      "[Epoch 11/500] [Batch 193/312] [D IS TRAINING] [D loss: 0.3769] [G loss: 3.8800]\n",
      "[Epoch 11/500] [Batch 194/312] [D IS TRAINING] [D loss: 0.1978] [G loss: 2.0340]\n",
      "[Epoch 11/500] [Batch 195/312] [D IS TRAINING] [D loss: 0.2108] [G loss: 1.8205]\n",
      "[Epoch 11/500] [Batch 196/312] [D IS TRAINING] [D loss: 0.1986] [G loss: 2.4835]\n",
      "[Epoch 11/500] [Batch 197/312] [D IS TRAINING] [D loss: 0.1403] [G loss: 2.3095]\n",
      "[Epoch 11/500] [Batch 198/312] [D IS TRAINING] [D loss: 0.2062] [G loss: 2.3662]\n",
      "[Epoch 11/500] [Batch 199/312] [D IS TRAINING] [D loss: 0.1789] [G loss: 2.2448]\n",
      "[Epoch 11/500] [Batch 200/312] [D IS TRAINING] [D loss: 0.1815] [G loss: 2.0449]\n",
      "[Epoch 11/500] [Batch 201/312] [D IS TRAINING] [D loss: 0.2028] [G loss: 2.5250]\n",
      "[Epoch 11/500] [Batch 202/312] [D IS TRAINING] [D loss: 0.1282] [G loss: 2.2373]\n",
      "[Epoch 11/500] [Batch 203/312] [D IS TRAINING] [D loss: 0.1911] [G loss: 2.4695]\n",
      "[Epoch 11/500] [Batch 204/312] [D IS TRAINING] [D loss: 0.2093] [G loss: 1.6416]\n",
      "[Epoch 11/500] [Batch 205/312] [D IS TRAINING] [D loss: 0.3344] [G loss: 3.6352]\n",
      "[Epoch 11/500] [Batch 206/312] [D IS TRAINING] [D loss: 0.2646] [G loss: 1.2663]\n",
      "[Epoch 11/500] [Batch 207/312] [D IS TRAINING] [D loss: 0.2013] [G loss: 2.3473]\n",
      "[Epoch 11/500] [Batch 208/312] [D IS TRAINING] [D loss: 0.2166] [G loss: 2.6610]\n",
      "[Epoch 11/500] [Batch 209/312] [D IS TRAINING] [D loss: 0.1672] [G loss: 1.9027]\n",
      "[Epoch 11/500] [Batch 210/312] [D IS TRAINING] [D loss: 0.2182] [G loss: 1.6856]\n",
      "[Epoch 11/500] [Batch 211/312] [D IS TRAINING] [D loss: 0.2498] [G loss: 2.8271]\n",
      "[Epoch 11/500] [Batch 212/312] [D IS TRAINING] [D loss: 0.1732] [G loss: 2.0067]\n",
      "[Epoch 11/500] [Batch 213/312] [D IS TRAINING] [D loss: 0.1717] [G loss: 2.1726]\n",
      "[Epoch 11/500] [Batch 214/312] [D IS TRAINING] [D loss: 0.2174] [G loss: 2.0749]\n",
      "[Epoch 11/500] [Batch 215/312] [D IS TRAINING] [D loss: 0.2160] [G loss: 1.7249]\n",
      "[Epoch 11/500] [Batch 216/312] [D IS TRAINING] [D loss: 0.2377] [G loss: 3.0394]\n",
      "[Epoch 11/500] [Batch 217/312] [D IS TRAINING] [D loss: 0.1599] [G loss: 2.0203]\n",
      "[Epoch 11/500] [Batch 218/312] [D IS TRAINING] [D loss: 0.2258] [G loss: 1.5705]\n",
      "[Epoch 11/500] [Batch 219/312] [D IS TRAINING] [D loss: 0.2803] [G loss: 3.1437]\n",
      "[Epoch 11/500] [Batch 220/312] [D IS TRAINING] [D loss: 0.2126] [G loss: 2.0801]\n",
      "[Epoch 11/500] [Batch 221/312] [D IS TRAINING] [D loss: 0.1439] [G loss: 2.9301]\n",
      "[Epoch 11/500] [Batch 222/312] [D IS TRAINING] [D loss: 0.1220] [G loss: 2.6095]\n",
      "[Epoch 11/500] [Batch 223/312] [D IS TRAINING] [D loss: 0.1515] [G loss: 2.7286]\n",
      "[Epoch 11/500] [Batch 224/312] [D IS TRAINING] [D loss: 0.2355] [G loss: 1.6063]\n",
      "[Epoch 11/500] [Batch 225/312] [D IS TRAINING] [D loss: 0.2781] [G loss: 2.4824]\n",
      "[Epoch 11/500] [Batch 226/312] [D IS TRAINING] [D loss: 0.2079] [G loss: 1.7100]\n",
      "[Epoch 11/500] [Batch 227/312] [D IS TRAINING] [D loss: 0.2103] [G loss: 1.8239]\n",
      "[Epoch 11/500] [Batch 228/312] [D IS TRAINING] [D loss: 0.2139] [G loss: 3.3654]\n",
      "[Epoch 11/500] [Batch 229/312] [D IS TRAINING] [D loss: 0.2272] [G loss: 1.7529]\n",
      "[Epoch 11/500] [Batch 230/312] [D IS TRAINING] [D loss: 0.1830] [G loss: 2.2924]\n",
      "[Epoch 11/500] [Batch 231/312] [D IS TRAINING] [D loss: 0.1692] [G loss: 2.7280]\n",
      "[Epoch 11/500] [Batch 232/312] [D IS TRAINING] [D loss: 0.1836] [G loss: 1.8547]\n",
      "[Epoch 11/500] [Batch 233/312] [D IS TRAINING] [D loss: 0.1395] [G loss: 2.4480]\n",
      "[Epoch 11/500] [Batch 234/312] [D IS TRAINING] [D loss: 0.1778] [G loss: 2.8019]\n",
      "[Epoch 11/500] [Batch 235/312] [D IS TRAINING] [D loss: 0.2014] [G loss: 1.6029]\n",
      "[Epoch 11/500] [Batch 236/312] [D IS TRAINING] [D loss: 0.1755] [G loss: 2.8302]\n",
      "[Epoch 11/500] [Batch 237/312] [D IS TRAINING] [D loss: 0.1632] [G loss: 2.0937]\n",
      "[Epoch 11/500] [Batch 238/312] [D IS TRAINING] [D loss: 0.1728] [G loss: 1.8396]\n",
      "[Epoch 11/500] [Batch 239/312] [D IS TRAINING] [D loss: 0.1799] [G loss: 2.9410]\n",
      "[Epoch 11/500] [Batch 240/312] [D IS TRAINING] [D loss: 0.1822] [G loss: 2.0778]\n",
      "[Epoch 11/500] [Batch 241/312] [D IS TRAINING] [D loss: 0.1722] [G loss: 1.9844]\n",
      "[Epoch 11/500] [Batch 242/312] [D IS TRAINING] [D loss: 0.1886] [G loss: 2.4602]\n",
      "[Epoch 11/500] [Batch 243/312] [D IS TRAINING] [D loss: 0.2018] [G loss: 1.8167]\n",
      "[Epoch 11/500] [Batch 244/312] [D IS TRAINING] [D loss: 0.1542] [G loss: 2.8016]\n",
      "[Epoch 11/500] [Batch 245/312] [D IS TRAINING] [D loss: 0.1546] [G loss: 2.2559]\n",
      "[Epoch 11/500] [Batch 246/312] [D IS TRAINING] [D loss: 0.1981] [G loss: 1.9476]\n",
      "[Epoch 11/500] [Batch 247/312] [D IS TRAINING] [D loss: 0.1923] [G loss: 1.9765]\n",
      "[Epoch 11/500] [Batch 248/312] [D IS TRAINING] [D loss: 0.2109] [G loss: 2.3879]\n",
      "[Epoch 11/500] [Batch 249/312] [D IS TRAINING] [D loss: 0.1720] [G loss: 2.0190]\n",
      "[Epoch 11/500] [Batch 250/312] [D IS TRAINING] [D loss: 0.1834] [G loss: 2.3063]\n",
      "[Epoch 11/500] [Batch 251/312] [D IS TRAINING] [D loss: 0.1789] [G loss: 2.1650]\n",
      "[Epoch 11/500] [Batch 252/312] [D IS TRAINING] [D loss: 0.1351] [G loss: 2.2550]\n",
      "[Epoch 11/500] [Batch 253/312] [D IS TRAINING] [D loss: 0.1409] [G loss: 2.6361]\n",
      "[Epoch 11/500] [Batch 254/312] [D IS TRAINING] [D loss: 0.1778] [G loss: 2.3068]\n",
      "[Epoch 11/500] [Batch 255/312] [D IS TRAINING] [D loss: 0.1828] [G loss: 1.9203]\n",
      "[Epoch 11/500] [Batch 256/312] [D IS TRAINING] [D loss: 0.1857] [G loss: 2.7059]\n",
      "[Epoch 11/500] [Batch 257/312] [D IS TRAINING] [D loss: 0.1469] [G loss: 2.3577]\n",
      "[Epoch 11/500] [Batch 258/312] [D IS TRAINING] [D loss: 0.1342] [G loss: 2.2448]\n",
      "[Epoch 11/500] [Batch 259/312] [D IS TRAINING] [D loss: 0.1297] [G loss: 2.4768]\n",
      "[Epoch 11/500] [Batch 260/312] [D IS TRAINING] [D loss: 0.1508] [G loss: 2.4090]\n",
      "[Epoch 11/500] [Batch 261/312] [D IS TRAINING] [D loss: 0.1254] [G loss: 2.5549]\n",
      "[Epoch 11/500] [Batch 262/312] [D IS TRAINING] [D loss: 0.1645] [G loss: 1.8258]\n",
      "[Epoch 11/500] [Batch 263/312] [D IS TRAINING] [D loss: 0.2256] [G loss: 3.2838]\n",
      "[Epoch 11/500] [Batch 264/312] [D IS TRAINING] [D loss: 0.3308] [G loss: 0.9762]\n",
      "[Epoch 11/500] [Batch 265/312] [D IS TRAINING] [D loss: 0.4320] [G loss: 4.4950]\n",
      "[Epoch 11/500] [Batch 266/312] [D IS TRAINING] [D loss: 0.2170] [G loss: 1.8561]\n",
      "[Epoch 11/500] [Batch 267/312] [D IS TRAINING] [D loss: 0.2416] [G loss: 1.5752]\n",
      "[Epoch 11/500] [Batch 268/312] [D IS TRAINING] [D loss: 0.4340] [G loss: 3.6249]\n",
      "[Epoch 11/500] [Batch 269/312] [D IS TRAINING] [D loss: 0.3874] [G loss: 0.8537]\n",
      "[Epoch 11/500] [Batch 270/312] [D IS TRAINING] [D loss: 0.3679] [G loss: 4.2531]\n",
      "[Epoch 11/500] [Batch 271/312] [D IS TRAINING] [D loss: 0.2837] [G loss: 1.4146]\n",
      "[Epoch 11/500] [Batch 272/312] [D IS TRAINING] [D loss: 0.3101] [G loss: 3.3156]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--model_path\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/rnap/data/celeb/female/model\u001b[39m\u001b[38;5;124m'\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaved model path\u001b[39m\u001b[38;5;124m'\u001b[39m) \n\u001b[1;32m     17\u001b[0m opt \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--n_epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m500\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m64\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--lr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.0001\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--b1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--b2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.999\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--n_cpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--latent_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--img_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m64\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     19\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--channels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--sample_interval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5000\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--csv_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfemale_mac.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--save_interval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m50\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--model_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/cristianespinosa/Documents/RESUME_Summer_2023/GAN_Project/RESUME_UCM_GAN/Model\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 21\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     73\u001b[0m     counter\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[43md_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     optimizer_D\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m] [Batch \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m] [D IS TRAINING] [D loss: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m] [G loss: \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;241m%\u001b[39m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, opt\u001b[38;5;241m.\u001b[39mn_epochs, i, \u001b[38;5;28mlen\u001b[39m(dataloader), d_loss\u001b[38;5;241m.\u001b[39mitem(), g_loss\u001b[38;5;241m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tensor/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--n_epochs\", type=int, default=200, help=\"Number of epochs of training\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Size of the batches\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"Learning rate for the optimizer\")\n",
    "    parser.add_argument(\"--b1\", type=float, default=0.5, help=\"Beta1 parameter for the optimizer\")\n",
    "    parser.add_argument(\"--b2\", type=float, default=0.999, help=\"Beta2 parameter for the optimizer\")\n",
    "    parser.add_argument(\"--n_cpu\", type=int, default=8, help=\"Number of CPU threads for data loading\")\n",
    "    parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"Dimensionality of the latent space\")\n",
    "    parser.add_argument(\"--img_size\", type=int, default=64, help=\"Size of each image dimension\")\n",
    "    parser.add_argument(\"--channels\", type=int, default=1, help=\"Number of image channels\")\n",
    "    parser.add_argument(\"--sample_interval\", type=int, default=10000, help=\"Interval between image samples\")\n",
    "    parser.add_argument(\"--csv_file\", type=str, default=\"male.csv\", help=\"CSV file containing image paths and labels\")\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=50, help=\"Interval between saving model\")\n",
    "    parser.add_argument('--model_path', type=str, default='/home/rnap/data/celeb/female/model', help='Saved model path') \n",
    "    \n",
    "    opt = parser.parse_args([\"--n_epochs\", \"500\", \"--batch_size\", \"64\", \"--lr\", \"0.0001\", \"--b1\", \"0.5\",\n",
    "                         \"--b2\", \"0.999\", \"--n_cpu\", \"8\", \"--latent_dim\", \"100\", \"--img_size\", \"64\", \n",
    "                         \"--channels\", \"1\", \"--sample_interval\", \"5000\", \"--csv_file\", \"female_mac.csv\",\n",
    "                         \"--save_interval\", \"50\", \"--model_path\", \"/Users/cristianespinosa/Documents/RESUME_Summer_2023/GAN_Project/RESUME_UCM_GAN/Model\"])\n",
    "    train(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d476ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = pd.read_csv('female_mac.csv')\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa489f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
